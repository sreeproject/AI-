{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "collapsed_sections": [
        "FJNUwmbgGyua",
        "w6K7xa23Elo4",
        "yQaldy8SH6Dl",
        "mDgbUHAGgjLW",
        "35m5QtbWiB9F",
        "PoPl-ycgm1ru",
        "nA9Y7ga8ng1Z",
        "PBTbrJXOngz2",
        "u3PMJOP6ngxN",
        "dauF4eBmngu3",
        "bKJF3rekwFvQ",
        "MSa1f5Uengrz",
        "GF8Ens_Soomf",
        "0wOQAZs5pc--",
        "K5QZ13OEpz2H",
        "lQ7QKXXCp7Bj",
        "448CDAPjqfQr",
        "KSlN3yHqYklG",
        "t6dVpIINYklI",
        "ijmpgYnKYklI",
        "-JiQyfWJYklI",
        "EM7whBJCYoAo",
        "fge-S5ZAYoAp",
        "85gYPyotYoAp",
        "RoGjAbkUYoAp",
        "4Of9eVA-YrdM",
        "iky9q4vBYrdO",
        "F6T5p64dYrdO",
        "y-Ehk30pYrdP",
        "bamQiAODYuh1",
        "QHF8YVU7Yuh3",
        "GwzvFGzlYuh3",
        "qYpmQ266Yuh3",
        "OH-pJp9IphqM",
        "bbFf2-_FphqN",
        "_ouA3fa0phqN",
        "Seke61FWphqN",
        "PIIx-8_IphqN",
        "t27r6nlMphqO",
        "r2jJGEOYphqO",
        "b0JNsNcRphqO",
        "BZR9WyysphqO",
        "jj7wYXLtphqO",
        "eZrbJ2SmphqO",
        "rFu4xreNphqO",
        "YJ55k-q6phqO",
        "gCFgpxoyphqP",
        "OVtJsKN_phqQ",
        "lssrdh5qphqQ",
        "U2RJ9gkRphqQ",
        "1M8mcRywphqQ",
        "tgIPom80phqQ",
        "JMzcOPDDphqR",
        "x-EpHcCOp1ci",
        "X_VqEhTip1ck",
        "8zGJKyg5p1ck",
        "PVzmfK_Ep1ck",
        "n3dbpmDWp1ck",
        "ylSl6qgtp1ck",
        "ZWILFDl5p1ck",
        "M7G43BXep1ck",
        "Ag9LCva-p1cl",
        "E6MkPsBcp1cl",
        "2cELzS2fp1cl",
        "3MPXvC8up1cl",
        "NC_X3p0fY2L0",
        "UV0SzAkaZNRQ",
        "YPEH6qLeZNRQ",
        "q29F0dvdveiT",
        "EXh0U9oCveiU",
        "22aHeOlLveiV",
        "g-ATYxFrGrvw",
        "Yfr_Vlr8HBkt",
        "8yEUt7NnHlrM",
        "tEA2Xm5dHt1r",
        "I79__PHVH19G",
        "Ou-I18pAyIpj",
        "fF3858GYyt-u",
        "4_0_7-oCpUZd",
        "hwyV_J3ipUZe",
        "3yB-zSqbpUZe",
        "dEUvejAfpUZe",
        "Fd15vwWVpUZf",
        "bn_IUdTipZyH",
        "49K5P_iCpZyH",
        "Nff-vKELpZyI",
        "kLW572S8pZyI",
        "dWbDXHzopZyI",
        "yLjJCtPM0KBk",
        "xiyOF9F70UgQ",
        "7wuGOrhz0itI",
        "id1riN9m0vUs",
        "578E2V7j08f6",
        "89xtkJwZ18nB",
        "67NQN5KX2AMe",
        "Iwf50b-R2tYG",
        "GMQiZwjn3iu7",
        "WVIkgGqN3qsr",
        "XkPnILGE3zoT",
        "Hlsf0x5436Go",
        "mT9DMSJo4nBL",
        "c49ITxTc407N",
        "OeJFEK0N496M",
        "9ExmJH0g5HBk",
        "cJNqERVU536h",
        "k5UmGsbsOxih",
        "T0VqWOYE6DLQ",
        "qBMux9mC6MCf",
        "-oLEiFgy-5Pf",
        "C74aWNz2AliB",
        "2DejudWSA-a0",
        "pEMng2IbBLp7",
        "rAdphbQ9Bhjc",
        "TNVZ9zx19K6k",
        "nqoHp30x9hH9",
        "rMDnDkt2B6du",
        "yiiVWRdJDDil",
        "1UUpS68QDMuG",
        "kexQrXU-DjzY",
        "T5CmagL3EC8N",
        "BhH2vgX9EjGr",
        "qjKvONjwE8ra",
        "P1XJ9OREExlT",
        "VFOzZv6IFROw",
        "TIqpNgepFxVj",
        "VfCC591jGiD4",
        "OB4l2ZhMeS1U",
        "ArJBuiUVfxKd",
        "4qY1EAkEfxKe",
        "PiV4Ypx8fxKe",
        "TfvqoZmBfxKf",
        "dJ2tPlVmpsJ0",
        "JWYfwnehpsJ1",
        "-jK_YjpMpsJ2",
        "HAih1iBOpsJ2",
        "zVGeBEFhpsJ2",
        "bmKjuQ-FpsJ3",
        "Fze-IPXLpx6K",
        "7AN1z2sKpx6M",
        "9PIHJqyupx6M",
        "_-qAgymDpx6N",
        "Z-hykwinpx6N",
        "h_CCil-SKHpo",
        "cBFFvTBNJzUa",
        "HvGl1hHyA_VK",
        "EyNgTHvd2WFk",
        "KH5McJBi2d8v",
        "iW_Lq9qf2h6X",
        "-Kee-DAl2viO",
        "gCX9965dhzqZ",
        "gIfDvo9L0UH2"
      ],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sreeproject/AI-/blob/main/Brain_Tumor_Sample_ML_Submission_Template_2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Project Name**    -  Brain Tumor MRI Image Classification\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "vncDsAP0Gaoa"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### **Project Type**    - Classification/Supervised\n",
        "##### **Contribution**    - Individual\n"
      ],
      "metadata": {
        "id": "beRrZCGUAJYm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Project Summary -**"
      ],
      "metadata": {
        "id": "FJNUwmbgGyua"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "This project aims to develop a deep learning-based solution for classifying brain MRI images into multiple categories according to tumor type. It involves building a custom CNN model from scratch and enhancing performance through transfer learning using pretrained models. The project also includes deploying a user-friendly Streamlit web application to enable real-time tumor type predictions from uploaded MRI images.\n"
      ],
      "metadata": {
        "id": "F6v_1wHtG2nS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **GitHub Link -**"
      ],
      "metadata": {
        "id": "w6K7xa23Elo4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Provide your GitHub Link here."
      ],
      "metadata": {
        "id": "h1o69JH3Eqqn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Problem Statement**\n"
      ],
      "metadata": {
        "id": "yQaldy8SH6Dl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        " Brain Tumor MRI Image Classification"
      ],
      "metadata": {
        "id": "DpeJGUA3kjGy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **General Guidelines** : -  "
      ],
      "metadata": {
        "id": "mDgbUHAGgjLW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1.   Well-structured, formatted, and commented code is required.\n",
        "2.   Exception Handling, Production Grade Code & Deployment Ready Code will be a plus. Those students will be awarded some additional credits.\n",
        "     \n",
        "     The additional credits will have advantages over other students during Star Student selection.\n",
        "       \n",
        "             [ Note: - Deployment Ready Code is defined as, the whole .ipynb notebook should be executable in one go\n",
        "                       without a single error logged. ]\n",
        "\n",
        "3.   Each and every logic should have proper comments.\n",
        "4. You may add as many number of charts you want. Make Sure for each and every chart the following format should be answered.\n",
        "        \n",
        "\n",
        "```\n",
        "# Chart visualization code\n",
        "```\n",
        "            \n",
        "\n",
        "*   Why did you pick the specific chart?\n",
        "*   What is/are the insight(s) found from the chart?\n",
        "* Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason.\n",
        "\n",
        "5. You have to create at least 15 logical & meaningful charts having important insights.\n",
        "\n",
        "\n",
        "[ Hints : - Do the Vizualization in  a structured way while following \"UBM\" Rule.\n",
        "\n",
        "U - Univariate Analysis,\n",
        "\n",
        "B - Bivariate Analysis (Numerical - Categorical, Numerical - Numerical, Categorical - Categorical)\n",
        "\n",
        "M - Multivariate Analysis\n",
        " ]\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "6. You may add more ml algorithms for model creation. Make sure for each and every algorithm, the following format should be answered.\n",
        "\n",
        "\n",
        "*   Explain the ML Model used and it's performance using Evaluation metric Score Chart.\n",
        "\n",
        "\n",
        "*   Cross- Validation & Hyperparameter Tuning\n",
        "\n",
        "*   Have you seen any improvement? Note down the improvement with updates Evaluation metric Score Chart.\n",
        "\n",
        "*   Explain each evaluation metric's indication towards business and the business impact pf the ML model used.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "ZrxVaUj-hHfC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ***Let's Begin !***"
      ],
      "metadata": {
        "id": "O_i_v8NEhb9l"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***1. Know Your Data***"
      ],
      "metadata": {
        "id": "HhfV-JJviCcP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Import Libraries"
      ],
      "metadata": {
        "id": "Y3lxredqlCYt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import Libraries\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import torch\n",
        "from torch.utils.data import dataset,dataloader\n",
        "import glob\n",
        "import cv2\n",
        "import os\n",
        "import torch\n",
        "import torchvision.transforms as transforms\n",
        "from torchvision.datasets import ImageFolder\n",
        "from torch.utils.data import DataLoader\n",
        "import torchvision.models as models\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import matplotlib.pyplot as plt\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')"
      ],
      "metadata": {
        "id": "M8Vqi-pPk-HR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "to-y-_6ac8cP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Dataset Loading"
      ],
      "metadata": {
        "id": "3RnN4peoiCZX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "4I-GR5-mePBP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "root = r'/content/drive/MyDrive/Tumour'\n",
        "print(\"dataset exists:\", os.path.exists(root))\n",
        "\n",
        "if os.path.exists(root):\n",
        "    print(\"Subfolders:\")\n",
        "    for sub in os.listdir(root):\n",
        "        print(\"  ➤\", sub)"
      ],
      "metadata": {
        "id": "Z9RqEDiIc7ak"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Dataset First View"
      ],
      "metadata": {
        "id": "FidazFZ1U-P1"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "y49SUxdjW2t-"
      },
      "outputs": [],
      "source": [
        "os.listdir(\"/content/drive/MyDrive/Tumour/train\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "os.listdir(\"/content/drive/MyDrive/Tumour/valid\")"
      ],
      "metadata": {
        "id": "0VtCM3WYvSzm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pHfFBuW4vWTp"
      },
      "outputs": [],
      "source": [
        "os.listdir(\"/content/drive/MyDrive/Tumour/test\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "6MQDwSCdeftY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "import tensorflow.keras.preprocessing\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "from tensorflow.keras.preprocessing import image\n",
        "img=image.load_img('/content/drive/MyDrive/Tumour/train/glioma/Tr-gl_0011_jpg.rf.61e213cb5a0f97fedd1bacd0428c0133.jpg')\n",
        "plt.imshow(img)"
      ],
      "metadata": {
        "id": "cMjgD9taegaB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "3yjiAvcOy91-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.preprocessing import image\n",
        "import matplotlib.pyplot as plt\n",
        "import glob\n",
        "\n",
        "path = r'/content/drive/MyDrive/Tumour/valid/glioma/*.jpg'\n",
        "\n",
        "files = glob.glob(path)\n",
        "if files:\n",
        "    img_path = files[0]  # just pick the first one\n",
        "    img = image.load_img(img_path)\n",
        "    plt.imshow(img)\n",
        "    plt.title(os.path.basename(img_path))\n",
        "    plt.axis('off')\n",
        "    plt.show()\n",
        "else:\n",
        "    print(\"No images found in folder.\")\n"
      ],
      "metadata": {
        "id": "hySWUI1tc7LK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "import tensorflow.keras.preprocessing\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "from tensorflow.keras.preprocessing import image\n",
        "img=image.load_img('/content/drive/MyDrive/Tumour/test/glioma/Tr-gl_0016_jpg.rf.99746694ea97fe0b73108832b462d48e.jpg')\n",
        "plt.imshow(img)"
      ],
      "metadata": {
        "id": "NtpW9Dt7WC7N"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "qZp5W2e-ViUj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Dataset Rows & Columns count"
      ],
      "metadata": {
        "id": "t8ZBue8MVjBf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "img=image.load_img('/content/drive/MyDrive/Tumour/test/glioma/Tr-gl_0016_jpg.rf.99746694ea97fe0b73108832b462d48e.jpg')\n",
        "img_array = np.array(img) # Convert PIL Image to NumPy array\n",
        "print(img_array.shape) # Access the shape of the NumPy array"
      ],
      "metadata": {
        "id": "MX59-Zjkdw9f"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "z9TT6k87PhLd"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "9j08ZmZqc7hS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Dataset Information"
      ],
      "metadata": {
        "id": "aihHLxwVWB30"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Path to images inside extracted folder\n",
        "\n",
        "data_dir1 = '/content/drive/MyDrive/Tumour/train/*/*.jpg'\n",
        "train = []\n",
        "for f in glob.glob(data_dir1, recursive=True):\n",
        "    img = cv2.imread(f)\n",
        "    if img is not None:\n",
        "        train.append(img)\n",
        "    else:\n",
        "        print(f\"Failed to load: {f}\")\n",
        "\n",
        "print(f\"Total images loaded: {len(train)}\")"
      ],
      "metadata": {
        "id": "QVun60Upfou5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "mXlkgBb7fo12"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Path to images inside extracted folder\n",
        "\n",
        "data_dir2 = '/content/drive/MyDrive/Tumour/valid/*/*.jpg'\n",
        "valid = []\n",
        "for f in glob.glob(data_dir2, recursive=True):\n",
        "    img = cv2.imread(f)\n",
        "    if img is not None:\n",
        "        valid.append(img)\n",
        "    else:\n",
        "        print(f\"Failed to load: {f}\")\n",
        "\n",
        "print(f\"Total images loaded: {len(valid)}\")"
      ],
      "metadata": {
        "id": "5VRgQNnxUruZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Path to images inside extracted folder\n",
        "\n",
        "data_dir3 = '/content/drive/MyDrive/Tumour/test/*/*.jpg'\n",
        "test = []\n",
        "for f in glob.glob(data_dir3, recursive=True):\n",
        "    img = cv2.imread(f)\n",
        "    if img is not None:\n",
        "        test.append(img)\n",
        "    else:\n",
        "        print(f\"Failed to load: {f}\")\n",
        "\n",
        "print(f\"Total images loaded: {len(test)}\")"
      ],
      "metadata": {
        "id": "y-kaFDZKYcpQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Train contains 1695 images, valid contains 502 images and test contains 246 images."
      ],
      "metadata": {
        "id": "REC6rwMKMvme"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "pxW3Cpcgvs9s"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from collections import defaultdict\n",
        "from PIL import Image\n",
        "dataset_path = \"/content/drive/MyDrive/Tumour/train\"  # or any folder containing class subfolders\n",
        "\n",
        "# Step 1: Collect class-wise image counts and image resolutions\n",
        "class_counts = defaultdict(int)\n",
        "image_sizes = []\n",
        "\n",
        "# Walk through the dataset\n",
        "for class_name in os.listdir(dataset_path):\n",
        "    class_dir = os.path.join(dataset_path, class_name)\n",
        "    if os.path.isdir(class_dir):\n",
        "        for image_name in os.listdir(class_dir):\n",
        "            img_path = os.path.join(class_dir, image_name)\n",
        "            try:\n",
        "                with Image.open(img_path) as img:\n",
        "                  class_counts[class_name] += 1\n",
        "                  image_sizes.append(img.size)\n",
        "            except Exception as e:\n",
        "                  print(f\"Skipping image: {img_path} | Error: {e}\")\n",
        "\n",
        "# Step 2: Convert to DataFrame\n",
        "df_counts = pd.DataFrame(list(class_counts.items()), columns=['Tumor_Type', 'Image_Count'])\n",
        "df_sizes = pd.DataFrame(image_sizes, columns=['Width', 'Height'])\n",
        "\n",
        "# Step 3: Plot Class Distribution\n",
        "plt.figure(figsize=(8, 5))\n",
        "sns.barplot(data=df_counts, x='Tumor_Type', y='Image_Count', palette='Set2')\n",
        "plt.title(\"Class Distribution (Tumor Types)\")\n",
        "plt.ylabel(\"Number of Images\")\n",
        "plt.xlabel(\"Tumor Type\")\n",
        "plt.xticks(rotation=45)\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# Step 4: Plot Image Resolution Distribution\n",
        "plt.figure(figsize=(8, 5))\n",
        "sns.histplot(df_sizes['Width'], color='skyblue', kde=True, label='Width', bins=30)\n",
        "sns.histplot(df_sizes['Height'], color='orange', kde=True, label='Height', bins=30)\n",
        "plt.title(\"Image Resolution Distribution\")\n",
        "plt.xlabel(\"Pixels\")\n",
        "plt.legend()\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# Step 5: Print summary statistics\n",
        "print(\"Class Distribution:\\n\", df_counts)\n",
        "print(\"\\nImage Resolution Stats:\\n\", df_sizes.describe())"
      ],
      "metadata": {
        "id": "7jpoe34iWchV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure(figsize=(7, 7))\n",
        "plt.pie(df_counts['Image_Count'], labels=df_counts['Tumor_Type'], autopct='%1.1f%%', startangle=140, colors=sns.color_palette('Set2'))\n",
        "plt.title(\"Tumor Class Distribution (Pie Chart)\")\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "Tuh8u5DGJpM_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from collections import defaultdict\n",
        "from PIL import Image\n",
        "dataset_path = \"/content/drive/MyDrive/Tumour/valid\"  # or any folder containing class subfolders\n",
        "\n",
        "# Step 1: Collect class-wise image counts and image resolutions\n",
        "class_counts = defaultdict(int)\n",
        "image_sizes = []\n",
        "\n",
        "# Walk through the dataset\n",
        "for class_name in os.listdir(dataset_path):\n",
        "    class_dir = os.path.join(dataset_path, class_name)\n",
        "    if os.path.isdir(class_dir):\n",
        "        for image_name in os.listdir(class_dir):\n",
        "            img_path = os.path.join(class_dir, image_name)\n",
        "            try:\n",
        "                with Image.open(img_path) as img:\n",
        "                  class_counts[class_name] += 1\n",
        "                  image_sizes.append(img.size)\n",
        "            except Exception as e:\n",
        "                  print(f\"Skipping image: {img_path} | Error: {e}\")\n",
        "\n",
        "# Step 2: Convert to DataFrame\n",
        "df_counts1 = pd.DataFrame(list(class_counts.items()), columns=['Tumor_Type', 'Image_Count'])\n",
        "df_sizes = pd.DataFrame(image_sizes, columns=['Width', 'Height'])\n",
        "\n",
        "# Step 3: Plot Class Distribution\n",
        "plt.figure(figsize=(8, 5))\n",
        "sns.barplot(data=df_counts, x='Tumor_Type', y='Image_Count', palette='Set2')\n",
        "plt.title(\"Class Distribution (Tumor Types)\")\n",
        "plt.ylabel(\"Number of Images\")\n",
        "plt.xlabel(\"Tumor Type\")\n",
        "plt.xticks(rotation=45)\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# Step 4: Plot Image Resolution Distribution\n",
        "plt.figure(figsize=(8, 5))\n",
        "sns.histplot(df_sizes['Width'], color='skyblue', kde=True, label='Width', bins=30)\n",
        "sns.histplot(df_sizes['Height'], color='orange', kde=True, label='Height', bins=30)\n",
        "plt.title(\"Image Resolution Distribution\")\n",
        "plt.xlabel(\"Pixels\")\n",
        "plt.legend()\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# Step 5: Print summary statistics\n",
        "print(\"Class Distribution:\\n\", df_counts1)\n",
        "print(\"\\nImage Resolution Stats:\\n\", df_sizes.describe())"
      ],
      "metadata": {
        "id": "rlBVp3LkJwHd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure(figsize=(7, 7))\n",
        "plt.pie(df_counts1['Image_Count'], labels=df_counts1['Tumor_Type'], autopct='%1.1f%%', startangle=140, colors=sns.color_palette('Set2'))\n",
        "plt.title(\"Tumor Class Distribution (Pie Chart)\")\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "3z4HpypEJ4oB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from collections import defaultdict\n",
        "from PIL import Image\n",
        "dataset_path = \"/content/drive/MyDrive/Tumour/test\"  # or any folder containing class subfolders\n",
        "\n",
        "# Step 1: Collect class-wise image counts and image resolutions\n",
        "class_counts = defaultdict(int)\n",
        "image_sizes = []\n",
        "\n",
        "# Walk through the dataset\n",
        "for class_name in os.listdir(dataset_path):\n",
        "    class_dir = os.path.join(dataset_path, class_name)\n",
        "    if os.path.isdir(class_dir):\n",
        "        for image_name in os.listdir(class_dir):\n",
        "            img_path = os.path.join(class_dir, image_name)\n",
        "            try:\n",
        "                with Image.open(img_path) as img:\n",
        "                  class_counts[class_name] += 1\n",
        "                  image_sizes.append(img.size)\n",
        "            except Exception as e:\n",
        "                  print(f\"Skipping image: {img_path} | Error: {e}\")\n",
        "\n",
        "# Step 2: Convert to DataFrame\n",
        "df_counts2 = pd.DataFrame(list(class_counts.items()), columns=['Tumor_Type', 'Image_Count'])\n",
        "df_sizes = pd.DataFrame(image_sizes, columns=['Width', 'Height'])\n",
        "\n",
        "# Step 3: Plot Class Distribution\n",
        "plt.figure(figsize=(8, 5))\n",
        "sns.barplot(data=df_counts, x='Tumor_Type', y='Image_Count', palette='Set2')\n",
        "plt.title(\"Class Distribution (Tumor Types)\")\n",
        "plt.ylabel(\"Number of Images\")\n",
        "plt.xlabel(\"Tumor Type\")\n",
        "plt.xticks(rotation=45)\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# Step 4: Plot Image Resolution Distribution\n",
        "plt.figure(figsize=(8, 5))\n",
        "sns.histplot(df_sizes['Width'], color='skyblue', kde=True, label='Width', bins=30)\n",
        "sns.histplot(df_sizes['Height'], color='orange', kde=True, label='Height', bins=30)\n",
        "plt.title(\"Image Resolution Distribution\")\n",
        "plt.xlabel(\"Pixels\")\n",
        "plt.legend()\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# Step 5: Print summary statistics\n",
        "print(\"Class Distribution:\\n\", df_counts2)\n",
        "print(\"\\nImage Resolution Stats:\\n\", df_sizes.describe())"
      ],
      "metadata": {
        "id": "9Pz0B6vHJ6jz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure(figsize=(7, 7))\n",
        "plt.pie(df_counts['Image_Count'], labels=df_counts2['Tumor_Type'], autopct='%1.1f%%', startangle=140, colors=sns.color_palette('Set2'))\n",
        "plt.title(\"Tumor Class Distribution (Pie Chart)\")\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "MXyIMVubLy8l"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "6Yyi7I4FL8yt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data_dir = \"/content/brain\"\n",
        "data_dir = pathlib.Path(data_dir)"
      ],
      "metadata": {
        "id": "zgKHxD2n7NcM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define transformations (resize, normalize, augment train)\n",
        "import pathlib\n",
        "\n",
        "data_dir='/content/drive/MyDrive/Tumour'\n",
        "#train_dir = os.path.join(dataset_dir, 'train')\n",
        "data_dir = pathlib.Path(data_dir)\n",
        "transform = transforms.Compose([\n",
        "    transforms.Resize((224, 224)),\n",
        "    transforms.RandomHorizontalFlip(),\n",
        "    transforms.RandomVerticalFlip(p=0.5),\n",
        "    transforms.RandomRotation(30),\n",
        "    transforms.RandomAffine(degrees=0, translate=(0.1, 0.1)),  # Random shift (10%)\n",
        "    transforms.ColorJitter(brightness=0.2, contrast=0.2),   # Random brightness & contrast\n",
        "    transforms.RandomResizedCrop(224, scale=(0.8, 1.0)),    # Random zoom (scale)\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize([0.5]*3, [0.5]*3)\n",
        "])"
      ],
      "metadata": {
        "id": "p1ymtLtAWd1F"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "FUsFv9i2lRp9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "valid_dir = os.path.join(data_dir, 'valid')\n",
        "test_dir = os.path.join(data_dir, 'test')\n",
        "train_dir = os.path.join(data_dir, 'train')\n",
        "train_dataset = ImageFolder(train_dir, transform=transform)\n",
        "valid_dataset = ImageFolder(valid_dir, transform=transform)\n",
        "test_dataset = ImageFolder(test_dir, transform=transform)\n",
        "\n",
        "# Create data loaders\n",
        "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
        "valid_loader = DataLoader(valid_dataset, batch_size=32, shuffle=False)\n",
        "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)\n",
        "\n",
        "# Class names\n",
        "classes = train_dataset.classes\n",
        "print(\"Train Classes:\", classes)\n",
        "classes1 = valid_dataset.classes\n",
        "print(\"valid Classes:\", classes1)\n",
        "classes2 = test_dataset.classes\n",
        "print(\"test Classes:\", classes2)"
      ],
      "metadata": {
        "id": "vQu2iVQK8iEl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#visualize augmented images from training dataset.\n",
        "def imshow(img):\n",
        "    img = img.numpy().transpose((1, 2, 0))\n",
        "    img = img * 0.5 + 0.5  # unnormalize\n",
        "    plt.imshow(img)\n",
        "    plt.axis('off')\n",
        "    plt.show()\n",
        "\n",
        "# Visualize one batch\n",
        "data_iter = iter(train_loader)\n",
        "images, labels = next(data_iter)\n",
        "\n",
        "for i in range(5):\n",
        "    imshow(images[i])"
      ],
      "metadata": {
        "id": "1v9BOLwEAgLD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "WcLsFzUFOH7X"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#visualize augmented images from valid dataset.\n",
        "def imshow(img):\n",
        "    img = img.numpy().transpose((1, 2, 0))\n",
        "    img = img * 0.5 + 0.5  # unnormalize\n",
        "    plt.imshow(img)\n",
        "    plt.axis('off')\n",
        "    plt.show()\n",
        "\n",
        "# Visualize one batch\n",
        "data_iter = iter(valid_loader)\n",
        "images, labels = next(data_iter)\n",
        "\n",
        "for i in range(5):\n",
        "    imshow(images[i])"
      ],
      "metadata": {
        "id": "JUED_HcEBzVF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "JT3TtJ0aOUer"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#visualize augmented images from test dataset.\n",
        "def imshow(img):\n",
        "    img = img.numpy().transpose((1, 2, 0))\n",
        "    img = img * 0.5 + 0.5  # unnormalize\n",
        "    plt.imshow(img)\n",
        "    plt.axis('off')\n",
        "    plt.show()\n",
        "\n",
        "# Visualize one batch\n",
        "data_iter = iter(test_loader)\n",
        "images, labels = next(data_iter)\n",
        "\n",
        "for i in range(5):\n",
        "    imshow(images[i])"
      ],
      "metadata": {
        "id": "KIzCPxpVB7nv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "v5ScqwgGpuc_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8-3ctO27W2i1"
      },
      "outputs": [],
      "source": [
        "import torchvision\n",
        "\n",
        "train_set = torchvision.datasets.ImageFolder(data_dir.joinpath('train'), transform=transform)\n",
        "train_set.transform\n",
        "val_set = torchvision.datasets.ImageFolder(data_dir.joinpath('valid'), transform=transform)\n",
        "val_set.transform\n",
        "test_set = torchvision.datasets.ImageFolder(data_dir.joinpath('test'), transform=transform)\n",
        "test_set.transform"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_set"
      ],
      "metadata": {
        "id": "j_bqbIk579pQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "val_set"
      ],
      "metadata": {
        "id": "ZoLQzkTp8oAJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_set"
      ],
      "metadata": {
        "id": "XxHhAysI8r4T"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "wrE6kLgk8vHI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qQ2eoeHfW2dh"
      },
      "outputs": [],
      "source": [
        "img, label = train_set[1000]\n",
        "plt.imshow(img.permute(1,2,0))"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "UniX3mDh8u88"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "batch_size = 64\n",
        "train_loader = torch.utils.data.DataLoader(train_set, batch_size=batch_size, shuffle=True, num_workers=2, pin_memory=True)\n",
        "val_loader = torch.utils.data.DataLoader(val_set, batch_size=batch_size, shuffle=True, num_workers=2, pin_memory=True)\n",
        "test_loader = torch.utils.data.DataLoader(test_set,batch_size=batch_size, shuffle=True,num_workers=2, pin_memory=True)"
      ],
      "metadata": {
        "id": "_Tw8QBLFqV3F"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "next(iter(train_loader))[0].shape"
      ],
      "metadata": {
        "id": "q7-wvt3e9oPQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "next(iter(test_loader))[0].shape"
      ],
      "metadata": {
        "id": "pnnS0iXfqVvv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "next(iter(val_loader))[0].shape"
      ],
      "metadata": {
        "id": "UZfulps89Y3e"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Ow-1k5QW-whi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Model building\n",
        "#### Custom CNN"
      ],
      "metadata": {
        "id": "W8dBeTjQidWU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class CNNModel(nn.Module):\n",
        "  def __init__(self):\n",
        "    super(CNNModel, self).__init__()\n",
        "\n",
        "    #Conv 1: 224x224 input\n",
        "    self.conv1 = nn.Conv2d(in_channels=3, out_channels=16, kernel_size=5, stride=1, padding=0) # (224-5+1)/1 = 220\n",
        "\n",
        "    #Max Pool 1: 220x220 input\n",
        "    self.maxpool1 = nn.MaxPool2d(kernel_size=2) # 220/2 = 110\n",
        "\n",
        "    #Conv 2: 110x110 input\n",
        "    self.conv2 = nn.Conv2d(in_channels=16, out_channels=32, kernel_size=5, stride=1, padding=0) # (110-5+1)/1 = 106\n",
        "\n",
        "    #Max Pool 2: 106x106 input\n",
        "    self.maxpool2 = nn.MaxPool2d(kernel_size=2) # 106/2 = 53\n",
        "\n",
        "    #Conv 3: 53x53 input\n",
        "    self.conv3 = nn.Conv2d(in_channels=32, out_channels=64, kernel_size=5, stride=1, padding=0) # (53-5+1)/1 = 49\n",
        "\n",
        "    #Max Pool 3: 49x49 input\n",
        "    self.maxpool3 = nn.MaxPool2d(kernel_size=2) # 49/2 = 24 (floor)\n",
        "\n",
        "    #Conv 4: 24x24 input\n",
        "    self.conv4 = nn.Conv2d(in_channels=64, out_channels=128, kernel_size=5, stride=1, padding=0) # (24-5+1)/1 = 20\n",
        "\n",
        "    #Max Pool 4: 20x20 input\n",
        "    self.maxpool4 = nn.MaxPool2d(kernel_size=2) # 20/2 = 10\n",
        "\n",
        "    #Activation Function\n",
        "    self.leakyRelu = nn.LeakyReLU()\n",
        "\n",
        "    # Calculate flattened size dynamically\n",
        "    self._to_linear = 128 * 10 * 10 # Output channels of last conv * final spatial dimensions\n",
        "\n",
        "    #Fully connected Layer 1\n",
        "    self.fc1 = nn.Linear(self._to_linear, 1024)\n",
        "\n",
        "    #Fully connected Layer 2\n",
        "    self.fc2 = nn.Linear(1024, len(classes)) # Use len(classes) for the number of output classes\n",
        "\n",
        "  def forward(self, x):\n",
        "    #Layer 1\n",
        "    out = self.leakyRelu(self.conv1(x))\n",
        "    out = self.maxpool1(out)\n",
        "\n",
        "    #Layer 2\n",
        "    out = self.leakyRelu(self.conv2(out))\n",
        "    out = self.maxpool2(out)\n",
        "\n",
        "    #Layer 3\n",
        "    out = self.leakyRelu(self.conv3(out))\n",
        "    out = self.maxpool3(out)\n",
        "\n",
        "    #Layer 4\n",
        "    out = self.leakyRelu(self.conv4(out))\n",
        "    out = self.maxpool4(out)\n",
        "\n",
        "    #flatten\n",
        "    out = out.view(out.size(0), -1)\n",
        "\n",
        "    # Linear Function\n",
        "    out = self.leakyRelu(self.fc1(out))\n",
        "    out = self.fc2(out)\n",
        "\n",
        "    return out"
      ],
      "metadata": {
        "id": "i_E_vpqm-xWd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "hWGP9Cqk-wRt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Training CNN\n",
        "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "\n",
        "# Class names (moved from cell vQu2iVQK8iEl)\n",
        "train_dataset = ImageFolder(\"/content/drive/MyDrive/Tumour/train\", transform=transform) # Assuming transform is defined\n",
        "classes = train_dataset.classes\n",
        "\n",
        "\n",
        "num_epochs = 10\n",
        "model = CNNModel()\n",
        "error = nn.CrossEntropyLoss()\n",
        "\n",
        "lr = 0.001\n",
        "optimizer = optim.Adam(model.parameters(), lr=lr)\n",
        "scheduler = ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=5)"
      ],
      "metadata": {
        "id": "3sQQlSYw_B3F"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "yDtXfqK5_Yvf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "train_loss_list = []\n",
        "val_loss_list = []\n",
        "test_loss_list = []\n",
        "accuracy_list = []\n",
        "\n",
        "# Move model and criterion to device\n",
        "model.to(device)\n",
        "error.to(device)\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    train_loss = 0\n",
        "    val_loss = 0\n",
        "    test_loss = 0\n",
        "    correct_train = 0\n",
        "    total_train = 0\n",
        "    correct_val = 0\n",
        "    total_val = 0\n",
        "    correct_test = 0\n",
        "    total_test = 0\n",
        "\n",
        "    # Training loop\n",
        "    model.train()\n",
        "    for i, (images, labels) in enumerate(train_loader):\n",
        "        images, labels = images.to(device), labels.to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(images)\n",
        "        loss = error(outputs, labels)\n",
        "        train_loss += loss.item() * images.size(0) # Multiply by batch size for correct average later\n",
        "\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        # print(\"Loss in Iteration:\", i, \":\", loss.item()) # Optional: print loss per iteration\n",
        "\n",
        "        _, predicted = torch.max(outputs.data, 1)\n",
        "        total_train += labels.size(0)\n",
        "        correct_train += (predicted == labels).sum().item()\n",
        "\n",
        "\n",
        "    # Validation loop\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        for images, labels in val_loader:\n",
        "            images, labels = images.to(device), labels.to(device)\n",
        "            outputs = model(images)\n",
        "            v_loss = error(outputs, labels)\n",
        "            val_loss += v_loss.item() * images.size(0) # Multiply by batch size\n",
        "\n",
        "            _, predicted = torch.max(outputs.data, 1)\n",
        "            total_val += labels.size(0)\n",
        "            correct_val += (predicted == labels).sum().item()\n",
        "\n",
        "    # Test loop\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        for images, labels in test_loader:\n",
        "            images, labels = images.to(device), labels.to(device)\n",
        "            outputs1 = model(images) # Use outputs1 for test\n",
        "            tes_loss = error(outputs1, labels) # Use outputs1 here\n",
        "            test_loss += tes_loss.item() * images.size(0) # Multiply by batch size\n",
        "\n",
        "            _, predicted1 = torch.max(outputs1.data, 1) # Use predicted1 for test\n",
        "            total_test += labels.size(0)\n",
        "            correct_test += (predicted1 == labels).sum().item()\n",
        "\n",
        "\n",
        "    # Calculate average losses and accuracies\n",
        "    avg_train_loss = train_loss / total_train\n",
        "    avg_val_loss = val_loss / total_val\n",
        "    avg_test_loss = test_loss / total_test\n",
        "\n",
        "    train_accuracy = 100 * correct_train / total_train\n",
        "    val_accuracy = 100 * correct_val / total_val\n",
        "    test_accuracy = 100 * correct_test / total_test\n",
        "\n",
        "\n",
        "    # Append to lists\n",
        "    train_loss_list.append(avg_train_loss)\n",
        "    val_loss_list.append(avg_val_loss)\n",
        "    test_loss_list.append(avg_test_loss)\n",
        "    accuracy_list.append(train_accuracy) # Store train, val, test accuracy for the epoch\n",
        "    accuracy_list.append(val_accuracy)\n",
        "    accuracy_list.append(test_accuracy)\n",
        "\n",
        "\n",
        "    # Step the scheduler based on validation loss\n",
        "    scheduler.step(avg_val_loss)\n",
        "\n",
        "    print(f\"Epoch: {epoch+1}/{num_epochs}, Train Loss: {avg_train_loss:.4f}, Valid Loss: {avg_val_loss:.4f}, Test Loss: {avg_test_loss:.4f}, Train Acc: {train_accuracy:.2f}%, Valid Acc: {val_accuracy:.2f}%, Test Acc: {test_accuracy:.2f}%\")"
      ],
      "metadata": {
        "id": "wlsmIX41_aCB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#In the case of CNN i got the test accc and valid acc is 73%. which is a littile bit low and losses are there. So pretrained the model"
      ],
      "metadata": {
        "id": "zEZQc65TJJjK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Assuming 'classes' is defined, e.g., as a list of class names\n",
        "# For demonstration purposes, let's define a dummy 'classes'\n",
        "classes = ['class','class1', 'class2', 'class3']"
      ],
      "metadata": {
        "id": "0JuOAJTTrjwo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "num_classes=len(classes)"
      ],
      "metadata": {
        "id": "WJL-a1ajO2-i"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "RjeUDIJyx1Jp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Transfer Learning\n",
        "\n",
        "###Load pretrained models (Example : ResNet50)\n"
      ],
      "metadata": {
        "id": "OzyqLMzCTPLu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "resnet_model = models.resnet50(pretrained=True)\n",
        "\n",
        "# Freeze all layers\n",
        "for param in resnet_model.parameters():\n",
        "    param.requires_grad = False\n",
        "\n",
        "# Replace the final layer\n",
        "resnet_model.fc = nn.Sequential(\n",
        "    nn.Linear(resnet_model.fc.in_features, 512),\n",
        "    nn.ReLU(),\n",
        "    nn.Dropout(0.4),\n",
        "    nn.Linear(512, num_classes)\n",
        ")\n",
        "\n",
        "resnet_model = resnet_model.to(device)"
      ],
      "metadata": {
        "id": "PePYmtK9x1V6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.Adam(resnet_model.fc.parameters(), lr=0.001)"
      ],
      "metadata": {
        "id": "TOKd1urCx1Yl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Train the Model\n",
        "def train_model(model, train_loader, val_loader,test_loader, epochs=10):\n",
        "    model.train()\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        running_loss = 0.0\n",
        "        correct = 0\n",
        "        total = 0\n",
        "\n",
        "        for inputs, labels in train_loader:\n",
        "            inputs, labels = inputs.to(device), labels.to(device)\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            outputs = model(inputs)\n",
        "            loss = criterion(outputs, labels)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            running_loss += loss.item()\n",
        "            _, predicted = torch.max(outputs.data, 1)\n",
        "            total += labels.size(0)\n",
        "            correct += (predicted == labels).sum().item()\n",
        "\n",
        "        acc = 100 * correct / total\n",
        "        print(f\"Epoch {epoch+1}/{epochs}, Loss: {running_loss:.4f}, Accuracy: {acc:.2f}%\")"
      ],
      "metadata": {
        "id": "1SNgpsZRx1a6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_model(resnet_model, train_loader, valid_loader,test_loader, epochs=10)"
      ],
      "metadata": {
        "id": "dKvk74nGx1dM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#In the case of resnet 50 i got the  Accuracy  84% and loss is really very less here. Pretrained the model to get very high accuracy"
      ],
      "metadata": {
        "id": "TMhGs65f-dIf"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "4zjhImkATsZY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "######Model Training\n",
        "\n",
        "\n",
        "#####Train both custom CNN and transfer learning models.\n"
      ],
      "metadata": {
        "id": "jyoiLD5vs3AO"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "NoFCJkoRUfSF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Define Custom CNN Architecture\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "class CustomCNN(nn.Module):\n",
        "    def __init__(self, num_classes):\n",
        "        super(CustomCNN, self).__init__()\n",
        "\n",
        "        self.conv_block1 = nn.Sequential(\n",
        "            nn.Conv2d(3, 32, kernel_size=3, padding=1),\n",
        "            nn.BatchNorm2d(32),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(2)\n",
        "        )\n",
        "\n",
        "        self.conv_block2 = nn.Sequential(\n",
        "            nn.Conv2d(32, 64, kernel_size=3, padding=1),\n",
        "            nn.BatchNorm2d(64),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(2)\n",
        "        )\n",
        "\n",
        "        self.conv_block3 = nn.Sequential(\n",
        "            nn.Conv2d(64, 128, kernel_size=3, padding=1),\n",
        "            nn.BatchNorm2d(128),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(2)\n",
        "        )\n",
        "\n",
        "        self.dropout = nn.Dropout(0.5)\n",
        "        self.fc1 = nn.Linear(128 * 28 * 28, 256)  # assuming input image size is 224x224\n",
        "        self.fc2 = nn.Linear(256, num_classes)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.conv_block1(x)\n",
        "        x = self.conv_block2(x)\n",
        "        x = self.conv_block3(x)\n",
        "        x = x.view(x.size(0), -1)  # Flatten\n",
        "        x = self.dropout(F.relu(self.fc1(x)))\n",
        "        x = self.fc2(x)\n",
        "        return x"
      ],
      "metadata": {
        "id": "8vOpBrLb1YS4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Unified Training Function (for Custom CNN or ResNet)\n",
        "\n",
        "import torch\n",
        "import numpy as np\n",
        "import copy\n",
        "import os\n",
        "\n",
        "class EarlyStopping:\n",
        "    def __init__(self, patience=3, delta=0.001):\n",
        "        self.patience = patience\n",
        "        self.delta = delta\n",
        "        self.counter = 0\n",
        "        self.best_loss = np.inf # Changed np.Inf to np.inf\n",
        "        self.early_stop = False\n",
        "\n",
        "    def __call__(self, val_loss):\n",
        "        if val_loss < self.best_loss - self.delta:\n",
        "            self.best_loss = val_loss\n",
        "            self.counter = 0\n",
        "        else:\n",
        "            self.counter += 1\n",
        "            print(\"EarlyStopping counter: {self.counter} / {self.patience}\")\n",
        "            if self.counter >= self.patience:\n",
        "                self.early_stop = True\n",
        "\n",
        "def save_checkpoint(model, path='best_model.pth'):\n",
        "    torch.save(model.state_dict(), path)\n",
        "    print(f\"Model saved at {path}\")\n",
        "\n",
        "def train_with_callbacks(model, criterion, optimizer, train_loader, val_loader, device,\n",
        "                         num_epochs=25, early_stopping=None, checkpoint_path='best_model.pth'):\n",
        "\n",
        "    best_model_wts = copy.deepcopy(model.state_dict())\n",
        "    best_val_loss = float('inf')\n",
        "    best_test_loss = float('inf')  # Initialize best_test_loss\n",
        "\n",
        "    train_loss_history = []\n",
        "    val_loss_history = []\n",
        "    test_loss_history =[]\n",
        "    train_acc_history = []\n",
        "    val_acc_history = []\n",
        "    test_acc_history = []\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        print(f\"\\n Epoch {epoch+1}/{num_epochs}\")\n",
        "\n",
        "        model.train()\n",
        "        train_loss, correct, total = 0.0, 0, 0\n",
        "        for inputs, labels in train_loader:\n",
        "            inputs, labels = inputs.to(device), labels.to(device)\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            outputs = model(inputs)\n",
        "            loss = criterion(outputs, labels)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            train_loss += loss.item() * inputs.size(0)\n",
        "            _, preds = torch.max(outputs, 1)\n",
        "            correct += (preds == labels).sum().item()\n",
        "            total += labels.size(0)\n",
        "\n",
        "        epoch_train_loss = train_loss / total\n",
        "        epoch_train_acc = correct / total\n",
        "        train_loss_history.append(epoch_train_loss)\n",
        "        train_acc_history.append(epoch_train_acc)\n",
        "\n",
        "        # Validation phase\n",
        "        model.eval()\n",
        "        val_loss, correct, total = 0.0, 0, 0\n",
        "        with torch.no_grad():\n",
        "            for inputs, labels in val_loader:\n",
        "                inputs, labels = inputs.to(device), labels.to(device)\n",
        "                outputs = model(inputs)\n",
        "                loss = criterion(outputs, labels)\n",
        "\n",
        "                val_loss += loss.item() * inputs.size(0)\n",
        "                _, preds = torch.max(outputs, 1)\n",
        "                correct += (preds == labels).sum().item()\n",
        "                total += labels.size(0)\n",
        "\n",
        "        epoch_val_loss = val_loss / total\n",
        "        epoch_val_acc = correct / total\n",
        "        val_loss_history.append(epoch_val_loss)\n",
        "        val_acc_history.append(epoch_val_acc)\n",
        "\n",
        "\n",
        "\n",
        "        # Test phase\n",
        "        model.eval()\n",
        "        test_loss, correct, total = 0.0, 0, 0\n",
        "        with torch.no_grad():\n",
        "            for inputs, labels in test_loader: # Changed from val_loader to test_loader\n",
        "                inputs, labels = inputs.to(device), labels.to(device)\n",
        "                outputs = model(inputs)\n",
        "                loss = criterion(outputs, labels)\n",
        "\n",
        "                test_loss += loss.item() * inputs.size(0)\n",
        "                _, preds = torch.max(outputs, 1)\n",
        "                correct += (preds == labels).sum().item()\n",
        "                total += labels.size(0)\n",
        "\n",
        "        epoch_test_loss = test_loss / total\n",
        "        epoch_test_acc = correct / total\n",
        "        test_loss_history.append(epoch_test_loss) # Corrected append\n",
        "        test_acc_history.append(epoch_test_acc) # Corrected append\n",
        "\n",
        "        print(f\"Train Loss: {epoch_train_loss:.4f}, Acc: {epoch_train_acc:.4f}\")\n",
        "        print(f\"Val   Loss: {epoch_val_loss:.4f}, Acc: {epoch_val_acc:.4f}\")\n",
        "        print(f\"Test   Loss: {epoch_test_loss:.4f}, Acc: {epoch_test_acc:.4f}\")\n",
        "\n",
        "\n",
        "        # Checkpoint\n",
        "        if epoch_val_loss < best_val_loss:\n",
        "            best_val_loss = epoch_val_loss\n",
        "            best_model_wts = copy.deepcopy(model.state_dict())\n",
        "            save_checkpoint(model, checkpoint_path)\n",
        "        if epoch_test_loss < best_test_loss:\n",
        "            best_test_loss = epoch_test_loss\n",
        "            best_model_wts = copy.deepcopy(model.state_dict())\n",
        "            save_checkpoint(model, checkpoint_path)\n",
        "\n",
        "\n",
        "        # Early Stopping\n",
        "        if early_stopping:\n",
        "            early_stopping(epoch_val_loss)\n",
        "            if early_stopping.early_stop:\n",
        "                print(\"Early stopping triggered based on validation loss.\")\n",
        "                break\n",
        "        if early_stopping:\n",
        "            early_stopping(epoch_test_loss)\n",
        "            if early_stopping.early_stop:\n",
        "                print(\"Early stopping triggered based on test loss.\")\n",
        "                break\n",
        "\n",
        "\n",
        "    model.load_state_dict(best_model_wts)\n",
        "    return model, train_loss_history, val_loss_history,test_loss_history, train_acc_history, val_acc_history,test_acc_history"
      ],
      "metadata": {
        "id": "iKLF_vJ1RsKn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Train Both Models\n",
        "# Custom CNN (Example)\n",
        "#num_classes = len(classes)  # classes = train_dataset.classes\n",
        "#cnn_model = CNNModel(num_classes).to(device)\n",
        "cnn_model = CustomCNN(num_classes).to(device)\n",
        "\n",
        "optimizer_cnn = torch.optim.Adam(cnn_model.parameters(), lr=1e-4)\n",
        "early_stopper = EarlyStopping(patience=5)\n",
        "\n",
        "cnn_model, cnn_train_loss, cnn_val_loss,cnn_test_loss, cnn_train_acc, cnn_val_acc,cnn_test_acc = train_with_callbacks(\n",
        "    cnn_model,\n",
        "    nn.CrossEntropyLoss(),\n",
        "    optimizer_cnn,\n",
        "    train_loader,\n",
        "    valid_loader,\n",
        "    device,\n",
        "    num_epochs=10,\n",
        "    early_stopping=early_stopper,\n",
        "    checkpoint_path='cnn_best_model.pth'\n",
        ")"
      ],
      "metadata": {
        "id": "EjtWkGJPRp7U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "evr-iutqRrZ6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "x3p6qVncuMJg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torchvision import models, transforms\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "\n",
        "# Assuming you have a custom dataset and loaders, these are placeholders\n",
        "# for demonstration. Replace them with your actual dataset and loaders.\n",
        "class CustomDataset(Dataset):\n",
        "    def __init__(self, data, labels, transform=None):\n",
        "        self.data = data\n",
        "        self.labels = labels\n",
        "        self.transform = transform\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        image = self.data[idx]\n",
        "        label = self.labels[idx]\n",
        "        if self.transform:\n",
        "            image = self.transform(image)\n",
        "        return image, label\n",
        "\n",
        "# Placeholder for your data and loaders\n",
        "# You should define your actual train_dataset, valid_dataset, etc.\n",
        "# For example:\n",
        "# from torchvision.datasets import ImageFolder\n",
        "# from torch.utils.data import DataLoader\n",
        "# train_dataset = ImageFolder('path/to/train', transform=your_transform)\n",
        "# train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
        "# num_classes = len(train_dataset.classes)\n",
        "num_classes = 10 # Example number of classes\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# ---\n",
        "# 1. Load pretrained ResNet50\n",
        "# ---\n",
        "# Use a more modern weight as it often performs better\n",
        "weights = models.ResNet50_Weights.IMAGENET1K_V2\n",
        "resnet = models.resnet50(weights=weights)\n",
        "\n",
        "# ---\n",
        "# 2. Adjusting the fine-tuning strategy\n",
        "# ---\n",
        "# Freeze all layers first\n",
        "for param in resnet.parameters():\n",
        "    param.requires_grad = False\n",
        "\n",
        "# Unfreeze the last two blocks (layer3 and layer4) for more effective fine-tuning.\n",
        "# This provides a better balance between feature reuse and task-specific learning.\n",
        "# Fine-tuning only layer4 and fc might not be enough to learn the new features well.\n",
        "for name, param in resnet.named_parameters():\n",
        "    if \"layer3\" in name or \"layer4\" in name or \"fc\" in name:\n",
        "        param.requires_grad = True\n",
        "\n",
        "# ---\n",
        "# 3. Replace final layer for custom classification\n",
        "# ---\n",
        "# This part is correct, but let's make it a more robust replacement using Sequential\n",
        "resnet.fc = nn.Sequential(\n",
        "    nn.Linear(resnet.fc.in_features, 512),\n",
        "    nn.ReLU(),\n",
        "    nn.Dropout(0.3),\n",
        "    nn.Linear(512, num_classes)\n",
        ")\n",
        "\n",
        "# ---\n",
        "# 4. Move model to GPU\n",
        "# ---\n",
        "resnet = resnet.to(device)\n",
        "\n",
        "# ---\n",
        "# 5. Use optimizer with a better approach\n",
        "# ---\n",
        "# Use a more sophisticated optimizer like AdamW which often performs better than Adam\n",
        "# and a lower learning rate for the frozen layers while a higher one for the new layers.\n",
        "# We also use a separate optimizer for the new layers to have more control.\n",
        "optimizer_resnet = optim.AdamW(\n",
        "    filter(lambda p: p.requires_grad, resnet.parameters()),\n",
        "    lr=1e-4,\n",
        "    weight_decay=1e-5\n",
        ")\n",
        "\n",
        "# ---\n",
        "# 6. Define loss function\n",
        "# ---\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "# ---\n",
        "# 7. Add learning rate scheduler\n",
        "# ---\n",
        "# Reduce the learning rate as training progresses to avoid overshooting the minimum.\n",
        "# This helps the model converge more smoothly and improves accuracy.\n",
        "scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer_resnet, 'max', patience=5, factor=0.1)\n",
        "\n",
        "# ---\n",
        "# 8. Define a robust training function with data augmentation\n",
        "# ---\n",
        "# Data augmentation is critical for preventing overfitting, especially with smaller datasets.\n",
        "train_transforms = transforms.Compose([\n",
        "    transforms.RandomResizedCrop(224),\n",
        "    transforms.RandomHorizontalFlip(),\n",
        "    transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
        "])\n",
        "\n",
        "val_test_transforms = transforms.Compose([\n",
        "    transforms.Resize(256),\n",
        "    transforms.CenterCrop(224),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
        "])\n",
        "\n",
        "# Your custom train_model function should be updated to include the scheduler logic.\n",
        "# The `train_model` function you're using is not provided, but here is an example\n",
        "# of how the training loop should be structured to use the scheduler.\n",
        "def improved_train_model(model, train_loader, val_loader, optimizer, criterion, scheduler, epochs, device):\n",
        "    best_val_accuracy = 0.0\n",
        "    for epoch in range(epochs):\n",
        "        model.train()\n",
        "        running_loss = 0.0\n",
        "        for inputs, labels in train_loader:\n",
        "            inputs, labels = inputs.to(device), labels.to(device)\n",
        "            optimizer.zero_grad()\n",
        "            outputs = model(inputs)\n",
        "            loss = criterion(outputs, labels)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            running_loss += loss.item()\n",
        "\n",
        "        # Validation phase\n",
        "        model.eval()\n",
        "        correct = 0\n",
        "        total = 0\n",
        "        with torch.no_grad():\n",
        "            for inputs, labels in val_loader:\n",
        "                inputs, labels = inputs.to(device), labels.to(device)\n",
        "                outputs = model(inputs)\n",
        "                _, predicted = torch.max(outputs.data, 1)\n",
        "                total += labels.size(0)\n",
        "                correct += (predicted == labels).sum().item()\n",
        "\n",
        "        val_accuracy = 100 * correct / total\n",
        "        print(f\"Epoch {epoch+1}/{epochs}, Loss: {running_loss/len(train_loader):.4f}, Validation Accuracy: {val_accuracy:.2f}%\")\n",
        "\n",
        "        # Step the scheduler based on validation accuracy\n",
        "        scheduler.step(val_accuracy)\n",
        "\n",
        "        # Save the best model\n",
        "        if val_accuracy > best_val_accuracy:\n",
        "            best_val_accuracy = val_accuracy\n",
        "            torch.save(model.state_dict(), 'resnet_best_model.pth')\n",
        "\n",
        "    return model"
      ],
      "metadata": {
        "id": "z_ZkjJ8AuMQ7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "improved_train_model(\n",
        "  model=resnet,\n",
        " train_loader=train_loader,\n",
        " val_loader=valid_loader,\n",
        "  optimizer=optimizer_resnet,\n",
        "  criterion=criterion,\n",
        "  scheduler=scheduler,\n",
        "  epochs=10,\n",
        "  device=device\n",
        ")"
      ],
      "metadata": {
        "id": "6oMJUwh11cBQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Pretrained model resnet50 i got 94% accuracy which is good. Here i got more accuracy uesing resnet50 compare to custom CNN"
      ],
      "metadata": {
        "id": "b9Rd9Df-S3SG"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "SKq9C8_nS1Zo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "####Model Evaluation\n",
        "\n",
        "\n",
        "###Evaluate models using metrics like accuracy, precision, recall, F1-score, and confusion matrix.\n"
      ],
      "metadata": {
        "id": "-n3HG1CGvwPz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix, classification_report\n",
        "\n",
        "def evaluate_model(model, data_loader, device):\n",
        "    model.eval()\n",
        "    all_preds = []\n",
        "    all_labels = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for inputs, labels in data_loader:\n",
        "            inputs, labels = inputs.to(device), labels.to(device)\n",
        "            outputs = model(inputs)\n",
        "            _, preds = torch.max(outputs, 1)\n",
        "            all_preds.extend(preds.cpu().numpy())\n",
        "            all_labels.extend(labels.cpu().numpy())\n",
        "\n",
        "    # Metrics\n",
        "    acc = accuracy_score(all_labels, all_preds)\n",
        "    precision = precision_score(all_labels, all_preds, average='weighted', zero_division=0)\n",
        "    recall = recall_score(all_labels, all_preds, average='weighted', zero_division=0)\n",
        "    f1 = f1_score(all_labels, all_preds, average='weighted', zero_division=0)\n",
        "\n",
        "    print(\"Accuracy:\", round(acc, 4))\n",
        "    print(\"Precision:\", round(precision, 4))\n",
        "    print(\"Recall:\", round(recall, 4))\n",
        "    print(\"F1 Score:\", round(f1, 4))\n",
        "    print(\"\\nConfusion Matrix:\\n\", confusion_matrix(all_labels, all_preds))\n",
        "    print(\"\\nClassification Report:\\n\", classification_report(all_labels, all_preds))"
      ],
      "metadata": {
        "id": "c6H7l0yzvv6-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "K_ERdXsMbQUB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "evaluate_model(model, test_loader,device)"
      ],
      "metadata": {
        "id": "Wk5u6043bS2T"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "evaluate_model(resnet, test_loader,device)"
      ],
      "metadata": {
        "id": "Xlr0w8auvvt9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Model Comparison\n",
        "\n",
        "\n",
        "# Compare results of custom CNN vs pretrained models.\n",
        "\n",
        "\n",
        "#Identify the most accurate, efficient, and reliable model for deployment.\n"
      ],
      "metadata": {
        "id": "1tH2n-lUyKXr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "models = ['Custom CNN', 'ResNet50']\n",
        "accuracies = [73, 96]\n",
        "f1_scores = [70, 95]\n",
        "\n",
        "plt.figure(figsize=(8,5))\n",
        "bar_width = 0.35\n",
        "x = range(len(models))\n",
        "\n",
        "plt.bar(x, accuracies, width=bar_width, label='Accuracy')\n",
        "plt.bar([i + bar_width for i in x], f1_scores, width=bar_width, label='F1-score')\n",
        "plt.xticks([i + bar_width / 2 for i in x], models)\n",
        "plt.ylabel('Score (%)')\n",
        "plt.title('Model Comparison')\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "UqMlnJH0yJAA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The graph titled \"Model Comparison\" showcases the performance of two deep learning models—Custom CNN and ResNet50—based on accuracy and F1-score metrics. Custom CNN reaches approximately 73% accuracy and70% F1-score, while ResNet50 significantly outperforms it with about 96% accuracy and 95% F1-score. This visualization clearly highlights ResNet50’s superior performance, making it the stronger choice for this classification task."
      ],
      "metadata": {
        "id": "0FvUgLbUcmHp"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "JFIQonhWGa59"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Duplicate Values"
      ],
      "metadata": {
        "id": "35m5QtbWiB9F"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Dataset Duplicate Value Count"
      ],
      "metadata": {
        "id": "1sLdpKYkmox0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Missing Values/Null Values"
      ],
      "metadata": {
        "id": "PoPl-ycgm1ru"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Missing Values/Null Values Count"
      ],
      "metadata": {
        "id": "GgHWkxvamxVg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Visualizing the missing values"
      ],
      "metadata": {
        "id": "3q5wnI3om9sJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### What did you know about your dataset?"
      ],
      "metadata": {
        "id": "H0kj-8xxnORC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here"
      ],
      "metadata": {
        "id": "gfoNAAC-nUe_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***2. Understanding Your Variables***"
      ],
      "metadata": {
        "id": "nA9Y7ga8ng1Z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Dataset Columns"
      ],
      "metadata": {
        "id": "j7xfkqrt5Ag5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Dataset Describe"
      ],
      "metadata": {
        "id": "DnOaZdaE5Q5t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Variables Description"
      ],
      "metadata": {
        "id": "PBTbrJXOngz2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here"
      ],
      "metadata": {
        "id": "aJV4KIxSnxay"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Check Unique Values for each variable."
      ],
      "metadata": {
        "id": "u3PMJOP6ngxN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Check Unique Values for each variable."
      ],
      "metadata": {
        "id": "zms12Yq5n-jE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3. ***Data Wrangling***"
      ],
      "metadata": {
        "id": "dauF4eBmngu3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Data Wrangling Code"
      ],
      "metadata": {
        "id": "bKJF3rekwFvQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Write your code to make your dataset analysis ready."
      ],
      "metadata": {
        "id": "wk-9a2fpoLcV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### What all manipulations have you done and insights you found?"
      ],
      "metadata": {
        "id": "MSa1f5Uengrz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here."
      ],
      "metadata": {
        "id": "LbyXE7I1olp8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***4. Data Vizualization, Storytelling & Experimenting with charts : Understand the relationships between variables***"
      ],
      "metadata": {
        "id": "GF8Ens_Soomf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 1"
      ],
      "metadata": {
        "id": "0wOQAZs5pc--"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 1 visualization code"
      ],
      "metadata": {
        "id": "7v_ESjsspbW7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "K5QZ13OEpz2H"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here."
      ],
      "metadata": {
        "id": "XESiWehPqBRc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "lQ7QKXXCp7Bj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here"
      ],
      "metadata": {
        "id": "C_j1G7yiqdRP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "448CDAPjqfQr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here"
      ],
      "metadata": {
        "id": "3cspy4FjqxJW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 2"
      ],
      "metadata": {
        "id": "KSlN3yHqYklG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 2 visualization code"
      ],
      "metadata": {
        "id": "R4YgtaqtYklH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "t6dVpIINYklI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here."
      ],
      "metadata": {
        "id": "5aaW0BYyYklI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "ijmpgYnKYklI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here"
      ],
      "metadata": {
        "id": "PSx9atu2YklI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "-JiQyfWJYklI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here"
      ],
      "metadata": {
        "id": "BcBbebzrYklV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 3"
      ],
      "metadata": {
        "id": "EM7whBJCYoAo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 3 visualization code"
      ],
      "metadata": {
        "id": "t6GMdE67YoAp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "fge-S5ZAYoAp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here."
      ],
      "metadata": {
        "id": "5dBItgRVYoAp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "85gYPyotYoAp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here"
      ],
      "metadata": {
        "id": "4jstXR6OYoAp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "RoGjAbkUYoAp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here"
      ],
      "metadata": {
        "id": "zfJ8IqMcYoAp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 4"
      ],
      "metadata": {
        "id": "4Of9eVA-YrdM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 4 visualization code"
      ],
      "metadata": {
        "id": "irlUoxc8YrdO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "iky9q4vBYrdO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here."
      ],
      "metadata": {
        "id": "aJRCwT6DYrdO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "F6T5p64dYrdO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here"
      ],
      "metadata": {
        "id": "Xx8WAJvtYrdO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "y-Ehk30pYrdP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here"
      ],
      "metadata": {
        "id": "jLNxxz7MYrdP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 5"
      ],
      "metadata": {
        "id": "bamQiAODYuh1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 5 visualization code"
      ],
      "metadata": {
        "id": "TIJwrbroYuh3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "QHF8YVU7Yuh3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here."
      ],
      "metadata": {
        "id": "dcxuIMRPYuh3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "GwzvFGzlYuh3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here"
      ],
      "metadata": {
        "id": "uyqkiB8YYuh3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "qYpmQ266Yuh3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here"
      ],
      "metadata": {
        "id": "_WtzZ_hCYuh4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 6"
      ],
      "metadata": {
        "id": "OH-pJp9IphqM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 6 visualization code"
      ],
      "metadata": {
        "id": "kuRf4wtuphqN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "bbFf2-_FphqN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here."
      ],
      "metadata": {
        "id": "loh7H2nzphqN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "_ouA3fa0phqN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here"
      ],
      "metadata": {
        "id": "VECbqPI7phqN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "Seke61FWphqN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here"
      ],
      "metadata": {
        "id": "DW4_bGpfphqN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 7"
      ],
      "metadata": {
        "id": "PIIx-8_IphqN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 7 visualization code"
      ],
      "metadata": {
        "id": "lqAIGUfyphqO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "t27r6nlMphqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here."
      ],
      "metadata": {
        "id": "iv6ro40sphqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "r2jJGEOYphqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here"
      ],
      "metadata": {
        "id": "Po6ZPi4hphqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "b0JNsNcRphqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here"
      ],
      "metadata": {
        "id": "xvSq8iUTphqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 8"
      ],
      "metadata": {
        "id": "BZR9WyysphqO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 8 visualization code"
      ],
      "metadata": {
        "id": "TdPTWpAVphqO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "jj7wYXLtphqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here."
      ],
      "metadata": {
        "id": "Ob8u6rCTphqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "eZrbJ2SmphqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here"
      ],
      "metadata": {
        "id": "mZtgC_hjphqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "rFu4xreNphqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here"
      ],
      "metadata": {
        "id": "ey_0qi68phqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 9"
      ],
      "metadata": {
        "id": "YJ55k-q6phqO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 9 visualization code"
      ],
      "metadata": {
        "id": "B2aS4O1ophqO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "gCFgpxoyphqP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here."
      ],
      "metadata": {
        "id": "TVxDimi2phqP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "OVtJsKN_phqQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here"
      ],
      "metadata": {
        "id": "ngGi97qjphqQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "lssrdh5qphqQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here"
      ],
      "metadata": {
        "id": "tBpY5ekJphqQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 10"
      ],
      "metadata": {
        "id": "U2RJ9gkRphqQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 10 visualization code"
      ],
      "metadata": {
        "id": "GM7a4YP4phqQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "1M8mcRywphqQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here."
      ],
      "metadata": {
        "id": "8agQvks0phqQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "tgIPom80phqQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here"
      ],
      "metadata": {
        "id": "Qp13pnNzphqQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "JMzcOPDDphqR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here"
      ],
      "metadata": {
        "id": "R4Ka1PC2phqR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 11"
      ],
      "metadata": {
        "id": "x-EpHcCOp1ci"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 11 visualization code"
      ],
      "metadata": {
        "id": "mAQTIvtqp1cj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "X_VqEhTip1ck"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here."
      ],
      "metadata": {
        "id": "-vsMzt_np1ck"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "8zGJKyg5p1ck"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here"
      ],
      "metadata": {
        "id": "ZYdMsrqVp1ck"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "PVzmfK_Ep1ck"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here"
      ],
      "metadata": {
        "id": "druuKYZpp1ck"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 12"
      ],
      "metadata": {
        "id": "n3dbpmDWp1ck"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 12 visualization code"
      ],
      "metadata": {
        "id": "bwevp1tKp1ck"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "ylSl6qgtp1ck"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here."
      ],
      "metadata": {
        "id": "m2xqNkiQp1ck"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "ZWILFDl5p1ck"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here"
      ],
      "metadata": {
        "id": "x-lUsV2mp1ck"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "M7G43BXep1ck"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here"
      ],
      "metadata": {
        "id": "5wwDJXsLp1cl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 13"
      ],
      "metadata": {
        "id": "Ag9LCva-p1cl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 13 visualization code"
      ],
      "metadata": {
        "id": "EUfxeq9-p1cl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "E6MkPsBcp1cl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here."
      ],
      "metadata": {
        "id": "V22bRsFWp1cl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "2cELzS2fp1cl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here"
      ],
      "metadata": {
        "id": "ozQPc2_Ip1cl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "3MPXvC8up1cl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here"
      ],
      "metadata": {
        "id": "GL8l1tdLp1cl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 14 - Correlation Heatmap"
      ],
      "metadata": {
        "id": "NC_X3p0fY2L0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Correlation Heatmap visualization code"
      ],
      "metadata": {
        "id": "xyC9zolEZNRQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "UV0SzAkaZNRQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here."
      ],
      "metadata": {
        "id": "DVPuT8LYZNRQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "YPEH6qLeZNRQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here"
      ],
      "metadata": {
        "id": "bfSqtnDqZNRR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 15 - Pair Plot"
      ],
      "metadata": {
        "id": "q29F0dvdveiT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Pair Plot visualization code"
      ],
      "metadata": {
        "id": "o58-TEIhveiU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "EXh0U9oCveiU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here."
      ],
      "metadata": {
        "id": "eMmPjTByveiU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "22aHeOlLveiV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here"
      ],
      "metadata": {
        "id": "uPQ8RGwHveiV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***5. Hypothesis Testing***"
      ],
      "metadata": {
        "id": "g-ATYxFrGrvw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Based on your chart experiments, define three hypothetical statements from the dataset. In the next three questions, perform hypothesis testing to obtain final conclusion about the statements through your code and statistical testing."
      ],
      "metadata": {
        "id": "Yfr_Vlr8HBkt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here."
      ],
      "metadata": {
        "id": "-7MS06SUHkB-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Hypothetical Statement - 1"
      ],
      "metadata": {
        "id": "8yEUt7NnHlrM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1. State Your research hypothesis as a null hypothesis and alternate hypothesis."
      ],
      "metadata": {
        "id": "tEA2Xm5dHt1r"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here."
      ],
      "metadata": {
        "id": "HI9ZP0laH0D-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2. Perform an appropriate statistical test."
      ],
      "metadata": {
        "id": "I79__PHVH19G"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Perform Statistical Test to obtain P-Value"
      ],
      "metadata": {
        "id": "oZrfquKtyian"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which statistical test have you done to obtain P-Value?"
      ],
      "metadata": {
        "id": "Ou-I18pAyIpj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here."
      ],
      "metadata": {
        "id": "s2U0kk00ygSB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Why did you choose the specific statistical test?"
      ],
      "metadata": {
        "id": "fF3858GYyt-u"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here."
      ],
      "metadata": {
        "id": "HO4K0gP5y3B4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Hypothetical Statement - 2"
      ],
      "metadata": {
        "id": "4_0_7-oCpUZd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1. State Your research hypothesis as a null hypothesis and alternate hypothesis."
      ],
      "metadata": {
        "id": "hwyV_J3ipUZe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here."
      ],
      "metadata": {
        "id": "FnpLGJ-4pUZe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2. Perform an appropriate statistical test."
      ],
      "metadata": {
        "id": "3yB-zSqbpUZe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Perform Statistical Test to obtain P-Value"
      ],
      "metadata": {
        "id": "sWxdNTXNpUZe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which statistical test have you done to obtain P-Value?"
      ],
      "metadata": {
        "id": "dEUvejAfpUZe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here."
      ],
      "metadata": {
        "id": "oLDrPz7HpUZf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Why did you choose the specific statistical test?"
      ],
      "metadata": {
        "id": "Fd15vwWVpUZf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here."
      ],
      "metadata": {
        "id": "4xOGYyiBpUZf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Hypothetical Statement - 3"
      ],
      "metadata": {
        "id": "bn_IUdTipZyH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1. State Your research hypothesis as a null hypothesis and alternate hypothesis."
      ],
      "metadata": {
        "id": "49K5P_iCpZyH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here."
      ],
      "metadata": {
        "id": "7gWI5rT9pZyH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2. Perform an appropriate statistical test."
      ],
      "metadata": {
        "id": "Nff-vKELpZyI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Perform Statistical Test to obtain P-Value"
      ],
      "metadata": {
        "id": "s6AnJQjtpZyI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which statistical test have you done to obtain P-Value?"
      ],
      "metadata": {
        "id": "kLW572S8pZyI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here."
      ],
      "metadata": {
        "id": "ytWJ8v15pZyI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Why did you choose the specific statistical test?"
      ],
      "metadata": {
        "id": "dWbDXHzopZyI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here."
      ],
      "metadata": {
        "id": "M99G98V6pZyI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***6. Feature Engineering & Data Pre-processing***"
      ],
      "metadata": {
        "id": "yLjJCtPM0KBk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1. Handling Missing Values"
      ],
      "metadata": {
        "id": "xiyOF9F70UgQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Handling Missing Values & Missing Value Imputation"
      ],
      "metadata": {
        "id": "iRsAHk1K0fpS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### What all missing value imputation techniques have you used and why did you use those techniques?"
      ],
      "metadata": {
        "id": "7wuGOrhz0itI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here."
      ],
      "metadata": {
        "id": "1ixusLtI0pqI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2. Handling Outliers"
      ],
      "metadata": {
        "id": "id1riN9m0vUs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Handling Outliers & Outlier treatments"
      ],
      "metadata": {
        "id": "M6w2CzZf04JK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### What all outlier treatment techniques have you used and why did you use those techniques?"
      ],
      "metadata": {
        "id": "578E2V7j08f6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here."
      ],
      "metadata": {
        "id": "uGZz5OrT1HH-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3. Categorical Encoding"
      ],
      "metadata": {
        "id": "89xtkJwZ18nB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Encode your categorical columns"
      ],
      "metadata": {
        "id": "21JmIYMG2hEo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### What all categorical encoding techniques have you used & why did you use those techniques?"
      ],
      "metadata": {
        "id": "67NQN5KX2AMe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here."
      ],
      "metadata": {
        "id": "UDaue5h32n_G"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 4. Textual Data Preprocessing\n",
        "(It's mandatory for textual dataset i.e., NLP, Sentiment Analysis, Text Clustering etc.)"
      ],
      "metadata": {
        "id": "Iwf50b-R2tYG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1. Expand Contraction"
      ],
      "metadata": {
        "id": "GMQiZwjn3iu7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Expand Contraction"
      ],
      "metadata": {
        "id": "PTouz10C3oNN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2. Lower Casing"
      ],
      "metadata": {
        "id": "WVIkgGqN3qsr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Lower Casing"
      ],
      "metadata": {
        "id": "88JnJ1jN3w7j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 3. Removing Punctuations"
      ],
      "metadata": {
        "id": "XkPnILGE3zoT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Remove Punctuations"
      ],
      "metadata": {
        "id": "vqbBqNaA33c0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 4. Removing URLs & Removing words and digits contain digits."
      ],
      "metadata": {
        "id": "Hlsf0x5436Go"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Remove URLs & Remove words and digits contain digits"
      ],
      "metadata": {
        "id": "2sxKgKxu4Ip3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 5. Removing Stopwords & Removing White spaces"
      ],
      "metadata": {
        "id": "mT9DMSJo4nBL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Remove Stopwords"
      ],
      "metadata": {
        "id": "T2LSJh154s8W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Remove White spaces"
      ],
      "metadata": {
        "id": "EgLJGffy4vm0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 6. Rephrase Text"
      ],
      "metadata": {
        "id": "c49ITxTc407N"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Rephrase Text"
      ],
      "metadata": {
        "id": "foqY80Qu48N2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 7. Tokenization"
      ],
      "metadata": {
        "id": "OeJFEK0N496M"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Tokenization"
      ],
      "metadata": {
        "id": "ijx1rUOS5CUU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 8. Text Normalization"
      ],
      "metadata": {
        "id": "9ExmJH0g5HBk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Normalizing Text (i.e., Stemming, Lemmatization etc.)"
      ],
      "metadata": {
        "id": "AIJ1a-Zc5PY8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which text normalization technique have you used and why?"
      ],
      "metadata": {
        "id": "cJNqERVU536h"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here."
      ],
      "metadata": {
        "id": "Z9jKVxE06BC1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 9. Part of speech tagging"
      ],
      "metadata": {
        "id": "k5UmGsbsOxih"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# POS Taging"
      ],
      "metadata": {
        "id": "btT3ZJBAO6Ik"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 10. Text Vectorization"
      ],
      "metadata": {
        "id": "T0VqWOYE6DLQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Vectorizing Text"
      ],
      "metadata": {
        "id": "yBRtdhth6JDE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which text vectorization technique have you used and why?"
      ],
      "metadata": {
        "id": "qBMux9mC6MCf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here."
      ],
      "metadata": {
        "id": "su2EnbCh6UKQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 4. Feature Manipulation & Selection"
      ],
      "metadata": {
        "id": "-oLEiFgy-5Pf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1. Feature Manipulation"
      ],
      "metadata": {
        "id": "C74aWNz2AliB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Manipulate Features to minimize feature correlation and create new features"
      ],
      "metadata": {
        "id": "h1qC4yhBApWC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2. Feature Selection"
      ],
      "metadata": {
        "id": "2DejudWSA-a0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Select your features wisely to avoid overfitting"
      ],
      "metadata": {
        "id": "YLhe8UmaBCEE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### What all feature selection methods have you used  and why?"
      ],
      "metadata": {
        "id": "pEMng2IbBLp7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here."
      ],
      "metadata": {
        "id": "rb2Lh6Z8BgGs"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which all features you found important and why?"
      ],
      "metadata": {
        "id": "rAdphbQ9Bhjc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here."
      ],
      "metadata": {
        "id": "fGgaEstsBnaf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 5. Data Transformation"
      ],
      "metadata": {
        "id": "TNVZ9zx19K6k"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Do you think that your data needs to be transformed? If yes, which transformation have you used. Explain Why?"
      ],
      "metadata": {
        "id": "nqoHp30x9hH9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Transform Your data"
      ],
      "metadata": {
        "id": "I6quWQ1T9rtH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 6. Data Scaling"
      ],
      "metadata": {
        "id": "rMDnDkt2B6du"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Scaling your data"
      ],
      "metadata": {
        "id": "dL9LWpySC6x_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which method have you used to scale you data and why?"
      ],
      "metadata": {
        "id": "yiiVWRdJDDil"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 7. Dimesionality Reduction"
      ],
      "metadata": {
        "id": "1UUpS68QDMuG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Do you think that dimensionality reduction is needed? Explain Why?"
      ],
      "metadata": {
        "id": "kexQrXU-DjzY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here."
      ],
      "metadata": {
        "id": "GGRlBsSGDtTQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# DImensionality Reduction (If needed)"
      ],
      "metadata": {
        "id": "kQfvxBBHDvCa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which dimensionality reduction technique have you used and why? (If dimensionality reduction done on dataset.)"
      ],
      "metadata": {
        "id": "T5CmagL3EC8N"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here."
      ],
      "metadata": {
        "id": "ZKr75IDuEM7t"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 8. Data Splitting"
      ],
      "metadata": {
        "id": "BhH2vgX9EjGr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Split your data to train and test. Choose Splitting ratio wisely."
      ],
      "metadata": {
        "id": "0CTyd2UwEyNM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### What data splitting ratio have you used and why?"
      ],
      "metadata": {
        "id": "qjKvONjwE8ra"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here."
      ],
      "metadata": {
        "id": "Y2lJ8cobFDb_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 9. Handling Imbalanced Dataset"
      ],
      "metadata": {
        "id": "P1XJ9OREExlT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Do you think the dataset is imbalanced? Explain Why."
      ],
      "metadata": {
        "id": "VFOzZv6IFROw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here."
      ],
      "metadata": {
        "id": "GeKDIv7pFgcC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Handling Imbalanced Dataset (If needed)"
      ],
      "metadata": {
        "id": "nQsRhhZLFiDs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### What technique did you use to handle the imbalance dataset and why? (If needed to be balanced)"
      ],
      "metadata": {
        "id": "TIqpNgepFxVj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here."
      ],
      "metadata": {
        "id": "qbet1HwdGDTz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***7. ML Model Implementation***"
      ],
      "metadata": {
        "id": "VfCC591jGiD4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ML Model - 1"
      ],
      "metadata": {
        "id": "OB4l2ZhMeS1U"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ML Model - 1 Implementation\n",
        "\n",
        "# Fit the Algorithm\n",
        "\n",
        "# Predict on the model"
      ],
      "metadata": {
        "id": "7ebyywQieS1U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1. Explain the ML Model used and it's performance using Evaluation metric Score Chart."
      ],
      "metadata": {
        "id": "ArJBuiUVfxKd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Visualizing evaluation Metric Score chart"
      ],
      "metadata": {
        "id": "rqD5ZohzfxKe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2. Cross- Validation & Hyperparameter Tuning"
      ],
      "metadata": {
        "id": "4qY1EAkEfxKe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ML Model - 1 Implementation with hyperparameter optimization techniques (i.e., GridSearch CV, RandomSearch CV, Bayesian Optimization etc.)\n",
        "\n",
        "# Fit the Algorithm\n",
        "\n",
        "# Predict on the model"
      ],
      "metadata": {
        "id": "Dy61ujd6fxKe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which hyperparameter optimization technique have you used and why?"
      ],
      "metadata": {
        "id": "PiV4Ypx8fxKe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here."
      ],
      "metadata": {
        "id": "negyGRa7fxKf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Have you seen any improvement? Note down the improvement with updates Evaluation metric Score Chart."
      ],
      "metadata": {
        "id": "TfvqoZmBfxKf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here."
      ],
      "metadata": {
        "id": "OaLui8CcfxKf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ML Model - 2"
      ],
      "metadata": {
        "id": "dJ2tPlVmpsJ0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1. Explain the ML Model used and it's performance using Evaluation metric Score Chart."
      ],
      "metadata": {
        "id": "JWYfwnehpsJ1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Visualizing evaluation Metric Score chart"
      ],
      "metadata": {
        "id": "yEl-hgQWpsJ1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2. Cross- Validation & Hyperparameter Tuning"
      ],
      "metadata": {
        "id": "-jK_YjpMpsJ2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ML Model - 1 Implementation with hyperparameter optimization techniques (i.e., GridSearch CV, RandomSearch CV, Bayesian Optimization etc.)\n",
        "\n",
        "# Fit the Algorithm\n",
        "\n",
        "# Predict on the model"
      ],
      "metadata": {
        "id": "Dn0EOfS6psJ2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which hyperparameter optimization technique have you used and why?"
      ],
      "metadata": {
        "id": "HAih1iBOpsJ2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here."
      ],
      "metadata": {
        "id": "9kBgjYcdpsJ2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Have you seen any improvement? Note down the improvement with updates Evaluation metric Score Chart."
      ],
      "metadata": {
        "id": "zVGeBEFhpsJ2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here."
      ],
      "metadata": {
        "id": "74yRdG6UpsJ3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 3. Explain each evaluation metric's indication towards business and the business impact pf the ML model used."
      ],
      "metadata": {
        "id": "bmKjuQ-FpsJ3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here."
      ],
      "metadata": {
        "id": "BDKtOrBQpsJ3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ML Model - 3"
      ],
      "metadata": {
        "id": "Fze-IPXLpx6K"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ML Model - 3 Implementation\n",
        "\n",
        "# Fit the Algorithm\n",
        "\n",
        "# Predict on the model"
      ],
      "metadata": {
        "id": "FFrSXAtrpx6M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1. Explain the ML Model used and it's performance using Evaluation metric Score Chart."
      ],
      "metadata": {
        "id": "7AN1z2sKpx6M"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Visualizing evaluation Metric Score chart"
      ],
      "metadata": {
        "id": "xIY4lxxGpx6M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2. Cross- Validation & Hyperparameter Tuning"
      ],
      "metadata": {
        "id": "9PIHJqyupx6M"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ML Model - 3 Implementation with hyperparameter optimization techniques (i.e., GridSearch CV, RandomSearch CV, Bayesian Optimization etc.)\n",
        "\n",
        "# Fit the Algorithm\n",
        "\n",
        "# Predict on the model"
      ],
      "metadata": {
        "id": "eSVXuaSKpx6M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which hyperparameter optimization technique have you used and why?"
      ],
      "metadata": {
        "id": "_-qAgymDpx6N"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here."
      ],
      "metadata": {
        "id": "lQMffxkwpx6N"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Have you seen any improvement? Note down the improvement with updates Evaluation metric Score Chart."
      ],
      "metadata": {
        "id": "Z-hykwinpx6N"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here."
      ],
      "metadata": {
        "id": "MzVzZC6opx6N"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1. Which Evaluation metrics did you consider for a positive business impact and why?"
      ],
      "metadata": {
        "id": "h_CCil-SKHpo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here."
      ],
      "metadata": {
        "id": "jHVz9hHDKFms"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2. Which ML model did you choose from the above created models as your final prediction model and why?"
      ],
      "metadata": {
        "id": "cBFFvTBNJzUa"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here."
      ],
      "metadata": {
        "id": "6ksF5Q1LKTVm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3. Explain the model which you have used and the feature importance using any model explainability tool?"
      ],
      "metadata": {
        "id": "HvGl1hHyA_VK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here."
      ],
      "metadata": {
        "id": "YnvVTiIxBL-C"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***8.*** ***Future Work (Optional)***"
      ],
      "metadata": {
        "id": "EyNgTHvd2WFk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1. Save the best performing ml model in a pickle file or joblib file format for deployment process.\n"
      ],
      "metadata": {
        "id": "KH5McJBi2d8v"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Save the File"
      ],
      "metadata": {
        "id": "bQIANRl32f4J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2. Again Load the saved model file and try to predict unseen data for a sanity check.\n"
      ],
      "metadata": {
        "id": "iW_Lq9qf2h6X"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the File and predict unseen data."
      ],
      "metadata": {
        "id": "oEXk9ydD2nVC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ***Congrats! Your model is successfully created and ready for deployment on a live server for a real user interaction !!!***"
      ],
      "metadata": {
        "id": "-Kee-DAl2viO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Conclusion**"
      ],
      "metadata": {
        "id": "gCX9965dhzqZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Write the conclusion here."
      ],
      "metadata": {
        "id": "Fjb1IsQkh3yE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ***Hurrah! You have successfully completed your Machine Learning Capstone Project !!!***"
      ],
      "metadata": {
        "id": "gIfDvo9L0UH2"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3ee11c2a"
      },
      "source": [
        "!pip install splitfolders"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "895d930a"
      },
      "source": [
        "import os\n",
        "\n",
        "# List the contents of the output directory\n",
        "split_dir = \"brain_split\"\n",
        "if os.path.exists(split_dir):\n",
        "    print(f\"Contents of '{split_dir}':\")\n",
        "    for item in os.listdir(split_dir):\n",
        "        print(f\"- {item}\")\n",
        "else:\n",
        "    print(f\"Directory '{split_dir}' not found.\")"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}