{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "collapsed_sections": [
        "n3dbpmDWp1ck",
        "ZWILFDl5p1ck",
        "M7G43BXep1ck",
        "Iwf50b-R2tYG",
        "WVIkgGqN3qsr",
        "XkPnILGE3zoT",
        "Hlsf0x5436Go",
        "mT9DMSJo4nBL",
        "c49ITxTc407N",
        "OeJFEK0N496M",
        "9ExmJH0g5HBk",
        "cJNqERVU536h",
        "k5UmGsbsOxih",
        "T0VqWOYE6DLQ",
        "qBMux9mC6MCf",
        "yiiVWRdJDDil",
        "KH5McJBi2d8v",
        "iW_Lq9qf2h6X",
        "-Kee-DAl2viO",
        "gIfDvo9L0UH2"
      ],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sreeproject/AI-/blob/main/Yet_another_copy_of_Sample_ML_Submission1_Template.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Project Name**    - Yes Bank Stock Close Price Prediction\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "vncDsAP0Gaoa"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### **Project Type**    - Regression/Supervised\n",
        "##### **Contribution**    - Individual\n"
      ],
      "metadata": {
        "id": "beRrZCGUAJYm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Project Summary -**"
      ],
      "metadata": {
        "id": "FJNUwmbgGyua"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Yes Bank is a well-known name in the financial sector. Since 2018, it has been in the news due to a fraud case involving Rana Kapoor. This raised curiosity about how such events affected the bankâ€™s stock prices and whether predictive models can capture those changes. The dataset contains monthly stock data from the beginning, including opening, closing, highest, and lowest prices. The main goal is to predict the monthly closing stock price."
      ],
      "metadata": {
        "id": "F6v_1wHtG2nS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **GitHub Link -**"
      ],
      "metadata": {
        "id": "w6K7xa23Elo4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "https://github.com/sreeproject/AI-/blob/main/Yet_another_copy_of_Sample_ML_Submission1_Template.ipynb\n",
        "https://github.com/sreeproject/AI-/blob/main/Yet_another_copy_of_Sample_ML_Submission1_Template.ipynb"
      ],
      "metadata": {
        "id": "h1o69JH3Eqqn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Problem Statement**\n"
      ],
      "metadata": {
        "id": "yQaldy8SH6Dl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Predict Yes Bank's stock close price prediction of the month"
      ],
      "metadata": {
        "id": "DpeJGUA3kjGy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **General Guidelines** : -  "
      ],
      "metadata": {
        "id": "mDgbUHAGgjLW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1.   Well-structured, formatted, and commented code is required.\n",
        "2.   Exception Handling, Production Grade Code & Deployment Ready Code will be a plus. Those students will be awarded some additional credits.\n",
        "     \n",
        "     The additional credits will have advantages over other students during Star Student selection.\n",
        "       \n",
        "             [ Note: - Deployment Ready Code is defined as, the whole .ipynb notebook should be executable in one go\n",
        "                       without a single error logged. ]\n",
        "\n",
        "3.   Each and every logic should have proper comments.\n",
        "4. You may add as many number of charts you want. Make Sure for each and every chart the following format should be answered.\n",
        "        \n",
        "\n",
        "```\n",
        "# Chart visualization code\n",
        "```\n",
        "            \n",
        "\n",
        "*   Why did you pick the specific chart?\n",
        "*   What is/are the insight(s) found from the chart?\n",
        "* Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason.\n",
        "\n",
        "5. You have to create at least 15 logical & meaningful charts having important insights.\n",
        "\n",
        "\n",
        "[ Hints : - Do the Vizualization in  a structured way while following \"UBM\" Rule.\n",
        "\n",
        "U - Univariate Analysis,\n",
        "\n",
        "B - Bivariate Analysis (Numerical - Categorical, Numerical - Numerical, Categorical - Categorical)\n",
        "\n",
        "M - Multivariate Analysis\n",
        " ]\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "6. You may add more ml algorithms for model creation. Make sure for each and every algorithm, the following format should be answered.\n",
        "\n",
        "\n",
        "*   Explain the ML Model used and it's performance using Evaluation metric Score Chart.\n",
        "\n",
        "\n",
        "*   Cross- Validation & Hyperparameter Tuning\n",
        "\n",
        "*   Have you seen any improvement? Note down the improvement with updates Evaluation metric Score Chart.\n",
        "\n",
        "*   Explain each evaluation metric's indication towards business and the business impact pf the ML model used.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "ZrxVaUj-hHfC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ***Let's Begin !***"
      ],
      "metadata": {
        "id": "O_i_v8NEhb9l"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***1. Know Your Data***"
      ],
      "metadata": {
        "id": "HhfV-JJviCcP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Import Libraries"
      ],
      "metadata": {
        "id": "Y3lxredqlCYt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import Libraries\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from datetime import datetime\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')"
      ],
      "metadata": {
        "id": "M8Vqi-pPk-HR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Dataset Loading"
      ],
      "metadata": {
        "id": "3RnN4peoiCZX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#from google.colab import drive\n",
        "#drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "zDcWJeUnOQMR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load Dataset\n",
        "data= pd.read_csv('data_YesBank_StockPrices.csv')\n",
        "data.head()"
      ],
      "metadata": {
        "id": "4CkvbW_SlZ_R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Dataset First View"
      ],
      "metadata": {
        "id": "x71ZqKXriCWQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Dataset First Look\n",
        "data.head()"
      ],
      "metadata": {
        "id": "LWNFOSvLl09H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Dataset Rows & Columns count"
      ],
      "metadata": {
        "id": "7hBIi_osiCS2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Dataset Rows & Columns count\n",
        "data.shape"
      ],
      "metadata": {
        "id": "Kllu7SJgmLij"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "185 entries and 5 rows are there"
      ],
      "metadata": {
        "id": "pPmUHafDTt1-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Dataset Information"
      ],
      "metadata": {
        "id": "JlHwYmJAmNHm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Dataset Info\n",
        "data.info()"
      ],
      "metadata": {
        "id": "e9hRXRi6meOf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Duplicate Values"
      ],
      "metadata": {
        "id": "35m5QtbWiB9F"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Dataset Duplicate Value Count\n",
        "\n",
        "duplicate_count = data.duplicated().sum()\n",
        "print(f\"Number of duplicate rows: {duplicate_count}\")"
      ],
      "metadata": {
        "id": "1sLdpKYkmox0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Missing Values/Null Values"
      ],
      "metadata": {
        "id": "PoPl-ycgm1ru"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Missing Values/Null Values Count\n",
        "data.isnull().sum()"
      ],
      "metadata": {
        "id": "GgHWkxvamxVg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "There is no missing values"
      ],
      "metadata": {
        "id": "WElb-v6jVzUE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Visualizing the missing values\n",
        "# Plot the heatmap\n",
        "plt.figure(figsize=(10,4))\n",
        "sns.heatmap(data.isnull())\n",
        "#sns.heatmap(data.isnull(), cbar=False, cmap='YlOrRd', yticklabels=False)\n",
        "plt.title('Missing Values Heatmap')\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "3q5wnI3om9sJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "There is no missing values. The dataset is clean and complete. so no imputation needed"
      ],
      "metadata": {
        "id": "YAMvVyEWV2Lk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "data.dtypes"
      ],
      "metadata": {
        "id": "xQ07mcpmqtZ6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### What did you know about your dataset?"
      ],
      "metadata": {
        "id": "H0kj-8xxnORC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here : This data set contains 185 entries and 5 rows. The field names are date,open,high,low,close. close is the target variable. Here date is object type. Other four fileds are numeric type. There is no missing values. There is no duplicate values."
      ],
      "metadata": {
        "id": "gfoNAAC-nUe_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***2. Understanding Your Variables***"
      ],
      "metadata": {
        "id": "nA9Y7ga8ng1Z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Dataset Columns\n",
        "data.columns"
      ],
      "metadata": {
        "id": "j7xfkqrt5Ag5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Dataset Describe\n",
        "data.describe()"
      ],
      "metadata": {
        "id": "DnOaZdaE5Q5t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Variables Description"
      ],
      "metadata": {
        "id": "PBTbrJXOngz2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here : 5 fieds are there\n",
        "\n",
        "Date - Date of record\n",
        "\n",
        "Open - Opening Price\n",
        "\n",
        "High - Highest price in day\n",
        "\n",
        "Low  - Lowest price in the day\n",
        "\n",
        "Close- Occupations of the\n",
        "       speaker"
      ],
      "metadata": {
        "id": "aJV4KIxSnxay"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Check Unique Values for each variable."
      ],
      "metadata": {
        "id": "u3PMJOP6ngxN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Check Unique Values for each variable.\n",
        "data.nunique()"
      ],
      "metadata": {
        "id": "zms12Yq5n-jE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Date, close all are unique values, no duplicated values in date and close"
      ],
      "metadata": {
        "id": "jdG4JhRLmhUK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3. ***Data Wrangling***"
      ],
      "metadata": {
        "id": "dauF4eBmngu3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Data Wrangling Code"
      ],
      "metadata": {
        "id": "bKJF3rekwFvQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Write your code to make your dataset analysis ready.\n",
        "# 1. Convert 'Date' column to datetime format (assuming \"Jul-05\" means July 2005)\n",
        "data['Date']=pd.to_datetime(data['Date'], format='%b-%y')\n",
        "#Sort data by date (just in case)\n",
        "data = data.sort_values(by='Date').reset_index(drop=True)\n",
        "\n",
        "data.head()"
      ],
      "metadata": {
        "id": "wk-9a2fpoLcV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data.dtypes"
      ],
      "metadata": {
        "id": "rF0eGLY8ngxh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### What all manipulations have you done and insights you found?"
      ],
      "metadata": {
        "id": "MSa1f5Uengrz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here. Loaded .csv file in to pandas dataframe\n",
        "\n",
        "Convert Date colume from String formate to datetime formate\n",
        "\n",
        "Sort data, using date colume\n",
        "\n",
        "Check dupicted values. No duplicated values are there\n",
        "\n",
        "check missing values. No missing values are there"
      ],
      "metadata": {
        "id": "LbyXE7I1olp8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***4. Data Vizualization, Storytelling & Experimenting with charts : Understand the relationships between variables***"
      ],
      "metadata": {
        "id": "GF8Ens_Soomf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 1"
      ],
      "metadata": {
        "id": "0wOQAZs5pc--"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 1 visualization code\n",
        "plt.figure(figsize=(14, 6))\n",
        "plt.plot(data['Date'], data['Close'], marker='o', linestyle='-')\n",
        "plt.title('Yes Bank Stock Close Price Over Time')\n",
        "plt.xlabel('Date')\n",
        "plt.ylabel('Close Price (INR)')\n",
        "plt.grid(True)\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "7v_ESjsspbW7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "K5QZ13OEpz2H"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Line cart is the most effective visualization of time-series data like monthly stock price.Heare Compare 2 variabes date and close prise.\n",
        "It shows trends like long term trends,sudden changes,patten like upward and downward."
      ],
      "metadata": {
        "id": "XESiWehPqBRc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "lQ7QKXXCp7Bj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "It helps to identify strong growth and decline,Understand when stocke reached its peak.Is there any inancial crises are there. Can evaluate major events like leadership change etc"
      ],
      "metadata": {
        "id": "C_j1G7yiqdRP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "448CDAPjqfQr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Business impact\n",
        "\n",
        "Stock holders can make decision about sell or buy stocks,Risk managment team can study the patten before large drops,Business strategy teams can use insights from the stock price trends to deside when take high impact decisions based on how the market feels bout the company\n",
        "\n",
        "Negative growth\n",
        "\n",
        "Yes it is clearly visible inside the graph after 2020.It related to Yes Bank moratorium crisis. It shows how the public trust and action impacted to the share values\n"
      ],
      "metadata": {
        "id": "3cspy4FjqxJW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 2"
      ],
      "metadata": {
        "id": "KSlN3yHqYklG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 2 visualization code\n",
        "plt.figure(figsize=(14, 6))\n",
        "plt.plot(data['Date'], data['High'], label='High', alpha=0.7)\n",
        "plt.plot(data['Date'], data['Low'], label='Low', alpha=0.7)\n",
        "plt.plot(data['Date'], data['Close'], label='Close', linewidth=2)\n",
        "plt.title('High, Low, and Close Prices Over Time')\n",
        "plt.xlabel('Date')\n",
        "plt.ylabel('Price (INR)')\n",
        "plt.legend()\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "R4YgtaqtYklH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "t6dVpIINYklI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Multiline chart.It shows full price range and market behaviour in each time period. Compre the low, high and close, we can see how wide the price range was in each month. Big gap between low and high hows market uncertinity"
      ],
      "metadata": {
        "id": "5aaW0BYyYklI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "ijmpgYnKYklI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Some months shows high gap between high and low prices, can notice sudden hike and down around 2018-2020.  this suggest market uncertinity, linked to any bad news or or instability within Yes Bank.\n",
        "\n"
      ],
      "metadata": {
        "id": "PSx9atu2YklI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "-JiQyfWJYklI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Positive business impact\n",
        "\n",
        "Knowing when prices are stable helps to plan better.Spotting risky time can ptotect our money. Can choose right time to invest. Sharing updates early people keep calm\n",
        "\n",
        "Negative impact\n",
        "\n",
        "Big price drops plus high volatility could mean money trouble or bad management.\n",
        "Prices donâ€™t go back up shows loss in company value, scaring away investors.\n",
        "\n"
      ],
      "metadata": {
        "id": "BcBbebzrYklV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 3"
      ],
      "metadata": {
        "id": "EM7whBJCYoAo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "## Chart - 3 visualization code\n",
        "# Optional: Sort by Date\n",
        "data = data.sort_values('Date')\n",
        "\n",
        "# Plotting the line chart\n",
        "plt.figure(figsize=(14, 6))\n",
        "\n",
        "plt.plot(data['Date'], data['Low'], label='Low', color='blue', linestyle='--')\n",
        "plt.plot(data['Date'], data['High'], label='High', color='red', linestyle='--')\n",
        "plt.plot(data['Date'], data['Open'], label='Open', color='orange')\n",
        "plt.plot(data['Date'], data['Close'], label='Close', color='green')\n",
        "\n",
        "# Chart formatting\n",
        "plt.title('Yes Bank Stock: Low, High, Open, Close Comparison Over Time')\n",
        "plt.xlabel('Date')\n",
        "plt.ylabel('Price (INR)')\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "t6GMdE67YoAp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "fge-S5ZAYoAp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Multi line chrt to compare low,high,open,close prices. It  showing how values change over time, making it easy to observe stock movement patterns.Can compare all four price types on the same time axis to see how they relate. It shows a clean, way to track price behavior over long periods"
      ],
      "metadata": {
        "id": "5dBItgRVYoAp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "85gYPyotYoAp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The chart shows that Yes Bank stock experienced noticeable volatility, as seen in the wide gaps between high and low prices. The close price line helps identify upward or downward trends over time. Differences between open and close prices reveal daily gains or losses, indicating investor sentiment. Repeated peaks and dips suggest possible support and resistance levels useful for trading decisions."
      ],
      "metadata": {
        "id": "4jstXR6OYoAp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "RoGjAbkUYoAp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Positive Impact:\n",
        "Yes, the insights can help make better business decisions. They show when prices are rising or falling, helping to plan when to buy or sell. They also help spot risky times and manage losses better.\n",
        "\n",
        "Negactive impact:\n",
        "Some patterns may show trouble. If prices keep falling or are too unstable, it could mean the company is losing trust or facing problems."
      ],
      "metadata": {
        "id": "zfJ8IqMcYoAp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 4"
      ],
      "metadata": {
        "id": "4Of9eVA-YrdM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 4 visualization code\n",
        "\n",
        "#Plot histogram of Close prices\n",
        "plt.figure(figsize=(10, 5))\n",
        "sns.histplot(data['Close'], bins=30, kde=True, color='blue')\n",
        "\n",
        "plt.title('Histogram of Close Prices')\n",
        "plt.xlabel('Close Price (INR)')\n",
        "plt.ylabel('Frequency')\n",
        "plt.grid(True)\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "irlUoxc8YrdO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "iky9q4vBYrdO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Chose the histogram because it clearly shows the distribution of stock prices, helping us understand how frequently different price ranges occur.\n",
        "\n",
        "The data is right-skewed and some outliers are present on the higher end of the price range.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "aJRCwT6DYrdO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "F6T5p64dYrdO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Most of the stock prices are low, and only a few are very high. This means the price usually stays low but sometimes jumps up. The chart also shows some outliers. These may be sudden changes in the market."
      ],
      "metadata": {
        "id": "Xx8WAJvtYrdO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "y-Ehk30pYrdP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Positive Business Impact:\n",
        "This insights help in planning better. Knowing that prices usually stay low helps set realistic targets. Sudden high prices can be used for quick profit opportunities.\n",
        "\n",
        "Negative Growth Signs:\n",
        "The outliers and right skew suggest instability. Sudden price jumps may be due to bad news, panic, or speculationâ€”showing the stock can be risky at times."
      ],
      "metadata": {
        "id": "jLNxxz7MYrdP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 5"
      ],
      "metadata": {
        "id": "bamQiAODYuh1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 5 visualization code\n",
        "# Using seaborn library\n",
        "#sns.distplot(data['Close'])\n",
        "\n",
        "sns.distplot(data['Close'], bins=30, kde=True, color='orange')"
      ],
      "metadata": {
        "id": "TIJwrbroYuh3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "QHF8YVU7Yuh3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "It is clearly shows how the stock's close prices are spread out, including the frequency, shape, and skewness of the data. It helps identify outliers, typical price ranges, and whether the data follows a normal pattern or notâ€”all of which are useful for financial decision-making."
      ],
      "metadata": {
        "id": "dcxuIMRPYuh3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "GwzvFGzlYuh3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Data is rightly skewed,this suggests the stock usually sells at lower prices but occasionally spikes. It also reveals outliersâ€”unusual price jumps that may be due to market events. Overall, it reflects common price behavior with rare but significant highs."
      ],
      "metadata": {
        "id": "uyqkiB8YYuh3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "qYpmQ266Yuh3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Positive Business Impact:\n",
        "The insights help businesses set realistic price expectations and plan investment strategies. Knowing that prices usually stay low can guide buying decisions, while rare spikes can be used for short-term gains.\n",
        "\n",
        "Negative Growth Signals:\n",
        "The presence of outliers and right skew may indicate price instability. These sudden price jumps could be due to negative events like market panic or poor performance, which can reduce investor confidence."
      ],
      "metadata": {
        "id": "_WtzZ_hCYuh4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 6"
      ],
      "metadata": {
        "id": "OH-pJp9IphqM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 6 visualization code\n",
        "import plotly.express as px\n",
        "fig = px.scatter(x=data['Date'], y=data['Close'])\n",
        "fig.show()"
      ],
      "metadata": {
        "id": "kuRf4wtuphqN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "bbFf2-_FphqN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Scatter plot helps visualize the relationship between two continuous variables. It clearly shows patterns, trends, or outliers that may not be obvious in line or bar charts. It's useful for detecting correlation, clusters, or unusual behavior in stock data."
      ],
      "metadata": {
        "id": "loh7H2nzphqN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "_ouA3fa0phqN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        " It shows how the Close price changes over time. It reveals periods of high volatility where prices jump or drop sharply. You can also spot overall trends, like rising or falling prices. Additionally, outlier points may indicate unusual market events or news impact."
      ],
      "metadata": {
        "id": "VECbqPI7phqN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "Seke61FWphqN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Positive Business Impact:\n",
        "These insights help in understanding how the stock price behaves over time. Spotting trends and volatility can guide better timing for investments or business actions. It also helps in predicting future movements based on past patterns.\n",
        "\n",
        "Negative Growth Signals:\n",
        "If the chart shows frequent sharp drops or inconsistent price patterns, it may indicate market instability. This could reduce investor confidence and signal possible internal or external business issues."
      ],
      "metadata": {
        "id": "DW4_bGpfphqN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 7"
      ],
      "metadata": {
        "id": "PIIx-8_IphqN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 7 visualization code\n",
        "plt.scatter(data['Open'], data['Close'], alpha=0.6)"
      ],
      "metadata": {
        "id": "lqAIGUfyphqO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "t27r6nlMphqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Open vs Close scatter plot because it helps to **analyze the relationship between the stock's opening and closing prices.** It shows how much the\n",
        "price changes within a day. If most points lie near the diagonal line, it means the price doesnâ€™t change much between open and close. Points far from the line indicate big  movements, useful for identifying volatile trading days."
      ],
      "metadata": {
        "id": "iv6ro40sphqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "r2jJGEOYphqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The scatter plot of Open vs Close prices shows so many ouliers - points lie close to a diagonal line, meaning the stock usually closes near its opening price. This suggests low intraday volatility on most days. However, some points are far from the line, indicating days with large price changes. These outliers may reflect important market events or investor reactions."
      ],
      "metadata": {
        "id": "Po6ZPi4hphqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "b0JNsNcRphqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Positive Business Impact:\n",
        "These insights help identify stable vs. volatile trading days, allowing better planning for trades or financial decisions. Investors can use this to predict risk levels and adjust their strategies accordingly. Recognizing consistent open-close behavior also supports confidence in stock stability during normal conditions.\n",
        "\n",
        "Negative Growth Indicators:\n",
        "The presence of many outliers shows frequent intraday price swings, which may signal market uncertainty or inconsistent performance. This can reduce investor trust and indicate potential business or financial instability."
      ],
      "metadata": {
        "id": "xvSq8iUTphqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 8"
      ],
      "metadata": {
        "id": "BZR9WyysphqO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 8 visualization code\n",
        "sns.distplot(data['Open'], bins=30, kde=True, color='orange')"
      ],
      "metadata": {
        "id": "TdPTWpAVphqO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "jj7wYXLtphqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The distribution plot because it helps visualize how the Open prices are spread across the dataset. It clearly shows the most frequent price ranges, overall distribution shape, and the presence of outliers or unusual openings. This chart is ideal for understanding the stock's typical starting price behavior, which is important for predicting intraday movements."
      ],
      "metadata": {
        "id": "Ob8u6rCTphqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "eZrbJ2SmphqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The chart shows that most Open prices fall within a lower range, with the distribution being right-skewed. This means the stock usually opens at a lower price, but there are a few days with unusually high opening prices. The KDE curve confirms the concentration of values in the lower range. Outliers indicate occasional market shocks or positive news impact before trading begins."
      ],
      "metadata": {
        "id": "mZtgC_hjphqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "rFu4xreNphqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Positive Business Impact:\n",
        "The insights help identify the usual opening price range, which is valuable for setting entry points for traders and planning buy strategies. Recognizing that most openings are stable also helps in risk assessment and trading confidence.\n",
        "\n",
        "Negative Growth Indicators:\n",
        "The unusually high opening prices (outliers) may indicate market overreactions or speculation. These sudden spikes can lead to short-term volatility, causing uncertainty for long-term investors and potentially affecting trust in the stockâ€™s consistency."
      ],
      "metadata": {
        "id": "ey_0qi68phqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 9"
      ],
      "metadata": {
        "id": "YJ55k-q6phqO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 9 visualization code\n",
        "sns.distplot(data['Low'], bins=30, kde=True, color='orange')"
      ],
      "metadata": {
        "id": "B2aS4O1ophqO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "gCFgpxoyphqP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "It is right skewed.\n",
        "The distribution plot because it shows how the Low prices are spread out and helps identify the overall shape of the data, including skewness. It also highlights frequent price ranges and any outliers. The KDE curve makes it easier to see trends in the distribution"
      ],
      "metadata": {
        "id": "TVxDimi2phqP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "OVtJsKN_phqQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The chart shows that most of the Low prices are in a lower range, meaning the stock typically trades at lower values. The right skew suggests that occasionally, the \"Low\" price is much higher than usualâ€”likely due to market spikes or volatility. It also reveals outliers, indicating rare but significant events. This helps understand the stock's general stability and rare risk periods."
      ],
      "metadata": {
        "id": "ngGi97qjphqQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "lssrdh5qphqQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Positive Business Impact:\n",
        "The insights help identify the stockâ€™s usual low-price behavior, which is useful for setting buy targets and understanding risk boundaries. It also supports portfolio planning by showing that most prices are stable within a lower range.\n",
        "\n",
        "Negative Growth Indicators:\n",
        "The right skew and outliers may signal price instability on some days, possibly due to negative news or market panic. These rare spikes in \"Low\" prices could reflect sudden uncertainty or poor investor sentiment, which may harm confidence."
      ],
      "metadata": {
        "id": "tBpY5ekJphqQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 10"
      ],
      "metadata": {
        "id": "U2RJ9gkRphqQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 10 visualization code\n",
        "import plotly.express as px\n",
        "fig=px.line(data,x='Date',y='Close')\n",
        "fig.show()"
      ],
      "metadata": {
        "id": "GM7a4YP4phqQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "1M8mcRywphqQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Plotly line chart because it provides an interactive view of how the Close price changes over time, making it easy to explore trends, spikes, or drops. Unlike static charts, you can zoom, hover, and inspect exact values, which helps in deeper analysis. Itâ€™s ideal for time series data like stock prices where tracking daily movements matters."
      ],
      "metadata": {
        "id": "8agQvks0phqQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "tgIPom80phqQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The chart shows how the Close price of the stock changes over time, helping to identify upward or downward trends. You can spot sharp rises or falls, indicating volatile periods. It also reveals stable phases where the price doesn't change much. These insights help understand the stock's past behavior and possible future movement."
      ],
      "metadata": {
        "id": "Qp13pnNzphqQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "JMzcOPDDphqR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Positive Business Impact:\n",
        "This insights help in understanding market trends and timing decisions. Spotting rising trends can support buying strategies, while stable phases may indicate low-risk periods. Businesses can use this to align investments or announcements with favorable market conditions.\n",
        "\n",
        "Negative Growth Indicators:\n",
        "If the chart shows frequent sharp drops or long-term decline in Close prices, it may signal weak investor confidence or underlying company issues. These trends can harm market reputation and reduce stock value over time."
      ],
      "metadata": {
        "id": "R4Ka1PC2phqR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 11"
      ],
      "metadata": {
        "id": "x-EpHcCOp1ci"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 11 visualization code\n",
        "sns.distplot(data['High'], bins=30, kde=True, color='orange')"
      ],
      "metadata": {
        "id": "mAQTIvtqp1cj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "X_VqEhTip1ck"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The distribution plot because it shows how the High prices are distributed across the dataset. It helps identify the most frequent high values, detect outliers, and understand whether the data is right skewed. This is important for assessing how often the stock reaches high price points and spotting unusual market behavior."
      ],
      "metadata": {
        "id": "-vsMzt_np1ck"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "8zGJKyg5p1ck"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The chart shows that most High prices are concentrated in a lower range, with a right-skewed distribution. This means the stock usually doesn't reach very high prices, but there are a few days with extremely high values, likely due to market events. It also reveals outliers, indicating occasional sharp price increases. Overall, the stock rarely hits peak highs, making those events noteworthy."
      ],
      "metadata": {
        "id": "ZYdMsrqVp1ck"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "PVzmfK_Ep1ck"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Positive Business Impact:\n",
        "These insights help investors and analysts understand the typical upper price limits of the stock. Knowing that high prices are rare helps set realistic sell targets and informs risk-reward strategies. It also supports planning during bullish market periods.\n",
        "\n",
        "Negative Growth Indicators:\n",
        "The right skew and outliers suggest that extreme highs are not consistent, which may reflect temporary hype or speculation rather than sustained performance. This unpredictability can reduce investor confidence and signal market volatility or poor long-term stability."
      ],
      "metadata": {
        "id": "druuKYZpp1ck"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 12"
      ],
      "metadata": {
        "id": "n3dbpmDWp1ck"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 12 visualization code"
      ],
      "metadata": {
        "id": "bwevp1tKp1ck"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "ylSl6qgtp1ck"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here."
      ],
      "metadata": {
        "id": "m2xqNkiQp1ck"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "ZWILFDl5p1ck"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here"
      ],
      "metadata": {
        "id": "x-lUsV2mp1ck"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "M7G43BXep1ck"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here"
      ],
      "metadata": {
        "id": "5wwDJXsLp1cl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 13"
      ],
      "metadata": {
        "id": "Ag9LCva-p1cl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 13 visualization code\n",
        "outlin=['Open',\n",
        "       'High', 'Low','Close']\n",
        "\n",
        "sns.boxplot(data=data[outlin], orient='h')  # 'orient' is set to 'h' for horizontal box plots\n",
        "\n",
        "plt.xlabel('Values')\n",
        "plt.title('Box Plot of All Columns')"
      ],
      "metadata": {
        "id": "EUfxeq9-p1cl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "E6MkPsBcp1cl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "the box plot because it clearly shows the spread, median, and outliers for each price typeâ€”Open, High, Low, and Close. It's ideal for comparing the range and variability of multiple stock price columns side by side. It also highlights outliers, which helps detect unusual price movements. This makes it a great tool for spotting both normal behavior and anomalies in the data."
      ],
      "metadata": {
        "id": "V22bRsFWp1cl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "2cELzS2fp1cl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The box plot shows that most stock prices (Open, High, Low, Close) are concentrated within a specific range. It reveals that the High prices tend to be greater and more spread out, while Low prices are more tightly grouped. The presence of outliers indicates days with unusual price spikes or drops. Overall, it highlights price variability and helps identify which price type is most volatile."
      ],
      "metadata": {
        "id": "ozQPc2_Ip1cl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "3MPXvC8up1cl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Positive Business Impact:\n",
        "The insights help in understanding price stability and volatility, which is crucial for making informed investment and trading decisions. Identifying consistent price behavior builds confidence in forecasting and risk planning. Detecting outliers can also help in investigating profitable market opportunities or key events.\n",
        "\n",
        "Negative Growth Indicators:\n",
        "The presence of frequent or extreme outliers may signal market instability or speculative activity. This could reduce investor trust and indicate potential internal or external issues affecting the stock's reliability."
      ],
      "metadata": {
        "id": "GL8l1tdLp1cl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 14 - Correlation Heatmap"
      ],
      "metadata": {
        "id": "NC_X3p0fY2L0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Correlation Heatmap visualization code\n",
        "#Plot the correlation heatmap\n",
        "sns.heatmap(data.corr(),annot=True);"
      ],
      "metadata": {
        "id": "xyC9zolEZNRQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "UV0SzAkaZNRQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "correlation heatmap because it clearly shows the strength and direction of relationships between multiple numerical features (like Open, High, Low, and Close prices). It helps identify which variables are strongly correlatedâ€”useful for feature selection, reducing redundancy, and improving model performance. The color-coded matrix makes it easy to interpret complex relationships at a glance."
      ],
      "metadata": {
        "id": "DVPuT8LYZNRQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "YPEH6qLeZNRQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The heatmap shows that Open, High, Low, and Close prices are strongly positively correlated, meaning they move closely together. This indicates that when one price increases or decreases, the others tend to follow. It also shows that Volume has a weaker correlation with price features, suggesting it doesnâ€™t directly influence daily prices. These insights help in choosing the most relevant features for building accurate predictive models."
      ],
      "metadata": {
        "id": "bfSqtnDqZNRR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 15 - Pair Plot"
      ],
      "metadata": {
        "id": "q29F0dvdveiT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Pair Plot visualization code\n",
        "sns.pairplot(data, kind=\"kde\")"
      ],
      "metadata": {
        "id": "o58-TEIhveiU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "EXh0U9oCveiU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The pair plot because it allows us to visually explore relationships between multiple numeric features in one chart. It helps detect patterns, correlations, clusters, and outliers across all variable combinations. The diagonal KDE plots show how each feature is distributed, while the scatter plots reveal how they interact. This makes it a powerful tool for feature analysis and early-stage data exploration."
      ],
      "metadata": {
        "id": "eMmPjTByveiU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "22aHeOlLveiV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The pair plot shows that Open, High, Low, and Close prices have strong linear relationships, as seen in the tight scatter clusters along diagonals. This confirms they move together consistently. The diagonal KDE plots reveal that most price values are concentrated in the lower range, confirming right-skewed distributions. It also helps to spot any outliers or unusual points across variable pairs."
      ],
      "metadata": {
        "id": "uPQ8RGwHveiV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***5. Hypothesis Testing***"
      ],
      "metadata": {
        "id": "g-ATYxFrGrvw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Based on your chart experiments, define three hypothetical statements from the dataset. In the next three questions, perform hypothesis testing to obtain final conclusion about the statements through your code and statistical testing."
      ],
      "metadata": {
        "id": "Yfr_Vlr8HBkt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Hypothesis 1:\n",
        "Hâ‚€ (Null): The average Close price before 2018 is equal to the average Close price after 2018.\n",
        "Ha (Alternate): The average Close price before 2018 is not equal to the average Close price after 2018.\n",
        "\n",
        "Hypothesis 2:\n",
        "Hâ‚€ (Null): Mean Close price = â‚¹50\n",
        "Ha (Alternate): Mean Close price < â‚¹50\n",
        "\n",
        "Hypothesis 3:\n",
        "Hâ‚€ (Null): There is no significant correlation between Open and Close prices.\n",
        "Ha (Alternate): There is a significant positive correlation between Open and Close prices."
      ],
      "metadata": {
        "id": "-7MS06SUHkB-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Hypothetical Statement - 1"
      ],
      "metadata": {
        "id": "8yEUt7NnHlrM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1. State Your research hypothesis as a null hypothesis and alternate hypothesis."
      ],
      "metadata": {
        "id": "tEA2Xm5dHt1r"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In hypothesis testing,the null hypothesis is(H0) is a statement there's no significant effect or relationship between variables,\n",
        "ex:- H0: the average close prise before 2018 is equal to average close prise after 2018\n",
        "\n",
        "The atranative hypothesis(Ha) is a statement that condradicts null hypothesis.\n",
        "ex:-Ha: the average close prise before 2018 is not equal to average close prise after 2018"
      ],
      "metadata": {
        "id": "HI9ZP0laH0D-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2. Perform an appropriate statistical test."
      ],
      "metadata": {
        "id": "I79__PHVH19G"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Perform Statistical Test to obtain P-Value\n",
        "from scipy.stats import ttest_ind\n",
        "\n",
        "# Split into two groups: before and after 2018\n",
        "before_2018 = data[data['Date'] < '2018-01-01']['Close']\n",
        "after_2018 = data[data['Date'] >= '2018-01-01']['Close']\n",
        "\n",
        "# Perform independent two-sample t-test\n",
        "t_stat, p_value = ttest_ind(before_2018,after_2018)\n",
        "\n",
        "# Output results\n",
        "print(f\"T-statistic: {t_stat}\")\n",
        "print(f\"p_value: {p_value}\")\n",
        "\n",
        "if p_value < 0.05:\n",
        "    print(\"Reject the null hypothesis:(accept altranative hypothesis) The average Close price before and after 2018 is significantly different.\")\n",
        "else:\n",
        "    print(\"Fail to reject the null hypothesis(accept null hypothesis): No significant difference in average Close prices.\")\n"
      ],
      "metadata": {
        "id": "oZrfquKtyian"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here p_value is 0.0034.That is less than the significant value 0.05. ie, p_value is less than 0.05 so accept atranative hypothesis ie, reject null hyothesis. That is there is a difference between close prise before 2018 and after 2018"
      ],
      "metadata": {
        "id": "ib-4OOHz0aLH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which statistical test have you done to obtain P-Value?"
      ],
      "metadata": {
        "id": "Ou-I18pAyIpj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "2-sample t-test called independed t-test\n"
      ],
      "metadata": {
        "id": "s2U0kk00ygSB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Why did you choose the specific statistical test?"
      ],
      "metadata": {
        "id": "fF3858GYyt-u"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Choose 2-sampe t-test becauese compareing average close prise of 2 independend group before and after 2018"
      ],
      "metadata": {
        "id": "HO4K0gP5y3B4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Hypothetical Statement - 2"
      ],
      "metadata": {
        "id": "4_0_7-oCpUZd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1. State Your research hypothesis as a null hypothesis and alternate hypothesis."
      ],
      "metadata": {
        "id": "hwyV_J3ipUZe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In hypothesis testing,The is no significant effect or relations with the varables,\n",
        "ex:- H0: mean close prise = â‚¹50\n",
        "\n",
        "The atranative hypothesis(Ha) is a statement that condradicts null hypothesis\n",
        "Ha: Mean close prise < â‚¹50"
      ],
      "metadata": {
        "id": "FnpLGJ-4pUZe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2. Perform an appropriate statistical test."
      ],
      "metadata": {
        "id": "3yB-zSqbpUZe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Perform Statistical Test to obtain P-Value\n",
        "from scipy.stats import ttest_1samp\n",
        "\n",
        "# Perform one-sample t-test on 'Close' column\n",
        "t_stat, p_value = ttest_1samp(data['Close'], 50)\n",
        "\n",
        "# Adjust p-value for one-tailed test (mean < 50)\n",
        "if t_stat < 0:\n",
        "   p_value =p_value/2\n",
        "\n",
        "# Output results\n",
        "print(f\"T-statistic: {t_stat}\")\n",
        "print(f\"p_value: {p_value}\")\n",
        "\n",
        "if p_value < 0.05:\n",
        "    print(\"Reject the null hypothesis:(accept altranative hypothesis) The average Close price is less than 50.\")\n",
        "else:\n",
        "    print(\"Fail to reject the null hypothesis(accept null hypothesis): No significant Close prices less than 50.\")\n"
      ],
      "metadata": {
        "id": "sWxdNTXNpUZe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here p value is less than 0.05. so Accept the altranative hypothesis."
      ],
      "metadata": {
        "id": "R5Ye14McC9nO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which statistical test have you done to obtain P-Value?"
      ],
      "metadata": {
        "id": "dEUvejAfpUZe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "one sample t-test"
      ],
      "metadata": {
        "id": "oLDrPz7HpUZf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Why did you choose the specific statistical test?"
      ],
      "metadata": {
        "id": "Fd15vwWVpUZf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "One sample t-test because compare mean close prise of a single sample to a reference value"
      ],
      "metadata": {
        "id": "4xOGYyiBpUZf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Hypothetical Statement - 3"
      ],
      "metadata": {
        "id": "bn_IUdTipZyH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1. State Your research hypothesis as a null hypothesis and alternate hypothesis."
      ],
      "metadata": {
        "id": "49K5P_iCpZyH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The null hypothesis is(H0) is a statement there's no significant effect or relationship between variables.\n",
        "ex:- Hâ‚€: There is no significant correlation between open an close prise\n",
        "The atranative hypothesis(Ha) is a statement that condradicts null hypothesis.\n",
        "ex: Ha: There is a positive correlation between open an close prises."
      ],
      "metadata": {
        "id": "7gWI5rT9pZyH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2. Perform an appropriate statistical test."
      ],
      "metadata": {
        "id": "Nff-vKELpZyI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Perform Statistical Test to obtain P-Value\n",
        "from scipy.stats import pearsonr\n",
        "\n",
        "# Perform Pearson correlation test\n",
        "corr_coef,p_value = pearsonr(data['Open'], data['Close'])\n",
        "\n",
        "# Display results\n",
        "print(\"Pearson Correlation Coefficient:\", corr_coef)\n",
        "print(f\"p_value: {p_value}\")\n",
        "\n",
        "if p_value < 0.05:\n",
        "    print(\"Reject the null hypothesis:(accept altranative hypothesis) Significant correlation beteween close and open prise.\")\n",
        "else:\n",
        "    print(\"Fail to reject the null hypothesis(accept null hypothesis): No significant beteween close and open prise.\")\n"
      ],
      "metadata": {
        "id": "s6AnJQjtpZyI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which statistical test have you done to obtain P-Value?"
      ],
      "metadata": {
        "id": "kLW572S8pZyI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Pearson Correlation Test"
      ],
      "metadata": {
        "id": "ytWJ8v15pZyI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Why did you choose the specific statistical test?"
      ],
      "metadata": {
        "id": "dWbDXHzopZyI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Pearson Correlation Test because examine the linear relationship between 2 continuous numeric variabes open and close"
      ],
      "metadata": {
        "id": "M99G98V6pZyI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***6. Feature Engineering & Data Pre-processing***"
      ],
      "metadata": {
        "id": "yLjJCtPM0KBk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1. Handling Missing Values"
      ],
      "metadata": {
        "id": "xiyOF9F70UgQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Handling Missing Values & Missing Value Imputation\n",
        "data.isnull().sum()"
      ],
      "metadata": {
        "id": "iRsAHk1K0fpS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#remove missing values from dataframe use\n",
        "#data.dropna(inplace=True)\n",
        "#Handle Missing Values\n",
        "#data.fillna(method='ffill', inplace=True) #forward fill\n",
        "#data.fillna(method='bfill', inplace=True) #backward fill\n",
        "#Median Imputation (Best for skewed data)\n",
        "#data['Column_Name'].fillna(data['Column_Name'].median(), inplace=True)\n",
        "#Mean Imputation (if data is normally distributed)\n",
        "#data['Column_Name'].fillna(data['Column_Name'].mean(), inplace=True)\n",
        "# Fill missing values in a categorical column with the most frequent value (mode)\n",
        "#data['Col_name'].fillna(data['Col_name'].mode()[0], inplace=True)\n"
      ],
      "metadata": {
        "id": "HX0l8jTXGZU4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### What all missing value imputation techniques have you used and why did you use those techniques?"
      ],
      "metadata": {
        "id": "7wuGOrhz0itI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "After loading and exporing the data set check for missing vales using\n",
        "data.isnull.sum().\n",
        "Here there is no missing values so no need to any imputation.\n",
        "If there were any missing values,use data.dropna(inplace=True) to remove it.\n",
        "\n",
        "Missing value impution\n",
        "\n",
        "In the case numeric columns use data and normally distributed data use\n",
        "data['col_name'].fillna(data['col_name'].mean(),inplace=True)\n",
        "\n",
        "In the case Categorical missing values use\n",
        "data['col_name'].fillna(data['col_name'].mode()[0], inplace=True)\n",
        "\n",
        "In the case of skewed data use\n",
        "data['col_name'].fillna(data['col_name'].median(),inplace=True)\n",
        "\n",
        "Here dataset is clean and complete so no need for missing vaue handling"
      ],
      "metadata": {
        "id": "1ixusLtI0pqI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2. Handling Outliers"
      ],
      "metadata": {
        "id": "id1riN9m0vUs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Handling Outliers & Outlier treatments\n",
        " #Function to remove outliers based on IQR\n",
        "def remove_outliers_iqr(df, column):\n",
        "    Q1 = df[column].quantile(0.25)\n",
        "    Q3 = df[column].quantile(0.75)\n",
        "    IQR = Q3 - Q1\n",
        "    lower_bound = Q1 - 1.5 * IQR\n",
        "    upper_bound = Q3 + 1.5 * IQR\n",
        "    return df[(df[column] >= lower_bound) & (df[column] <= upper_bound)]\n",
        "\n",
        "# Apply IQR method for 'Close' price\n",
        "data_iqr_filtered = remove_outliers_iqr(data, 'Close')\n",
        "print(f\"Original rows: {len(data)}, After IQR filtering: {len(data_iqr_filtered)}\")"
      ],
      "metadata": {
        "id": "M6w2CzZf04JK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### What all outlier treatment techniques have you used and why did you use those techniques?"
      ],
      "metadata": {
        "id": "578E2V7j08f6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here used IQR method to dectect oulliers and remove it from the column like Close.\n",
        "Below Q1 - 1.5 * IQR and above Q3 + 1.5 * IQR are consided as outliers.\n",
        "\n",
        "Use Boxplot to visualize the outliers\n",
        "\n",
        "Winsorization can also be used, whichensures that rare but meaningful stock movements are not entirely lost\n",
        "\n",
        "Z-score analysis can also used to flag outliers across multiple columns"
      ],
      "metadata": {
        "id": "uGZz5OrT1HH-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3. Categorical Encoding"
      ],
      "metadata": {
        "id": "89xtkJwZ18nB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Encode your categorical columns\n",
        "import pandas as pd\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "\n",
        "print(data.dtypes)\n",
        "\n",
        "# Identify categorical columns\n",
        "cat_cols = data.select_dtypes(include='object').columns\n",
        "print(\"Categorical columns:\", cat_cols)\n",
        "\n",
        "# Apply Label Encoding\n",
        "label_encoders = {}\n",
        "for col in cat_cols:\n",
        "    le = LabelEncoder()\n",
        "    data[col] = le.fit_transform(data[col].astype(str))\n",
        "    label_encoders[col] = le  # Save encoder if inverse transform is needed later\n",
        "\n",
        "print(data[cat_cols].head())"
      ],
      "metadata": {
        "id": "21JmIYMG2hEo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### What all categorical encoding techniques have you used & why did you use those techniques?"
      ],
      "metadata": {
        "id": "67NQN5KX2AMe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here use label encoding to convert categrical columns to numeric form that ml model can understnd. It is to assign a unique integer to each category in a categorical variable like 0,1\n",
        "\n",
        "Here i already convert Date colum to datetime format instead of using encoding techniques\n",
        "Onehot encodng can also be used. But here in the dataset has minimal categorical features no high cardinality."
      ],
      "metadata": {
        "id": "UDaue5h32n_G"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 4. Textual Data Preprocessing\n",
        "(It's mandatory for textual dataset i.e., NLP, Sentiment Analysis, Text Clustering etc.)"
      ],
      "metadata": {
        "id": "Iwf50b-R2tYG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1. Expand Contraction"
      ],
      "metadata": {
        "id": "GMQiZwjn3iu7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Expand Contraction"
      ],
      "metadata": {
        "id": "PTouz10C3oNN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2. Lower Casing"
      ],
      "metadata": {
        "id": "WVIkgGqN3qsr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Lower Casing"
      ],
      "metadata": {
        "id": "88JnJ1jN3w7j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 3. Removing Punctuations"
      ],
      "metadata": {
        "id": "XkPnILGE3zoT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Remove Punctuations"
      ],
      "metadata": {
        "id": "vqbBqNaA33c0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 4. Removing URLs & Removing words and digits contain digits."
      ],
      "metadata": {
        "id": "Hlsf0x5436Go"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Remove URLs & Remove words and digits contain digits"
      ],
      "metadata": {
        "id": "2sxKgKxu4Ip3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 5. Removing Stopwords & Removing White spaces"
      ],
      "metadata": {
        "id": "mT9DMSJo4nBL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Remove Stopwords"
      ],
      "metadata": {
        "id": "T2LSJh154s8W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Remove White spaces"
      ],
      "metadata": {
        "id": "EgLJGffy4vm0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 6. Rephrase Text"
      ],
      "metadata": {
        "id": "c49ITxTc407N"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Rephrase Text"
      ],
      "metadata": {
        "id": "foqY80Qu48N2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 7. Tokenization"
      ],
      "metadata": {
        "id": "OeJFEK0N496M"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Tokenization"
      ],
      "metadata": {
        "id": "ijx1rUOS5CUU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 8. Text Normalization"
      ],
      "metadata": {
        "id": "9ExmJH0g5HBk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Normalizing Text (i.e., Stemming, Lemmatization etc.)"
      ],
      "metadata": {
        "id": "AIJ1a-Zc5PY8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which text normalization technique have you used and why?"
      ],
      "metadata": {
        "id": "cJNqERVU536h"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here."
      ],
      "metadata": {
        "id": "Z9jKVxE06BC1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 9. Part of speech tagging"
      ],
      "metadata": {
        "id": "k5UmGsbsOxih"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# POS Taging"
      ],
      "metadata": {
        "id": "btT3ZJBAO6Ik"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 10. Text Vectorization"
      ],
      "metadata": {
        "id": "T0VqWOYE6DLQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Vectorizing Text"
      ],
      "metadata": {
        "id": "yBRtdhth6JDE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which text vectorization technique have you used and why?"
      ],
      "metadata": {
        "id": "qBMux9mC6MCf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here."
      ],
      "metadata": {
        "id": "su2EnbCh6UKQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 4. Feature Manipulation & Selection"
      ],
      "metadata": {
        "id": "-oLEiFgy-5Pf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1. Feature Manipulation"
      ],
      "metadata": {
        "id": "C74aWNz2AliB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Manipulate Features to minimize feature correlation and create new features\n",
        "#compute correlation matrix\n",
        "corr_matrix = data[['Close','Open','High','Low']].corr()\n",
        "corr_matrix"
      ],
      "metadata": {
        "id": "h1qC4yhBApWC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Low,High,open are highly corelated to close. Can't drop all fields. So i select correation between Close to high and Close to low fields and drop low,high columns and add 3 new fields\n",
        "\n",
        "data['Monthly_Return_%'] = data['Close'].pct_change() * 100\n",
        "\n",
        "data['Avg_Price'] = (data['High'] + data['Low']) / 2\n",
        "\n",
        "data['Price_Range'] = data['High'] - data['Low']\n"
      ],
      "metadata": {
        "id": "nO3WvFgSVZbo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "data['Monthly_Return_%'] = data['Close'].pct_change() * 100\n",
        "\n",
        "data['Avg_Price'] = (data['High'] + data['Low']) / 2\n",
        "\n",
        "data['Price_Range'] = data['High'] - data['Low']\n",
        "\n",
        "data.drop(['Low','High'],axis=1,inplace=True)\n",
        "\n",
        "new_data=data.copy()\n",
        "\n",
        "new_data.head()"
      ],
      "metadata": {
        "id": "8lXQHObDYRme"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2. Feature Selection"
      ],
      "metadata": {
        "id": "2DejudWSA-a0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Select your features wisely to avoid overfitting\n",
        "selected_features = [\n",
        "    'Open',             # Starting price of the day\n",
        "    'Close',            # Final price â€” our target for regression\n",
        "    'Avg_Price',        # Smooth trading indicator\n",
        "    'Price_Range',      # Volatility indicator\n",
        "    'Monthly_Return_%'  # Temporal performance\n",
        "]\n",
        "\n",
        "new_data.head()"
      ],
      "metadata": {
        "id": "YLhe8UmaBCEE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### What all feature selection methods have you used  and why?"
      ],
      "metadata": {
        "id": "pEMng2IbBLp7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here use correlation analysis to identify and remove highly corelated features like high and low to reduce multicollinearity. Create 3 new columns monthly_return_%,price_range,avg_price.\n",
        "Did manual selection by review features relevence to Close (target variable) to avoid overfitting"
      ],
      "metadata": {
        "id": "rb2Lh6Z8BgGs"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which all features you found important and why?"
      ],
      "metadata": {
        "id": "rAdphbQ9Bhjc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Avg_Price important as it shows daily fluctuations and better represents overall trading activity. Price_Range was key in capturing market volatility, which often influences investor decisions and stock movement. Open price is critical for modeling, and Monthly_Return_% reflects recent stock momentum useful in forecasting. These features were chosen because they are informative, less correlated, and directly influence or summarize the stockâ€™s behavior."
      ],
      "metadata": {
        "id": "fGgaEstsBnaf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 5. Data Transformation"
      ],
      "metadata": {
        "id": "TNVZ9zx19K6k"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Do you think that your data needs to be transformed? If yes, which transformation have you used. Explain Why?"
      ],
      "metadata": {
        "id": "nqoHp30x9hH9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Transform Your data\n",
        "# Need transformation to improve model performance apply logtransormation and minmax scaling\n",
        "# log transformation to reduse right skewness in features like close and price range and make data more normally disributed\n",
        "new_data.dropna(inplace=True)\n",
        "new_data['Log_Close'] = np.log1p(new_data['Close'])\n",
        "new_data['Log_Price_Range'] =np.log1p(new_data['Price_Range'])\n"
      ],
      "metadata": {
        "id": "I6quWQ1T9rtH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 6. Data Scaling"
      ],
      "metadata": {
        "id": "rMDnDkt2B6du"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Scaling your data\n",
        "# Use minmax Scaler for scaling\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "scaler=MinMaxScaler()\n",
        "scal_col = ['Open','Avg_Price', 'Monthly_Return_%', 'Log_Close', 'Log_Price_Range']\n",
        "data_scaled = new_data.copy()\n",
        "data_scaled[scal_col] = scaler.fit_transform(new_data[scal_col])\n",
        "data_scaled.head()"
      ],
      "metadata": {
        "id": "dL9LWpySC6x_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data_scaled.shape"
      ],
      "metadata": {
        "id": "1D0eOxc0F0He"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which method have you used to scale you data and why?\n",
        "Use MinMax Saler,it transform all feature values in to a uniform range [0,1]\n",
        "It works well when the data does not contain extreme outliers. The goal was to make the model training more stable and faster by aligning feature scales."
      ],
      "metadata": {
        "id": "yiiVWRdJDDil"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 7. Dimesionality Reduction"
      ],
      "metadata": {
        "id": "1UUpS68QDMuG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Do you think that dimensionality reduction is needed? Explain Why?"
      ],
      "metadata": {
        "id": "kexQrXU-DjzY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In this case dimensionality reduction is not needed."
      ],
      "metadata": {
        "id": "GGRlBsSGDtTQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# DImensionality Reduction (If needed)"
      ],
      "metadata": {
        "id": "kQfvxBBHDvCa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which dimensionality reduction technique have you used and why? (If dimensionality reduction done on dataset.)"
      ],
      "metadata": {
        "id": "T5CmagL3EC8N"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here."
      ],
      "metadata": {
        "id": "ZKr75IDuEM7t"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "data_scaled['Year'] = data_scaled['Date'].dt.year\n",
        "data_scaled['Month'] = data_scaled['Date'].dt.month\n",
        "data_scaled['Day'] = data_scaled['Date'].dt.day"
      ],
      "metadata": {
        "id": "UpiaM4acP_e9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 8. Data Splitting"
      ],
      "metadata": {
        "id": "BhH2vgX9EjGr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Split your data to train and test. Choose Splitting ratio wisely.\n",
        "y = data_scaled['Close']\n",
        "X = data_scaled.drop(['Close','Date'], axis=1)\n",
        "from sklearn.model_selection import train_test_split\n",
        "X_train, X_test, y_train,y_test = train_test_split(X, y, test_size = 0.2, random_state =42)\n",
        "print(X.head())\n"
      ],
      "metadata": {
        "id": "0CTyd2UwEyNM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(y.head())"
      ],
      "metadata": {
        "id": "BnQilWlyER8u"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X.shape,y.shape"
      ],
      "metadata": {
        "id": "4HlAkdHBXpcK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "split_index = int(len(X) * 0.8)\n",
        "#.iloc lets you manually split data by row index, preserving the chronological order.\n",
        "X_train, X_test = X.iloc[:split_index], X.iloc[split_index:]\n",
        "y_train, y_test = y.iloc[:split_index], y.iloc[split_index:]\n",
        "X_train.shape,X_test.shape, y_train.shape,y_test.shape"
      ],
      "metadata": {
        "id": "otsfjQhIFLG0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### What data splitting ratio have you used and why?"
      ],
      "metadata": {
        "id": "qjKvONjwE8ra"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here use 80:20 split. first 80% records are in train and last 20% records are in test. Use chronological order,to avoid data leakage. Random spliting avoid to real word senarios"
      ],
      "metadata": {
        "id": "Y2lJ8cobFDb_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 9. Handling Imbalanced Dataset"
      ],
      "metadata": {
        "id": "P1XJ9OREExlT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Do you think the dataset is imbalanced? Explain Why."
      ],
      "metadata": {
        "id": "VFOzZv6IFROw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Imbalanced dataset only cosider in classification problem. It is a regression task."
      ],
      "metadata": {
        "id": "GeKDIv7pFgcC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Handling Imbalanced Dataset (If needed)"
      ],
      "metadata": {
        "id": "nQsRhhZLFiDs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### What technique did you use to handle the imbalance dataset and why? (If needed to be balanced)"
      ],
      "metadata": {
        "id": "TIqpNgepFxVj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Imbalenced dataset considered in classification problems. It is clearly a regression problem"
      ],
      "metadata": {
        "id": "qbet1HwdGDTz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***7. ML Model Implementation***"
      ],
      "metadata": {
        "id": "VfCC591jGiD4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ML Model - 1"
      ],
      "metadata": {
        "id": "OB4l2ZhMeS1U"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ML Model - 1 Implementation\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
        "# Fit the Algorithm\n",
        "# Extract useful features\n",
        "model = LinearRegression()\n",
        "model.fit(X_train, y_train)\n",
        "# Predict on the model\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "# Evaluate\n",
        "mae = mean_absolute_error(y_test, y_pred)\n",
        "mse = mean_squared_error(y_test, y_pred)\n",
        "rmse = np.sqrt(mse)\n",
        "r2 = r2_score(y_test, y_pred)\n",
        "\n",
        "# Print metrics\n",
        "print(f\"MAE: {mae:.2f}\")\n",
        "print(f\"MSE: {mse:.2f}\")\n",
        "print(f\"RMSE: {rmse:.2f}\")\n",
        "print(f\"RÂ² Score: {r2:.2f}\")"
      ],
      "metadata": {
        "id": "7ebyywQieS1U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1. Explain the ML Model used and it's performance using Evaluation metric Score Chart."
      ],
      "metadata": {
        "id": "ArJBuiUVfxKd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Visualizing evaluation Metric Score chart\n",
        "metrics_names = ['MAE', 'MSE', 'RMSE', 'RÂ² Score']\n",
        "metric_values = [mae, mse, rmse, r2]\n",
        "\n",
        "# Create a temporary DataFrame\n",
        "metrics_df = pd.DataFrame({'Metric': metrics_names, 'Score': metric_values})\n",
        "\n",
        "# Plot using seaborn\n",
        "plt.figure(figsize=(8, 5))\n",
        "sns.barplot(data=metrics_df, x='Metric', y='Score', palette='pastel')\n",
        "plt.title('Linear Regression Model Evaluation Metrics')\n",
        "plt.ylabel('Score')\n",
        "\n",
        "# Annotate values on bars\n",
        "for index, row in metrics_df.iterrows():\n",
        "    plt.text(index, row.Score + 0.01, f'{row.Score:.2f}', ha='center', va='bottom')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "FdQgTcJzRlJ_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2. Cross- Validation & Hyperparameter Tuning"
      ],
      "metadata": {
        "id": "4qY1EAkEfxKe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#cross validation\n",
        "from sklearn.model_selection import cross_val_score\n",
        "from sklearn.linear_model import LinearRegression\n",
        "model = LinearRegression()\n",
        "\n",
        "# 5-Fold Cross-Validation (RÂ² Score)\n",
        "cv_scores = cross_val_score(model, X, y, cv=5, scoring='r2')\n",
        "\n",
        "print(\"Cross-Validation RÂ² Scores:\", cv_scores)\n",
        "print(\"Mean RÂ² Score:\", np.mean(cv_scores))"
      ],
      "metadata": {
        "id": "RKpxLo6HZzOh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ML Model - 1 Implementation with hyperparameter optimization techniques (i.e., GridSearch CV, RandomSearch CV, Bayesian Optimization etc.)\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "\n",
        "# Define model\n",
        "model = LinearRegression()\n",
        "\n",
        "# Define grid\n",
        "param_grid = {\n",
        "    'fit_intercept': [True, False], # Whether to calculate the intercept for this model.\n",
        "    'copy_X': [True, False],\n",
        "}\n",
        "\n",
        "# Grid search with 5-fold cross-validation\n",
        "grid_search = GridSearchCV(estimator=model, param_grid=param_grid,\n",
        "                           cv=5, scoring='r2', n_jobs=-1)\n",
        "\n",
        "# Fit search\n",
        "grid_search.fit(X, y)\n",
        "\n",
        "# Best parameters\n",
        "print(\"Best Params:\", grid_search.best_params_)\n",
        "print(\"Best RÂ² Score:\", grid_search.best_score_)\n"
      ],
      "metadata": {
        "id": "Dy61ujd6fxKe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which hyperparameter optimization technique have you used and why?"
      ],
      "metadata": {
        "id": "PiV4Ypx8fxKe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here i uesd the GridSearchCV to find the best hyperparameter for the model. It check all combinations and pick the best one. It also used cross validation checking to over come overfitting and work well on unseen data. This make model more reliable and accurate"
      ],
      "metadata": {
        "id": "negyGRa7fxKf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Have you seen any improvement? Note down the improvement with updates Evaluation metric Score Chart."
      ],
      "metadata": {
        "id": "TfvqoZmBfxKf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Yes,i have seen the improvement. In Linear regression r2 is 0.99 ie, possibly over fitting. After hyper parameter tunning r2 value is 0.94, which is more reliable\n"
      ],
      "metadata": {
        "id": "MmIXLaEAg64s"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ML Model - 2"
      ],
      "metadata": {
        "id": "oEU42DhZR_fW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ML Model - 2 Implementation\n",
        "from sklearn.tree import DecisionTreeRegressor\n",
        "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
        "# Initialize the model\n",
        "dt_model = DecisionTreeRegressor(random_state=42)\n",
        "dt_model.fit(X_train, y_train)\n",
        "y_pred_dt = dt_model.predict(X_test)\n",
        "mae_dt = mean_absolute_error(y_test, y_pred_dt)\n",
        "mse_dt = mean_squared_error(y_test, y_pred_dt)\n",
        "rmse_dt = np.sqrt(mse_dt)\n",
        "r2_dt = r2_score(y_test, y_pred_dt)\n",
        "\n",
        "print(f\"MAE: {mae_dt:.2f}\")\n",
        "print(f\"MSE: {mse_dt:.2f}\")\n",
        "print(f\"RMSE: {rmse_dt:.2f}\")\n",
        "print(f\"RÂ² Score: {r2_dt:.2f}\")"
      ],
      "metadata": {
        "id": "OJ72b9gaO-dc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1. Explain the ML Model used and it's performance using Evaluation metric Score Chart."
      ],
      "metadata": {
        "id": "OcEg9jqVSecL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Visualize Model Performance\n",
        "\n",
        "metrics_dt = ['MAE', 'MSE', 'RMSE', 'RÂ² Score']\n",
        "values_dt = [mae_dt, mse_dt, rmse_dt, r2_dt]\n",
        "\n",
        "metrics_df = pd.DataFrame({'Metric': metrics_dt, 'Score': values_dt})\n",
        "\n",
        "# Plot\n",
        "plt.figure(figsize=(8, 5))\n",
        "sns.barplot(data=metrics_df, x='Metric', y='Score', palette='viridis')\n",
        "\n",
        "# Add values on top of bars\n",
        "for i, val in enumerate(values_dt):\n",
        "    plt.text(i, val + 0.5, f\"{val:.2f}\", ha='center', va='bottom')\n",
        "\n",
        "plt.title('Decision Tree Regressor Evaluation Metrics')\n",
        "plt.ylabel('Score')\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "p5XI5aWRPcdE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2. Cross- Validation & Hyperparameter Tuning"
      ],
      "metadata": {
        "id": "GIL1DGSkStN_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import cross_val_score\n",
        "from sklearn.tree import DecisionTreeRegressor\n",
        "\n",
        "# Initialize model\n",
        "dt_model = DecisionTreeRegressor(random_state=42)\n",
        "\n",
        "# 5-Fold Cross-Validation using RÂ² Score\n",
        "cv_scores = cross_val_score(dt_model, X, y, cv=5, scoring='r2')\n",
        "\n",
        "# Print results\n",
        "print(\"Cross-Validation RÂ² Scores:\", cv_scores)\n",
        "print(\"Mean RÂ² Score:\", cv_scores.mean())"
      ],
      "metadata": {
        "id": "tCQ11rifRU6h"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.tree import DecisionTreeRegressor\n",
        "\n",
        "# Define parameter grid to tune\n",
        "param_grid = {\n",
        "    'max_depth': [3, 5, 10, 15],\n",
        "    'min_samples_split': [2, 5, 10],\n",
        "    'min_samples_leaf': [1, 2, 4]\n",
        "}\n",
        "\n",
        "# Initialize model\n",
        "dt = DecisionTreeRegressor(random_state=42)\n",
        "\n",
        "# GridSearchCV setup\n",
        "grid_search = GridSearchCV(estimator=dt, param_grid=param_grid,\n",
        "                           cv=5, scoring='r2', n_jobs=-1)\n",
        "\n",
        "# Fit to the data\n",
        "grid_search.fit(X, y)\n",
        "\n",
        "# Best parameters and score\n",
        "print(\"Best Parameters:\", grid_search.best_params_)\n",
        "print(\"Best Cross-Validation RÂ² Score:\", grid_search.best_score_)"
      ],
      "metadata": {
        "id": "6rbI-mEzRoT9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which hyperparameter optimization technique have you used and why?"
      ],
      "metadata": {
        "id": "9pj_m-KES9Ts"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "I used GridSearchCV to find the best hyperparameters for the model. It checks all possible combinations and selects the one that gives the best performance. GridSearchCV also uses cross-validation to prevent overfitting and ensure the model works well on unseen data. This makes the model more reliable and improves its generalization ability.\"\n",
        "\n"
      ],
      "metadata": {
        "id": "hEdsZbKxS-kv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Have you seen any improvement? Note down the improvement with updates Evaluation metric Score Chart."
      ],
      "metadata": {
        "id": "F7j9hyPjTGL4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Yes,i have seen the improvement. On decisiontreeregressor r2 is 0.98 ie, a little bit overfitting. After hyper parameter tunning usig gridsearch r2 value is 0.86. The tuned model showed better generalization by reducing overfitting, even though the RÂ² score slightly decreased. This makes the model more reliable for real-world, unseen data."
      ],
      "metadata": {
        "id": "CiVpfctFTHYn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ML Model - 3"
      ],
      "metadata": {
        "id": "dJ2tPlVmpsJ0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1. Explain the ML Model used and it's performance using Evaluation metric Score Chart."
      ],
      "metadata": {
        "id": "JWYfwnehpsJ1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Explain ML model\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
        "\n",
        "# Create the model\n",
        "rf_model = RandomForestRegressor(n_estimators=100, min_samples_split=2, min_samples_leaf=1)\n",
        "\n",
        "# Train the model\n",
        "rf_model.fit(X_train, y_train)\n",
        "\n",
        "# Predict\n",
        "y_pred_rf = rf_model.predict(X_test)\n",
        "\n",
        "# Evaluation Metrics\n",
        "mae_rf = mean_absolute_error(y_test, y_pred_rf)\n",
        "mse_rf = mean_squared_error(y_test, y_pred_rf)\n",
        "rmse_rf = np.sqrt(mse_rf)\n",
        "r2_rf = r2_score(y_test, y_pred_rf)\n",
        "\n",
        "# Print Results\n",
        "print(f\"MAE: {mae_rf:.2f}\")\n",
        "print(f\"MSE: {mse_rf:.2f}\")\n",
        "print(f\"RMSE: {rmse_rf:.2f}\")\n",
        "print(f\"RÂ² Score: {r2_rf:.2f}\")"
      ],
      "metadata": {
        "id": "wE7NR7PYlHDS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Visualizing evaluation Metric Score chart\n",
        "\n",
        "# Create DataFrame for plotting\n",
        "metrics_rf = ['MAE', 'MSE', 'RMSE', 'RÂ² Score']\n",
        "values_rf = [mae_rf, mse_rf, rmse_rf, r2_rf]\n",
        "\n",
        "metrics_df = pd.DataFrame({'Metric': metrics_rf, 'Score': values_rf})\n",
        "\n",
        "# Plot\n",
        "plt.figure(figsize=(8, 5))\n",
        "sns.barplot(data=metrics_df, x='Metric', y='Score', palette='viridis')\n",
        "\n",
        "# Add values on top of bars\n",
        "for i, val in enumerate(values_rf):\n",
        "    plt.text(i, val + 0.5, f\"{val:.2f}\", ha='center', va='bottom')\n",
        "\n",
        "plt.title('Random Forest Evaluation Metrics')\n",
        "plt.ylabel('Score')\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "yEl-hgQWpsJ1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2. Cross- Validation & Hyperparameter Tuning"
      ],
      "metadata": {
        "id": "-jK_YjpMpsJ2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import cross_val_score\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "\n",
        "# Initialize model\n",
        "dt_model = RandomForestRegressor(random_state=42)\n",
        "\n",
        "# 5-Fold Cross-Validation using RÂ² Score\n",
        "cv_scores = cross_val_score(dt_model, X, y, cv=5, scoring='r2')\n",
        "\n",
        "# Print results\n",
        "print(\"Cross-Validation RÂ² Scores:\", cv_scores)\n",
        "print(\"Mean RÂ² Score:\", cv_scores.mean())"
      ],
      "metadata": {
        "id": "jqKqYXUvTgxl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ML Model - 1 Implementation with hyperparameter optimization techniques (i.e., GridSearch CV, RandomSearch CV, Bayesian Optimization etc.)\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "\n",
        "# Define model\n",
        "rf = RandomForestRegressor(random_state=42)\n",
        "\n",
        "# Define grid\n",
        "param_grid = {\n",
        "    'n_estimators': [100, 200],\n",
        "    'max_depth': [5, 10, None],\n",
        "    'min_samples_split': [2, 5]\n",
        "}\n",
        "\n",
        "# Grid search with 5-fold cross-validation\n",
        "grid_search = GridSearchCV(estimator=rf, param_grid=param_grid,\n",
        "                           cv=5, scoring='r2', n_jobs=-1)\n",
        "\n",
        "# Fit search\n",
        "grid_search.fit(X, y)\n",
        "\n",
        "# Best parameters\n",
        "print(\"Best Params:\", grid_search.best_params_)\n",
        "print(\"Best RÂ² Score:\", grid_search.best_score_)\n"
      ],
      "metadata": {
        "id": "Dn0EOfS6psJ2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Compared to all other ML models, the RandomForestRegressor gave the most reliable and accurate results with an RÂ² score of 0.98. After hyperparameter tuning using GridSearchCV, the RÂ² score remained strong at 0.97, showing excellent generalization on unseen data. Its ensemble method reduces overfitting and captures complex patterns, making it the best choice for predicting Yes Bankâ€™s stock prices.**"
      ],
      "metadata": {
        "id": "WqLuvuA6ZSR-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which hyperparameter optimization technique have you used and why?"
      ],
      "metadata": {
        "id": "HAih1iBOpsJ2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here i uesd the GridSearchCV to find the best hyperparameter for the model. It check all combinations and pick the best one. It also used cross validation checking to over come overfitting and work well on unseen data. This make model more reliable and accurate."
      ],
      "metadata": {
        "id": "9kBgjYcdpsJ2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Have you seen any improvement? Note down the improvement with updates Evaluation metric Score Chart."
      ],
      "metadata": {
        "id": "zVGeBEFhpsJ2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Yes,i have seen the improvement. In Randomregressior r2 is 0.98 ie, Slightly optimistic but strong. After hyper parameter tunning r2 value is 0.97, which is more reliable.The model's error metrics like MAE and RMSE decreased, and the RÂ² score slightly improved. This indicates better accuracy and generalization, making **it more suitable for business use.**\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "74yRdG6UpsJ3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 3. Explain each evaluation metric's indication towards business and the business impact pf the ML model used."
      ],
      "metadata": {
        "id": "bmKjuQ-FpsJ3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "MAE(mean absolute error) - Average of absolute differences between predicted and actual prices. Lower MAE means more accurate daily stock price prediction.\n",
        "\n",
        "MSE(mean squred error) - Average of squared errors. Helps to find if the model is making any serious mistakes.\n",
        "\n",
        "RMSE(root mean squre error) - Square root of MSE, in the same units as target variable. It shows how far off predictions are in real money terms.\n",
        "\n",
        "R2 score - Proportion of variance in price explained by the model. Shows how well your model explains price changes.A higher RÂ² (like 0.97) means your model is reliable and captures real trends.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "BDKtOrBQpsJ3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1. Which Evaluation metrics did you consider for a positive business impact and why?"
      ],
      "metadata": {
        "id": "h_CCil-SKHpo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "These following evaluation metrics for a positive business impact:\n",
        "\n",
        "RÂ² Score â€“ It shows how well the model explains the variance in stock prices. A higher RÂ² means better prediction accuracy, which is crucial for making confident investment decisions.\n",
        "\n",
        "MAE (Mean Absolute Error) â€“ It indicates the average prediction error in actual units (â‚¹), helping stakeholders understand potential financial deviation per prediction.\n",
        "\n",
        "RMSE (Root Mean Squared Error) â€“ It penalizes larger errors more than MAE, which is important in financial forecasting to avoid big losses due to incorrect predictions.\n",
        "\n",
        "These metrics help assess both accuracy and risk, supporting better strategic planning and investment confidence."
      ],
      "metadata": {
        "id": "jHVz9hHDKFms"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2. Which ML model did you choose from the above created models as your final prediction model and why?"
      ],
      "metadata": {
        "id": "cBFFvTBNJzUa"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "I chose the Random Forest Regressor as the final prediction model.\n",
        "\n",
        "It delivered a strong balance between accuracy (RÂ² â‰ˆ 0.98) and generalization with lower error values (MAE, RMSE) after tuning. Unlike the Decision Tree, it is more robust and less prone to overfitting due to its ensemble nature. This makes it highly reliable for real-world stock price predictions and better suited for consistent business impact."
      ],
      "metadata": {
        "id": "6ksF5Q1LKTVm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3. Explain the model which you have used and the feature importance using any model explainability tool?"
      ],
      "metadata": {
        "id": "HvGl1hHyA_VK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "I used the Random Forest Regressor model for stock price prediction. It is an ensemble learning method that builds multiple decision trees and averages their results, making it more accurate and stable than a single decision tree. Random Forest handles non-linear relationships and feature interactions effectively, which is ideal for stock data.\n",
        "\n",
        "Random Forest provides a built-in way to calculate feature importance, which tells us how much each feature contributes to the prediction."
      ],
      "metadata": {
        "id": "YnvVTiIxBL-C"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***8.*** ***Future Work (Optional)***"
      ],
      "metadata": {
        "id": "EyNgTHvd2WFk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1. Save the best performing ml model in a pickle file or joblib file format for deployment process.\n"
      ],
      "metadata": {
        "id": "KH5McJBi2d8v"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Save the File\n",
        "#import pickle\n",
        "\n",
        "# Save the model\n",
        "#with open('best_model_rf.pkl', 'wb') as file:\n",
        "   # pickle.dump(rf_model, file)\n",
        "\n",
        "# To load later:\n",
        "# with open('best_model_rf.pkl', 'rb') as file:\n",
        "#     loaded_model = pickle.load(file)"
      ],
      "metadata": {
        "id": "bQIANRl32f4J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2. Again Load the saved model file and try to predict unseen data for a sanity check.\n"
      ],
      "metadata": {
        "id": "iW_Lq9qf2h6X"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the File and predict unseen data.\n",
        "#import pickle\n",
        "\n",
        "#with open('best_model_rf.pkl', 'rb') as file:\n",
        "    #loaded_model = pickle.load(file)"
      ],
      "metadata": {
        "id": "oEXk9ydD2nVC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ***Congrats! Your model is successfully created and ready for deployment on a live server for a real user interaction !!!***"
      ],
      "metadata": {
        "id": "-Kee-DAl2viO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Conclusion**"
      ],
      "metadata": {
        "id": "gCX9965dhzqZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Write the conclusion here."
      ],
      "metadata": {
        "id": "Fjb1IsQkh3yE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ***Hurrah! You have successfully completed your Machine Learning Capstone Project !!!***"
      ],
      "metadata": {
        "id": "gIfDvo9L0UH2"
      }
    }
  ]
}