{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "collapsed_sections": [
        "mDgbUHAGgjLW",
        "Iwf50b-R2tYG",
        "GMQiZwjn3iu7",
        "WVIkgGqN3qsr",
        "XkPnILGE3zoT",
        "Hlsf0x5436Go",
        "mT9DMSJo4nBL",
        "c49ITxTc407N",
        "OeJFEK0N496M",
        "9ExmJH0g5HBk",
        "cJNqERVU536h",
        "k5UmGsbsOxih",
        "T0VqWOYE6DLQ",
        "qBMux9mC6MCf",
        "bmKjuQ-FpsJ3",
        "-Kee-DAl2viO",
        "gIfDvo9L0UH2"
      ],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sreeproject/AI-/blob/main/Copy_of_Integreted_Retail_Analysis_ML_Submission_Template.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Project Name**    -  Integrated Retail Analysis For Store Optimization\n",
        "\n"
      ],
      "metadata": {
        "id": "vncDsAP0Gaoa"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### **Project Type**    - EDA/Regression/Clustering\n",
        "##### **Contribution**    - Individual"
      ],
      "metadata": {
        "id": "beRrZCGUAJYm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Project Summary -**  \n"
      ],
      "metadata": {
        "id": "FJNUwmbgGyua"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The \"Integrated Retail Analytics for Store Optimization\" project aims to use machine learning and data analysis to\n",
        "\n",
        "optimize store performance and forecast demand. The primary goal is to enhance customer experience through\n",
        "\n",
        "segmentation and personalized marketing strategies.\n",
        "\n",
        "The process is structured around several components:\n",
        "\n",
        "\n",
        "Anomaly Detection in sales data and over time to identify and handle unusual sales patterns.\n",
        "\n",
        "\n",
        "Data Preprocessing and Feature Engineering, which includes managing missing MarkDown values and creating new features from store and regional factors.\n",
        "\n",
        "\n",
        "Demand Forecasting models are built to predict weekly sales for each store and department, incorporating external factors like CPI, unemployment rates, and fuel prices.\n",
        "\n",
        "\n",
        "Customer Segmentation Analysis groups stores based on sales patterns and regional features.\n",
        "\n",
        "\n",
        "Market Basket Analysis is used to infer product associations for cross-selling.\n",
        "\n",
        "Ultimately, the insights from these analyses are used to formulate a comprehensive strategy for inventory management, marketing, and store optimization."
      ],
      "metadata": {
        "id": "F6v_1wHtG2nS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **GitHub Link -**"
      ],
      "metadata": {
        "id": "w6K7xa23Elo4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Provide your GitHub Link here."
      ],
      "metadata": {
        "id": "h1o69JH3Eqqn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Problem Statement**\n"
      ],
      "metadata": {
        "id": "yQaldy8SH6Dl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The goal is to leverage machine learning and data analysis to optimize overall store performance. By analyzing historical sales and inventory data, we can forecast demand more accurately and reduce stockouts or overstock situations. Customer data can be used to create segments, enabling targeted and relevant marketing strategies. Personalized recommendations and promotions can enhance the customer shopping experience. Overall, these approaches help drive sales growth, operational efficiency, and customer satisfaction.\n",
        "\n"
      ],
      "metadata": {
        "id": "DpeJGUA3kjGy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **General Guidelines** : -  "
      ],
      "metadata": {
        "id": "mDgbUHAGgjLW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1.   Well-structured, formatted, and commented code is required.\n",
        "2.   Exception Handling, Production Grade Code & Deployment Ready Code will be a plus. Those students will be awarded some additional credits.\n",
        "     \n",
        "     The additional credits will have advantages over other students during Star Student selection.\n",
        "       \n",
        "             [ Note: - Deployment Ready Code is defined as, the whole .ipynb notebook should be executable in one go\n",
        "                       without a single error logged. ]\n",
        "\n",
        "3.   Each and every logic should have proper comments.\n",
        "4. You may add as many number of charts you want. Make Sure for each and every chart the following format should be answered.\n",
        "        \n",
        "\n",
        "```\n",
        "# Chart visualization code\n",
        "```\n",
        "            \n",
        "\n",
        "*   Why did you pick the specific chart?\n",
        "*   What is/are the insight(s) found from the chart?\n",
        "* Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason.\n",
        "\n",
        "5. You have to create at least 15 logical & meaningful charts having important insights.\n",
        "\n",
        "\n",
        "[ Hints : - Do the Vizualization in  a structured way while following \"UBM\" Rule.\n",
        "\n",
        "U - Univariate Analysis,\n",
        "\n",
        "B - Bivariate Analysis (Numerical - Categorical, Numerical - Numerical, Categorical - Categorical)\n",
        "\n",
        "M - Multivariate Analysis\n",
        " ]\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "6. You may add more ml algorithms for model creation. Make sure for each and every algorithm, the following format should be answered.\n",
        "\n",
        "\n",
        "*   Explain the ML Model used and it's performance using Evaluation metric Score Chart.\n",
        "\n",
        "\n",
        "*   Cross- Validation & Hyperparameter Tuning\n",
        "\n",
        "*   Have you seen any improvement? Note down the improvement with updates Evaluation metric Score Chart.\n",
        "\n",
        "*   Explain each evaluation metric's indication towards business and the business impact pf the ML model used.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "ZrxVaUj-hHfC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ***Let's Begin !***"
      ],
      "metadata": {
        "id": "O_i_v8NEhb9l"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***1. Know Your Data***"
      ],
      "metadata": {
        "id": "HhfV-JJviCcP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Import Libraries"
      ],
      "metadata": {
        "id": "Y3lxredqlCYt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import Libraries\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")\n"
      ],
      "metadata": {
        "id": "M8Vqi-pPk-HR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Dataset Loading"
      ],
      "metadata": {
        "id": "3RnN4peoiCZX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load Dataset\n",
        "features_df = pd.read_csv('Features data set.csv')\n",
        "sales_df = pd.read_csv('sales data-set.csv')\n",
        "stores_df = pd.read_csv('stores data-set.csv')"
      ],
      "metadata": {
        "id": "4CkvbW_SlZ_R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Dataset First View"
      ],
      "metadata": {
        "id": "x71ZqKXriCWQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Dataset First Look\n",
        "features_df.head()"
      ],
      "metadata": {
        "id": "LWNFOSvLl09H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sales_df.head()"
      ],
      "metadata": {
        "id": "lnFgMf4_w4mc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "stores_df.head()"
      ],
      "metadata": {
        "id": "m6iN0ZCMw8y2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Dataset Rows & Columns count"
      ],
      "metadata": {
        "id": "7hBIi_osiCS2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Dataset Rows & Columns count\n",
        "features_df.shape"
      ],
      "metadata": {
        "id": "Kllu7SJgmLij"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "features_df.columns"
      ],
      "metadata": {
        "id": "s7ceh1CwxTL_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sales_df.shape"
      ],
      "metadata": {
        "id": "KvLw22E0xsFK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sales_df.columns"
      ],
      "metadata": {
        "id": "LIQLxa-gx3I-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "stores_df.shape"
      ],
      "metadata": {
        "id": "AuIGViCCyTNE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sales_df.columns"
      ],
      "metadata": {
        "id": "ArPO00ZFyZAI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Merge the datasets to create final dataframe"
      ],
      "metadata": {
        "id": "u91LnkZe1gfJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Merge sales and features dataframes on Store and Date\n",
        "df_merged = pd.merge(sales_df, features_df, on=['Store', 'Date','IsHoliday'], how='left')\n",
        "\n",
        "# Merge the result with stores dataframe on Store\n",
        "final_df = pd.merge(df_merged, stores_df, on='Store', how='left')\n",
        "final_df.head()"
      ],
      "metadata": {
        "id": "ZbmD72Xl376g"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "final_df.shape"
      ],
      "metadata": {
        "id": "HKMRq-ng33jh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Convert 'Date' to datetime objects\n",
        "final_df['Date'] = pd.to_datetime(df_merged['Date'], format='%d/%m/%Y')"
      ],
      "metadata": {
        "id": "d-hyl3lk9A6o"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "final_df.head()"
      ],
      "metadata": {
        "id": "ygmGEB4O44br"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Dataset Information"
      ],
      "metadata": {
        "id": "JlHwYmJAmNHm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Dataset Info\n",
        "final_df.info()"
      ],
      "metadata": {
        "id": "e9hRXRi6meOf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Duplicate Values"
      ],
      "metadata": {
        "id": "35m5QtbWiB9F"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Dataset Duplicate Value Count\n",
        "print(\"duplicate values in feature data:\", final_df.duplicated().sum())"
      ],
      "metadata": {
        "id": "1sLdpKYkmox0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Missing Values/Null Values"
      ],
      "metadata": {
        "id": "PoPl-ycgm1ru"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Missing Values/Null Values Count\n",
        "print(\"missing values in feature data:\", final_df.isnull().sum().sum())\n"
      ],
      "metadata": {
        "id": "GgHWkxvamxVg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "final_df.isnull().sum()"
      ],
      "metadata": {
        "id": "qEZNcP-L1Qcb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Visualizing the missing values\n",
        "# --- Heatmap of missing values ---\n",
        "plt.figure(figsize=(10,6))\n",
        "sns.heatmap(final_df.isnull(), cbar=False, cmap=\"viridis\")\n",
        "plt.title(\"Missing Values Heatmap - Final Dataset\")\n",
        "plt.show()\n",
        "\n",
        "# --- Bar chart of missing percentages ---\n",
        "missing_percent = final_df.isnull().mean() * 100\n",
        "plt.figure(figsize=(8,5))\n",
        "missing_percent.plot(kind='bar', color='tomato')\n",
        "plt.title(\"Percentage of Missing Values - Final Dataset\")\n",
        "plt.ylabel(\"Percentage (%)\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "3q5wnI3om9sJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Handling missing values"
      ],
      "metadata": {
        "id": "r-rLhhYPesUx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Fill missing MarkDown values with 0\n",
        "markdown_cols = ['MarkDown1', 'MarkDown2', 'MarkDown3', 'MarkDown4', 'MarkDown5']\n",
        "final_df[markdown_cols] = final_df[markdown_cols].fillna(0)"
      ],
      "metadata": {
        "id": "Z4RCDNUB69SV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "final_df.head()"
      ],
      "metadata": {
        "id": "1YotnJe27RcH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### What did you know about your dataset?"
      ],
      "metadata": {
        "id": "H0kj-8xxnORC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here"
      ],
      "metadata": {
        "id": "gfoNAAC-nUe_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "final_df"
      ],
      "metadata": {
        "id": "e0NKw3-bsiBr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***2. Understanding Your Variables***"
      ],
      "metadata": {
        "id": "nA9Y7ga8ng1Z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Dataset Columns\n",
        "final_df.columns"
      ],
      "metadata": {
        "id": "j7xfkqrt5Ag5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "final_df.dtypes"
      ],
      "metadata": {
        "id": "NhPq9FbFGX1I"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Dataset Describe\n",
        "print(\"\\nSummary Statistics:\\n\", final_df.describe())"
      ],
      "metadata": {
        "id": "DnOaZdaE5Q5t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Variables Description"
      ],
      "metadata": {
        "id": "PBTbrJXOngz2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Store -- Unique store ID (1–45) for each store.\n",
        "\n",
        "Dept -- Department number within a store(1-100)\n",
        "\n",
        "Date -- Week-ending date of sales.\n",
        "\n",
        "Weekly_Sales -- Sales for the given department, store, and week (in dollars).\n",
        "\n",
        "IsHoliday -- Whether the week includes a holiday.\n",
        "\n",
        " True = Holiday week\n",
        "\n",
        " False = Regular week.\n",
        "\n",
        "Temperature -- Average temperature for the week\n",
        "\n",
        "Fuel_Price -- Average cost of fuel\n",
        "\n",
        "MarkDown1-5 -- Promotional markdown values for that week. Represent discounts on different product categories.\n",
        "\n",
        "CPI(Consumer Price Index) -- Measure of inflation/price level. Higher CPI means goods/services are more expensive.\n",
        "\n",
        "Unemployment -- Regional unemployment rate (%).Reflects local economic conditions affecting consumer spending.  \n",
        "\n",
        "Type -- Store type (A, B, C).\n",
        "\n",
        "Size -- Physical size of the store (square feet).Larger stores usually have higher sales capacity."
      ],
      "metadata": {
        "id": "aJV4KIxSnxay"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Check Unique Values for each variable."
      ],
      "metadata": {
        "id": "u3PMJOP6ngxN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Check Unique Values for each variable.\n",
        "final_df.nunique()"
      ],
      "metadata": {
        "id": "zms12Yq5n-jE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3. ***Data Wrangling***"
      ],
      "metadata": {
        "id": "dauF4eBmngu3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Data Wrangling Code"
      ],
      "metadata": {
        "id": "bKJF3rekwFvQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create new features from 'Date'\n",
        "final_df['Year'] = final_df['Date'].dt.year\n",
        "final_df['Month'] = final_df['Date'].dt.month\n",
        "final_df['Week'] = final_df['Date'].dt.isocalendar().week.astype(int)\n",
        "final_df['Day'] =final_df['Date'].dt.day"
      ],
      "metadata": {
        "id": "88Urgi69rPKv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Write your code to make your dataset analysis ready."
      ],
      "metadata": {
        "id": "wk-9a2fpoLcV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***4. Data Vizualization, Storytelling & Experimenting with charts : Understand the relationships between variables***"
      ],
      "metadata": {
        "id": "GF8Ens_Soomf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 1"
      ],
      "metadata": {
        "id": "0wOQAZs5pc--"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 1 visualization code\n",
        "# Sales trend over time ---\n",
        "sales_trend = final_df.groupby(\"Date\")[\"Weekly_Sales\"].sum()\n",
        "plt.figure(figsize=(12,5))\n",
        "sales_trend.plot()\n",
        "plt.title(\"Total Weekly Sales Over Time\")\n",
        "plt.xlabel(\"Date\")\n",
        "plt.ylabel(\"Weekly Sales\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "7v_ESjsspbW7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "K5QZ13OEpz2H"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The graph shows weekly sales trends from mid-2010 to mid-2012. Sales fluctuate between 40 million and 80 million, with two sharp spikes around January 2011 and January 2012, likely indicating seasonal peaks—possibly due to holiday shopping or major promotions."
      ],
      "metadata": {
        "id": "XESiWehPqBRc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "lQ7QKXXCp7Bj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The chart shows weekly sales from mid-2010 to mid-2012, with consistent fluctuations between 40M and 60M. Sharp spikes in January 2011 and January 2012 suggest strong seasonal demand, likely tied to holiday or promotional events. These patterns highlight predictable cycles that can inform inventory planning and marketing strategies."
      ],
      "metadata": {
        "id": "C_j1G7yiqdRP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "448CDAPjqfQr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Yes, the insights from the chart can drive positive business impact. The clear seasonal spikes in January suggest predictable high-demand periods, allowing retailers to optimize inventory, staffing, and promotions to maximize revenue. However, the absence of long-term growth across two years may signal stagnation—if sales outside peak seasons remain flat, it could indicate missed opportunities in customer engagement or product strategy, potentially leading to negative growth if not addressed."
      ],
      "metadata": {
        "id": "3cspy4FjqxJW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 2"
      ],
      "metadata": {
        "id": "KSlN3yHqYklG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 2 visualization code\n",
        "# Holiday vs Non-Holiday Sales ---\n",
        "holiday_sales = final_df.groupby(\"IsHoliday\")[\"Weekly_Sales\"].mean()\n",
        "plt.figure(figsize=(6,4))\n",
        "holiday_sales.plot(kind='bar', color=['skyblue','orange'])\n",
        "plt.title(\"Average Weekly Sales: Holiday vs Non-Holiday\")\n",
        "plt.ylabel(\"Average Sales\")\n",
        "plt.xticks([0,1],[\"Non-Holiday\",\"Holiday\"], rotation=0)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "R4YgtaqtYklH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "t6dVpIINYklI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The chart compares average weekly sales during holiday and non-holiday periods. Holiday weeks show slightly higher sales, averaging just above 17,000, while non-holiday weeks average just above 16,000. This suggests holidays positively influence consumer spending and retail performance."
      ],
      "metadata": {
        "id": "5aaW0BYyYklI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "ijmpgYnKYklI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The chart reveals that average weekly sales are higher during holiday weeks compared to non-holiday weeks, indicating a clear boost in consumer spending during festive periods. This suggests that holidays are a key driver of retail performance and should be strategically leveraged for promotions and inventory planning. The difference, though modest, highlights the importance of aligning marketing and operations with seasonal demand."
      ],
      "metadata": {
        "id": "PSx9atu2YklI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "-JiQyfWJYklI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Yes, the insights from the chart can help create a positive business impact. The higher average sales during holiday weeks highlight the importance of aligning promotions, inventory, and staffing with seasonal demand to maximize revenue. There are no direct signs of negative growth, but if non-holiday sales remain stagnant or decline over time, it may indicate over-reliance on holiday periods—suggesting a need to strengthen engagement and sales strategies during regular weeks."
      ],
      "metadata": {
        "id": "BcBbebzrYklV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 3"
      ],
      "metadata": {
        "id": "EM7whBJCYoAo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 3 visualization code\n",
        "# Sales by Store Type ---\n",
        "store_sales = final_df.groupby(\"Type\")[\"Weekly_Sales\"].mean()\n",
        "plt.figure(figsize=(6,4))\n",
        "store_sales.plot(kind='bar', color='teal')\n",
        "plt.title(\"Average Weekly Sales by Store Type\")\n",
        "plt.ylabel(\"Average Sales\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "t6GMdE67YoAp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "fge-S5ZAYoAp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The chart compares average weekly sales across three store types—Type A, Type B, and Type C. Type A leads with nearly 20,000 in average sales, followed by Type B at around 12,000, and Type C trailing at approximately 8,000. This highlights significant performance differences across store formats, useful for strategic resource allocation and store-level planning."
      ],
      "metadata": {
        "id": "5dBItgRVYoAp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "85gYPyotYoAp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The chart shows that Store Type A consistently achieves the highest average weekly sales, followed by Type B and then Type C. This indicates that Store Type A is the most profitable format, suggesting it may benefit from further investment or replication. The performance gap also highlights opportunities to analyze and improve strategies for lower-performing store types."
      ],
      "metadata": {
        "id": "4jstXR6OYoAp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "RoGjAbkUYoAp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Yes, the insights from the chart can support positive business impact. Store Type A shows significantly higher average weekly sales, suggesting it’s the most profitable format and a strong candidate for expansion, investment, or replication. However, the lower performance of Store Types B and C may indicate inefficiencies or underutilized potential—if not addressed, these gaps could lead to negative growth due to missed revenue opportunities or poor resource allocation. Understanding what drives Type A’s success can help uplift the others."
      ],
      "metadata": {
        "id": "zfJ8IqMcYoAp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 4"
      ],
      "metadata": {
        "id": "4Of9eVA-YrdM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 4 visualization code\n",
        "# Sales by Department Type ---\n",
        "store_sales = final_df.groupby(\"Dept\")[\"Weekly_Sales\"].sum()\n",
        "plt.figure(figsize=(6,4))\n",
        "store_sales.plot()\n",
        "plt.title(\"Total Weekly Sales by Dept Type\")\n",
        "plt.xlabel(\"Dept\")\n",
        "plt.ylabel(\"Weekly Sales\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "irlUoxc8YrdO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "iky9q4vBYrdO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The graph displays weekly sales across department numbers ranging from 0 to 100. Sales values fluctuate significantly, with several sharp peaks and dips, indicating that some departments consistently outperform others. This variation highlights opportunities for targeted optimization and resource allocation based on department-level performance."
      ],
      "metadata": {
        "id": "aJRCwT6DYrdO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "F6T5p64dYrdO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The chart reveals that weekly sales vary significantly across departments, with certain departments consistently generating much higher sales than others. These peaks indicate high-performing departments that likely drive overall revenue, while troughs suggest underperforming areas that may need strategic review. This variability highlights the importance of department-level analysis for targeted resource allocation and performance optimization."
      ],
      "metadata": {
        "id": "Xx8WAJvtYrdO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "y-Ehk30pYrdP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Yes, the insights from the chart can drive positive business impact. The clear variation in weekly sales across departments allows businesses to identify high-performing areas and allocate resources more effectively, boosting overall efficiency and profitability. However, departments with consistently low sales may signal underperformance or misalignment with customer demand—if left unaddressed, this could lead to negative growth due to wasted inventory, poor space utilization, or missed revenue opportunities. Strategic intervention in these weaker departments is essential to prevent long-term decline."
      ],
      "metadata": {
        "id": "jLNxxz7MYrdP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 5"
      ],
      "metadata": {
        "id": "bamQiAODYuh1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 5 visualization code\n",
        "# Sales by CPI ---\n",
        "store_sales = final_df.groupby(\"CPI\")[\"Weekly_Sales\"].sum()\n",
        "plt.figure(figsize=(6,4))\n",
        "store_sales.plot()\n",
        "plt.title(\"Total Weekly Sales by CPI\")\n",
        "plt.xlabel(\"CPI\")\n",
        "plt.ylabel(\"Weekly Sales\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "TIJwrbroYuh3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "QHF8YVU7Yuh3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The chart shows how weekly sales vary with changes in the Consumer Price Index (CPI). Sales are clustered around CPI values of 120–140, where they peak near 20 million, but drop noticeably as CPI rises beyond 140, suggesting that higher inflation may negatively impact consumer spending and overall sales performance."
      ],
      "metadata": {
        "id": "dcxuIMRPYuh3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "GwzvFGzlYuh3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The chart shows a clear relationship between Consumer Price Index (CPI) and weekly sales. Sales are highest when CPI values range between 120 and 140, indicating strong consumer spending during moderate inflation. As CPI rises beyond 140, weekly sales drop significantly, suggesting that higher inflation may negatively impact purchasing behavior and overall retail performance. This pattern highlights CPI as a key economic factor influencing sales trends."
      ],
      "metadata": {
        "id": "uyqkiB8YYuh3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "qYpmQ266Yuh3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Yes, the insights from the scatter plot can help create a positive business impact. The clear drop in weekly sales as CPI increases beyond 140 suggests that rising inflation negatively affects consumer spending. Recognizing this trend allows businesses to adjust pricing strategies, optimize product mix, and plan promotions during high-CPI periods to maintain revenue. If ignored, the correlation between high CPI and reduced sales could lead to negative growth, as consumers may cut back on purchases due to reduced purchasing power—especially in price-sensitive segments. Proactive planning based on CPI trends is essential to mitigate this risk."
      ],
      "metadata": {
        "id": "_WtzZ_hCYuh4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 6"
      ],
      "metadata": {
        "id": "OH-pJp9IphqM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 6 visualization code\n",
        "store_sales = final_df.groupby(\"Fuel_Price\")[\"Weekly_Sales\"].sum()\n",
        "plt.figure(figsize=(6,4))\n",
        "store_sales.plot()\n",
        "plt.title(\"Total Weekly Sales by CPI\")\n",
        "plt.xlabel(\"CPI\")\n",
        "plt.ylabel(\"Weekly Sales\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "kuRf4wtuphqN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "bbFf2-_FphqN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The graph shows how weekly sales fluctuate across different Consumer Price Index (CPI) values ranging from 2.50 to 4.50. Sales vary noticeably, with several spikes and dense clusters, indicating that changes in CPI levels influence consumer spending patterns. This relationship is useful for understanding how inflation impacts retail performance."
      ],
      "metadata": {
        "id": "loh7H2nzphqN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 7"
      ],
      "metadata": {
        "id": "PIIx-8_IphqN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 7 visualization code\n",
        "# Sales by CPI ---\n",
        "store_sales = final_df.groupby(\"Unemployment\")[\"Weekly_Sales\"].sum()\n",
        "plt.figure(figsize=(6,4))\n",
        "store_sales.plot()\n",
        "plt.title(\"Total Weekly Sales by Unemployment\")\n",
        "plt.xlabel(\"Unemployment\")\n",
        "plt.ylabel(\"Weekly Sales\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "lqAIGUfyphqO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "t27r6nlMphqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The graph shows how weekly sales fluctuate across different unemployment rates ranging from 3 to 15. Sales are highly variable between 5 and 9, with frequent spikes and drops, while they appear more stable outside this range. This suggests that moderate unemployment levels may coincide with unpredictable consumer spending behavior."
      ],
      "metadata": {
        "id": "iv6ro40sphqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 8"
      ],
      "metadata": {
        "id": "BZR9WyysphqO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 8 visualization code\n",
        "#Sales by Store Size\n",
        "store_sales = final_df.groupby(\"Size\")[\"Weekly_Sales\"].mean()\n",
        "plt.figure(figsize=(6,4))\n",
        "store_sales.plot(kind='bar', color='teal')\n",
        "plt.title(\"Average Weekly Sales by Store size\")\n",
        "plt.ylabel(\"Average Sales\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "TdPTWpAVphqO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "jj7wYXLtphqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The chart illustrates how average weekly sales vary across different store sizes. Larger stores generally show higher average sales, with noticeable variation among specific size categories. This suggests that store size plays a significant role in sales performance and can inform decisions on expansion, layout, and resource allocation."
      ],
      "metadata": {
        "id": "Ob8u6rCTphqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 9"
      ],
      "metadata": {
        "id": "YJ55k-q6phqO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 9 visualization code\n",
        "#Weekly Sales Distribution\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.hist(df_merged['Weekly_Sales'], bins=50)\n",
        "plt.title('Distribution of Weekly Sales')\n",
        "plt.xlabel('Weekly Sales')\n",
        "plt.ylabel('Frequency')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "B2aS4O1ophqO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "gCFgpxoyphqP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The chart shows a right-skewed distribution of weekly sales, with most values concentrated below 50,000. As sales amounts increase, their frequency drops sharply, indicating that high weekly sales are rare, while lower sales are much more common. This pattern is useful for identifying typical performance ranges and spotting outliers."
      ],
      "metadata": {
        "id": "TVxDimi2phqP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Correlation Heatmap"
      ],
      "metadata": {
        "id": "NC_X3p0fY2L0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Correlation Heatmap visualization code\n",
        "plt.figure(figsize=(10,6))\n",
        "sns.heatmap(final_df[[\"Weekly_Sales\",\"Temperature\",\"Fuel_Price\",\"CPI\",\"Unemployment\",\"Size\"]].corr(),\n",
        "annot=True, cmap=\"coolwarm\", fmt=\".2f\")\n",
        "plt.title(\"Correlation Heatmap\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "xyC9zolEZNRQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "UV0SzAkaZNRQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The heatmap visualizes the relationships between variables like Weekly Sales, Temperature, Fuel Price, CPI, Unemployment, and Store Size. Most correlations are weak, but Store Size shows a moderate positive correlation (0.24) with Weekly Sales, suggesting larger stores tend to generate more revenue. Other variables like CPI, Unemployment, and Fuel Price show minimal or negative correlations with Weekly Sales, indicating limited direct influence."
      ],
      "metadata": {
        "id": "DVPuT8LYZNRQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "YPEH6qLeZNRQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The heatmap shows that Store Size has a moderate positive correlation (0.24) with Weekly Sales, indicating that larger stores tend to generate more revenue. Other variables like Temperature, Fuel Price, CPI, and Unemployment have very weak or negligible correlations with Weekly Sales, suggesting limited direct impact. The strongest negative relationship is between CPI and Unemployment (-0.30), reflecting typical economic dynamics."
      ],
      "metadata": {
        "id": "bfSqtnDqZNRR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Pair Plot"
      ],
      "metadata": {
        "id": "q29F0dvdveiT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Pair Plot visualization code\n",
        "pairplot_cols = [\"Weekly_Sales\", \"Temperature\", \"Fuel_Price\", \"CPI\", \"Unemployment\", \"Size\"]\n",
        "\n",
        "# Sample 5000 rows for faster plotting (optional)\n",
        "sample_df = final_df[pairplot_cols].sample(5000, random_state=42)\n",
        "\n",
        "# Pair Plot\n",
        "sns.pairplot(sample_df, diag_kind=\"kde\", corner=True, plot_kws={\"alpha\":0.5, \"s\":20})\n",
        "plt.suptitle(\"Pair Plot of Key Variables\", y=1.02)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "o58-TEIhveiU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "EXh0U9oCveiU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Weekly Sales show a visible spread when plotted against Store, suggesting that store identity or characteristics may influence sales performance. This pattern hints at store-level factors—like location, size, or management—playing a role in revenue variation."
      ],
      "metadata": {
        "id": "eMmPjTByveiU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Anomaly Detection"
      ],
      "metadata": {
        "id": "RW2PLWRQgABm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Statistical Method (Z-Score / IQR)\n",
        "\n",
        "Compute mean & standard deviation (or quartiles).\n",
        "\n",
        "Flag sales points that are too far from the \"normal\" range.\n",
        "\n",
        "Simple, fast, interpretable."
      ],
      "metadata": {
        "id": "MjQheOqFhT87"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Statistical Z-Score Method\n",
        "\n",
        "# Copy sales data\n",
        "sales_df = final_df[[\"Store\", \"Dept\", \"Date\", \"Weekly_Sales\"]].copy()\n",
        "\n",
        "# Calculate Z-score\n",
        "sales_df[\"z_score\"] = (sales_df[\"Weekly_Sales\"] - sales_df[\"Weekly_Sales\"].mean()) / sales_df[\"Weekly_Sales\"].std()\n",
        "\n",
        "# Flag anomalies (Z > 3 or Z < -3)\n",
        "sales_df[\"anomaly\"] = sales_df[\"z_score\"].apply(lambda x: 1 if np.abs(x) > 3 else 0)\n",
        "\n",
        "# Show anomalies\n",
        "anomalies = sales_df[sales_df[\"anomaly\"] == 1]\n",
        "print(f\"Detected {len(anomalies)} anomalies in sales data\")\n",
        "anomalies.head()"
      ],
      "metadata": {
        "id": "IpsV7iUgf-Q0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Visualization of Anomalies\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Pick one store for visualization\n",
        "store_id = 1\n",
        "dept_id = 1\n",
        "\n",
        "store_sales = sales_df[(sales_df[\"Store\"] == store_id) & (sales_df[\"Dept\"] == dept_id)]\n",
        "\n",
        "plt.figure(figsize=(12,6))\n",
        "plt.plot(store_sales[\"Date\"], store_sales[\"Weekly_Sales\"], label=\"Weekly Sales\")\n",
        "plt.scatter(store_sales[store_sales[\"anomaly\"]==1][\"Date\"],\n",
        "            store_sales[store_sales[\"anomaly\"]==1][\"Weekly_Sales\"],\n",
        "            color=\"red\", label=\"Anomalies\")\n",
        "plt.title(f\"Weekly Sales with Anomalies - Store {store_id}, Dept {dept_id}\")\n",
        "plt.xlabel(\"Date\")\n",
        "plt.ylabel(\"Sales\")\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "LLL7o4m7hgjv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The graph tracks weekly sales from 2010 to 2012, showing regular spikes that suggest seasonal or promotional events. Red dots mark anomalies, highlighting unusual deviations from typical sales patterns—either unexpected surges or drops. This visualization is useful for identifying outliers and understanding sales behavior over time."
      ],
      "metadata": {
        "id": "scCJGo7sLcde"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Detecting Unusual Sales Patterns\n",
        "\n",
        "Unusual sales patterns can be:\n",
        "\n",
        "Outliers in absolute values – unusually high/low sales compared to the overall dataset.\n",
        "\n",
        "Outliers in sales trends – e.g., sudden spikes or drops not aligned with other stores.\n",
        "\n",
        "Seasonal misalignment – some stores don’t follow the expected seasonal holiday effects."
      ],
      "metadata": {
        "id": "-7YhcBm9iSFP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Store-Level Outliers\n",
        "# Aggregate sales by store\n",
        "#Compute mean, median, standard deviation of sales per store & per department.\n",
        "#Compare each store/department against global distribution.\n",
        "\n",
        "store_sales = final_df.groupby(\"Store\")[\"Weekly_Sales\"].mean().reset_index()\n",
        "\n",
        "# Compute z-score\n",
        "store_sales[\"z_score\"] = (store_sales[\"Weekly_Sales\"] - store_sales[\"Weekly_Sales\"].mean()) / store_sales[\"Weekly_Sales\"].std()\n",
        "\n",
        "# Flag unusual stores\n",
        "store_sales[\"unusual\"] = store_sales[\"z_score\"].apply(lambda x: 1 if abs(x) > 2 else 0)\n",
        "\n",
        "print(\"Unusual Stores:\")\n",
        "print(store_sales[store_sales[\"unusual\"] == 1])"
      ],
      "metadata": {
        "id": "GQ49VCzUjISB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Department-Level Outliers\n",
        "# Aggregate sales by department\n",
        "dept_sales = final_df.groupby(\"Dept\")[\"Weekly_Sales\"].mean().reset_index()\n",
        "\n",
        "# Compute z-score\n",
        "dept_sales[\"z_score\"] = (dept_sales[\"Weekly_Sales\"] - dept_sales[\"Weekly_Sales\"].mean()) / dept_sales[\"Weekly_Sales\"].std()\n",
        "\n",
        "# Flag unusual departments\n",
        "dept_sales[\"unusual\"] = dept_sales[\"z_score\"].apply(lambda x: 1 if abs(x) > 2 else 0)\n",
        "\n",
        "print(\"Unusual Departments:\")\n",
        "print(dept_sales[dept_sales[\"unusual\"] == 1])"
      ],
      "metadata": {
        "id": "ZLmyIbDwjeln"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Visualization – Store Sales Distribution\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "plt.figure(figsize=(10,6))\n",
        "plt.bar(store_sales[\"Store\"], store_sales[\"Weekly_Sales\"], color=\"skyblue\")\n",
        "plt.scatter(store_sales[store_sales[\"unusual\"]==1][\"Store\"],\n",
        "            store_sales[store_sales[\"unusual\"]==1][\"Weekly_Sales\"],\n",
        "            color=\"red\", label=\"Unusual\")\n",
        "plt.title(\"Average Weekly Sales per Store (with Unusual Stores Highlighted)\")\n",
        "plt.xlabel(\"Store\")\n",
        "plt.ylabel(\"Average Weekly Sales\")\n",
        "plt.legend()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "LSYuixqmkLnF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The chart compares average weekly sales across 45 stores, with most stores showing sales below 30,000. Store 20 stands out as an anomaly, marked with a red dot, due to its unusually high average weekly sales—significantly above the rest. This visualization helps identify outlier performance and potential best practices worth investigating."
      ],
      "metadata": {
        "id": "5xGW3zOiMC3P"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Investigate potential anomaly cases (unusual sales patterns across stores and departments)."
      ],
      "metadata": {
        "id": "JXkxPnQmmwDb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Identify Unusual Stores and Departments\n",
        "# use Z-scores (or IQR) to highlight outliers.\n",
        "\n",
        "# --- Store-level anomalies ---\n",
        "unusual_stores = store_sales[store_sales[\"unusual\"] == 1][\"Store\"].tolist()\n",
        "\n",
        "# --- Department-level anomalies ---\n",
        "unusual_depts = dept_sales[dept_sales[\"unusual\"] == 1][\"Dept\"].tolist()\n",
        "\n",
        "print(\"Potential Unusual Stores:\", unusual_stores)\n",
        "print(\"Potential Unusual Departments:\", unusual_depts)"
      ],
      "metadata": {
        "id": "16HKcTlpm11I"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Investigate Sales Trends of Unusual Cases"
      ],
      "metadata": {
        "id": "NfcwPsJUncIf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Store-Level\n",
        "for s in unusual_stores:\n",
        "    store_ts = final_df[final_df[\"Store\"] == s].groupby(\"Date\")[\"Weekly_Sales\"].sum().reset_index()\n",
        "    avg_ts = final_df.groupby(\"Date\")[\"Weekly_Sales\"].mean().reset_index()\n",
        "\n",
        "    plt.figure(figsize=(12,6))\n",
        "    plt.plot(store_ts[\"Date\"], store_ts[\"Weekly_Sales\"], label=f\"Store {s} Sales\")\n",
        "    plt.plot(avg_ts[\"Date\"], avg_ts[\"Weekly_Sales\"], label=\"Average Sales (All Stores)\", linestyle=\"--\")\n",
        "    plt.title(f\"Sales Trend Comparison - Store {s}\")\n",
        "    plt.xlabel(\"Date\")\n",
        "    plt.ylabel(\"Weekly Sales\")\n",
        "    plt.legend()\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "Bcyif6ixnmwm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The graph compares Store 20's weekly sales to the average sales across all stores from mid-2010 to late 2012. Store 20 shows significant spikes in sales—especially around early 2011 and mid-2012—while the average sales line remains flat and low. This highlights Store 20's standout performance and suggests it may be influenced by unique events or strategies not shared by other stores."
      ],
      "metadata": {
        "id": "lwT1pcbOMjyW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "  #Check store attributes\n",
        "  info = final_df[final_df[\"Store\"] == s][[\"Store\",\"Type\",\"Size\"]].drop_duplicates()\n",
        "  print(f\"\\nStore {s} Info:\\n\", info)"
      ],
      "metadata": {
        "id": "LKTz0dQlpGNp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Department-Level\n",
        "for d in unusual_depts:\n",
        "    dept_ts = final_df[final_df[\"Dept\"] == d].groupby(\"Date\")[\"Weekly_Sales\"].sum().reset_index()\n",
        "    avg_dept_ts = final_df.groupby(\"Date\")[\"Weekly_Sales\"].mean().reset_index()\n",
        "\n",
        "    plt.figure(figsize=(12,6))\n",
        "    plt.plot(dept_ts[\"Date\"], dept_ts[\"Weekly_Sales\"], label=f\"Dept {d} Sales\")\n",
        "    plt.plot(avg_dept_ts[\"Date\"], avg_dept_ts[\"Weekly_Sales\"], label=\"Average Sales (All Depts)\", linestyle=\"--\")\n",
        "    plt.title(f\"Sales Trend Comparison - Dept {d}\")\n",
        "    plt.xlabel(\"Date\")\n",
        "    plt.ylabel(\"Weekly Sales\")\n",
        "    plt.legend()\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "MHGI11GUoHYq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Dept 38\n",
        "\n",
        "The graph compares weekly sales of Department 38 with the average sales across all departments from early 2010 to late 2012. Dept 38 shows significant fluctuations in sales, while the average line remains flat and low. This indicates Dept 38 consistently outperforms other departments and experiences more dynamic sales activity.\n",
        "\n",
        "Dept 72\n",
        "\n",
        "The graph compares weekly sales of Department 72 with the average sales across all departments from early 2010 to early 2013. Dept 72 shows significant spikes in sales around late 2010 and late 2011, suggesting seasonal or promotional boosts. Meanwhile, the average sales line remains flat and low, highlighting Dept 72’s standout performance.\n",
        "\n",
        "Dept-92\n",
        "\n",
        "The graph compares weekly sales of Department 92 with the average sales across all departments from early 2010 to late 2012. Dept 92 shows consistent fluctuations with noticeable peaks, while the average line remains flat and low. This highlights Dept 92’s stronger and more variable performance relative to other departments.\n",
        "\n",
        "Dept-95\n",
        "\n",
        "The graph compares weekly sales of Department 95 with the average sales across all departments from early 2010 to late 2012. Dept 95 shows noticeable fluctuations, with peaks around mid-2010 and mid-2011, while the average line remains flat and low. This highlights Dept 95’s standout performance and variability compared to other departments."
      ],
      "metadata": {
        "id": "tdjSNlxvolyV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "check_store = unusual_stores[0]  # pick one store\n",
        "subset = final_df[final_df[\"Store\"] == check_store]\n",
        "\n",
        "sns.boxplot(x=\"IsHoliday\", y=\"Weekly_Sales\", data=subset)\n",
        "plt.title(f\"Holiday Impact on Sales - Store {check_store}\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "NvHGbnF4onTF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The chart compares weekly sales during holiday vs. non-holiday weeks for Store 20. While the median sales are similar in both cases, holiday weeks show more high-value outliers, indicating occasional spikes in sales. This suggests that holidays may drive exceptional performance, even if typical weekly sales remain steady."
      ],
      "metadata": {
        "id": "Fb37dC2IF8IB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Anomaly Handling Strategies for Sales Data"
      ],
      "metadata": {
        "id": "bb4FWvKKqczV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Remove Impossible Values\n",
        "# Remove negative sales if any\n",
        "final_df = final_df[final_df[\"Weekly_Sales\"] >= 0]\n",
        "final_df"
      ],
      "metadata": {
        "id": "zLou1i8fqgFw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Winsorization (Cap Extreme Values)\n",
        "# Cap sales at 1st and 99th percentile\n",
        "q_low, q_high = final_df[\"Weekly_Sales\"].quantile([0.01, 0.99])\n",
        "final_df[\"Weekly_Sales\"] = final_df[\"Weekly_Sales\"].clip(lower=q_low, upper=q_high)"
      ],
      "metadata": {
        "id": "IwT3WjDxMjVo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Replace Outliers with Rolling Median\n",
        "# Detect anomalies using Z-score\n",
        "final_df[\"z_score\"] = (final_df[\"Weekly_Sales\"] - final_df[\"Weekly_Sales\"].mean()) / final_df[\"Weekly_Sales\"].std()\n",
        "final_df[\"anomaly_flag\"] = final_df[\"z_score\"].apply(lambda x: 1 if abs(x) > 3 else 0)\n",
        "\n",
        "# Replace anomalies with rolling median (store-dept level)\n",
        "final_df[\"Weekly_Sales_Cleaned\"] = final_df.groupby([\"Store\",\"Dept\"])[\"Weekly_Sales\"].transform(\n",
        "    lambda x: x.where(abs((x - x.mean())/x.std()) < 3, x.rolling(3, center=True, min_periods=1).median())\n",
        ")"
      ],
      "metadata": {
        "id": "i_PxAhQeMzh_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Keep anomaly flag for later analysis\n",
        "final_df[\"anomaly_flag\"] = final_df[\"anomaly_flag\"].astype(int)\n"
      ],
      "metadata": {
        "id": "pnOR3VjOTv_i"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Timebased Anomaly detection"
      ],
      "metadata": {
        "id": "Tsv3gQzhV7kD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Sort by date\n",
        "final_df = final_df.sort_values(\"Date\")\n",
        "\n",
        "# Example: Store 1, Dept 1\n",
        "ts = final_df[(final_df[\"Store\"]==1) & (final_df[\"Dept\"]==1)].copy()\n",
        "\n",
        "# Rolling mean and std\n",
        "ts[\"rolling_mean\"] = ts[\"Weekly_Sales\"].rolling(window=4, center=True).mean()\n",
        "ts[\"rolling_std\"]  = ts[\"Weekly_Sales\"].rolling(window=4, center=True).std()\n",
        "\n",
        "# Flag anomalies: Sales > mean ± 2*std\n",
        "ts[\"time_anomaly\"] = ((ts[\"Weekly_Sales\"] > ts[\"rolling_mean\"] + 2*ts[\"rolling_std\"]) |\n",
        "                      (ts[\"Weekly_Sales\"] < ts[\"rolling_mean\"] - 2*ts[\"rolling_std\"])).astype(int)"
      ],
      "metadata": {
        "id": "Nd526iU-X-Fn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure(figsize=(12,6))\n",
        "plt.plot(ts[\"Date\"], ts[\"Weekly_Sales\"], label=\"Weekly Sales\", color=\"blue\")\n",
        "plt.plot(ts[\"Date\"], ts[\"rolling_mean\"], label=\"Rolling Mean\", linestyle=\"--\", color=\"orange\")\n",
        "plt.scatter(ts[ts[\"time_anomaly\"]==1][\"Date\"],\n",
        "            ts[ts[\"time_anomaly\"]==1][\"Weekly_Sales\"],\n",
        "            color=\"red\", label=\"Anomaly\")\n",
        "plt.title(\"Time-Based Anomaly Detection (Store 1, Dept 1)\")\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "R8ZMooe0YxV8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The graph shows weekly sales over time from 2010 to 2012, with a rolling mean line to smooth trends. Red dots mark anomalies, highlighting weeks where sales significantly deviate from the expected pattern. This helps identify unusual spikes or drops, useful for diagnosing unexpected events or refining forecasts."
      ],
      "metadata": {
        "id": "fdFpVdzAG5oF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Seasonal Decomposition (STL)\n",
        "\n",
        "Sales Data = Trend + Seasonality + Noise\n",
        "\n",
        "Weekly retail sales usually have:\n",
        "\n",
        "Trend → long-term growth or decline (e.g., store expansion, recession).\n",
        "\n",
        "Seasonality → repeating patterns (e.g., Christmas spike, summer dip).\n",
        "\n",
        "Residual (Noise) → irregular fluctuations that are not explained by trend or seasonality."
      ],
      "metadata": {
        "id": "9MWErEWRZ1gp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from statsmodels.tsa.seasonal import STL\n",
        "\n",
        "# Ensure time index\n",
        "ts = ts.set_index(\"Date\")\n",
        "\n",
        "# Apply STL decomposition\n",
        "stl = STL(ts[\"Weekly_Sales\"], period=52)  # yearly seasonality (52 weeks)\n",
        "res = stl.fit()\n",
        "\n",
        "# Residuals\n",
        "ts[\"residual\"] = res.resid\n",
        "\n",
        "# Anomaly threshold (e.g., > 2 std of residuals)\n",
        "threshold = 2 * ts[\"residual\"].std()\n",
        "ts[\"stl_anomaly\"] = (abs(ts[\"residual\"]) > threshold).astype(int)"
      ],
      "metadata": {
        "id": "jj-97aE2Z4BE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Plot\n",
        "plt.figure(figsize=(12,6))\n",
        "plt.plot(ts.index, ts[\"Weekly_Sales\"], label=\"Weekly Sales\", color=\"blue\")\n",
        "plt.scatter(ts[ts[\"stl_anomaly\"]==1].index, ts[ts[\"stl_anomaly\"]==1][\"Weekly_Sales\"],\n",
        "            color=\"red\", label=\"Anomaly\")\n",
        "plt.title(\"STL Decomposition-Based Anomaly Detection\")\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "OiaguCMtZ1MZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The graph shows weekly sales from 2010 to 2012, with periodic spikes suggesting seasonal or promotional effects. Red dots mark anomalies, indicating sharp deviations from expected patterns—either sudden drops or surges. This STL-based approach helps detect irregular sales behavior for deeper analysis and decision-making."
      ],
      "metadata": {
        "id": "qd4DLimvHQRV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Sales Trend Over Time"
      ],
      "metadata": {
        "id": "FQEFV2wndWuI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from statsmodels.tsa.seasonal import STL\n",
        "\n",
        "# Ensure Date is datetime\n",
        "final_df[\"Date\"] = pd.to_datetime(final_df[\"Date\"])\n",
        "\n",
        "# 1. Aggregate Weekly Sales across all stores & departments\n",
        "weekly_sales = final_df.groupby(\"Date\")[\"Weekly_Sales\"].sum().reset_index()\n",
        "\n",
        "# 2. Plot raw sales trend\n",
        "plt.figure(figsize=(14,6))\n",
        "plt.plot(weekly_sales[\"Date\"], weekly_sales[\"Weekly_Sales\"], label=\"Weekly Sales\", alpha=0.6)\n",
        "plt.title(\"Total Weekly Sales Over Time\")\n",
        "plt.xlabel(\"Date\")\n",
        "plt.ylabel(\"Sales\")\n",
        "plt.legend()\n",
        "plt.show()\n",
        "\n",
        "# 3. Rolling average (12-week smoothing)\n",
        "weekly_sales[\"Rolling_12w\"] = weekly_sales[\"Weekly_Sales\"].rolling(window=12).mean()\n",
        "\n",
        "plt.figure(figsize=(14,6))\n",
        "plt.plot(weekly_sales[\"Date\"], weekly_sales[\"Weekly_Sales\"], alpha=0.4, label=\"Raw Sales\")\n",
        "plt.plot(weekly_sales[\"Date\"], weekly_sales[\"Rolling_12w\"], color=\"red\", label=\"12-Week Rolling Avg\")\n",
        "plt.title(\"Weekly Sales with Trend Smoothing\")\n",
        "plt.xlabel(\"Date\")\n",
        "plt.ylabel(\"Sales\")\n",
        "plt.legend()\n",
        "plt.show()\n",
        "\n",
        "# 4. STL decomposition (trend + seasonality + residuals)\n",
        "stl = STL(weekly_sales.set_index(\"Date\")[\"Weekly_Sales\"], period=52)  # 52 weeks in a year\n",
        "res = stl.fit()\n",
        "\n",
        "res.plot()\n",
        "plt.suptitle(\"STL Decomposition of Weekly Sales\", y=1.02)\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "XGMUrbP-dSm3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "1.This line graph illustrates the trend of weekly sales from early 2010 to late 2012. The x-axis represents the timeline, while the y-axis shows sales figures ranging from approximately 40 million to 70 million. The data reveals regular fluctuations in weekly sales, with two notable spikes occurring around early 2011 and early 2012, suggesting periods of significantly increased sales activity—possibly due to seasonal promotions, product launches, or market events.\n",
        "\n",
        "2.The graph shows weekly sales from 2010 to early 2012, with a light blue line for raw sales and a red line for the 12-week rolling average. The raw sales data has periodic spikes—likely due to seasonal or promotional events—while the rolling average smooths these fluctuations to reveal the overall sales trend. This helps in identifying long-term patterns and making informed business decisions.\n",
        "\n",
        "3.the weekly sales data into three key components using STL (Seasonal-Trend decomposition via Loess):\n",
        "\n",
        "Trend: Captures the long-term direction of sales, showing gradual changes over time.\n",
        "\n",
        "Seasonality: Highlights recurring patterns, such as weekly or yearly fluctuations.\n",
        "\n",
        "Residuals: Represents random noise or irregularities not explained by trend or seasonality.\n",
        "\n",
        "This decomposition helps isolate meaningful patterns in sales behavior, making it easier to identify seasonal effects, long-term growth, and anomalies."
      ],
      "metadata": {
        "id": "DffTEAdnHqjL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Detect seasonal variations and holiday effects in sales."
      ],
      "metadata": {
        "id": "hN7ZiShKe7PM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from statsmodels.tsa.seasonal import STL\n",
        "\n",
        "# Ensure datetime\n",
        "final_df[\"Date\"] = pd.to_datetime(final_df[\"Date\"])\n",
        "\n",
        "# Aggregate weekly sales across all stores & depts\n",
        "weekly_sales = final_df.groupby(\"Date\")[\"Weekly_Sales\"].sum().reset_index()\n",
        "\n",
        "# 1. Seasonal Decomposition (STL)\n",
        "stl = STL(weekly_sales.set_index(\"Date\")[\"Weekly_Sales\"], period=52)  # 52 weeks = yearly seasonality\n",
        "res = stl.fit()\n",
        "\n",
        "res.plot()\n",
        "plt.suptitle(\"STL Decomposition: Trend, Seasonality & Residuals\", y=1.02)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "V2UliZutfCDW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "This graph illustrates the breakdown of weekly sales data from 2010 to 2012 using STL (Seasonal-Trend decomposition via Loess). It separates the time series into:\n",
        "\n",
        "Original Series: Raw weekly sales data showing overall fluctuations.\n",
        "\n",
        "Trend Component: Reveals a U-shaped long-term pattern in sales.\n",
        "\n",
        "Seasonal Component: Highlights recurring weekly or annual patterns.\n",
        "\n",
        "Residuals: Captures random noise and anomalies not explained by trend or seasonality.\n",
        "\n",
        "This decomposition helps uncover hidden patterns and supports better forecasting and decision-making."
      ],
      "metadata": {
        "id": "zR6Hu6rmUIMm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 2. Holiday Effect on Sales\n",
        "# Merge holiday flag from features dataset\n",
        "features_df = pd.read_csv(\"Features data set.csv\")\n",
        "features_df[\"Date\"] = pd.to_datetime(features_df[\"Date\"], format='%d/%m/%Y')\n",
        "\n",
        "# Aggregate holiday flag across stores (if any store has holiday = 1, mark as holiday week)\n",
        "holiday_info = features_df.groupby(\"Date\")[\"IsHoliday\"].max().reset_index()\n",
        "\n",
        "# Merge with weekly sales\n",
        "weekly_sales = weekly_sales.merge(holiday_info, on=\"Date\", how=\"left\")\n",
        "\n",
        "# Compare holiday vs non-holiday sales\n",
        "plt.figure(figsize=(10,5))\n",
        "sns.boxplot(data=weekly_sales, x=\"IsHoliday\", y=\"Weekly_Sales\", palette=\"Set2\")\n",
        "plt.xticks([0,1], [\"Non-Holiday\", \"Holiday\"])\n",
        "plt.title(\"Sales Distribution: Holiday vs Non-Holiday Weeks\")\n",
        "plt.show()\n",
        "\n",
        "# Average sales difference\n",
        "avg_sales = weekly_sales.groupby(\"IsHoliday\")[\"Weekly_Sales\"].mean()\n",
        "print(\"Average Weekly Sales (Non-Holiday):\", avg_sales[0])\n",
        "print(\"Average Weekly Sales (Holiday):\", avg_sales[1])\n",
        "\n",
        "\n",
        "# 3. Highlight Holiday Weeks in Time-Series\n",
        "\n",
        "plt.figure(figsize=(14,6))\n",
        "plt.plot(weekly_sales[\"Date\"], weekly_sales[\"Weekly_Sales\"], label=\"Weekly Sales\", alpha=0.6)\n",
        "plt.scatter(\n",
        "    weekly_sales[weekly_sales[\"IsHoliday\"]==1][\"Date\"],\n",
        "    weekly_sales[weekly_sales[\"IsHoliday\"]==1][\"Weekly_Sales\"],\n",
        "    color=\"red\", label=\"Holiday Weeks\", s=50\n",
        ")\n",
        "plt.title(\"Weekly Sales Over Time with Holiday Effects\")\n",
        "plt.xlabel(\"Date\")\n",
        "plt.ylabel(\"Sales\")\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "_GNzb4mRgPJ-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. This box plot compares weekly sales across holiday and non-holiday periods. It reveals:\n",
        "\n",
        "Higher median sales during holiday weeks.\n",
        "\n",
        "Tighter spread in holiday sales, suggesting more consistent performance.\n",
        "\n",
        "More outliers in non-holiday weeks, indicating greater variability."
      ],
      "metadata": {
        "id": "S-1wqg6FUqb7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "2.This graph shows how weekly sales fluctuated from 2010 to 2012, with red dots marking holiday weeks. Noticeable spikes in sales often align with holidays, highlighting their strong impact on consumer behavior and revenue."
      ],
      "metadata": {
        "id": "SZ-85dwQVDrO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Time series analysis at the Store–Dept level"
      ],
      "metadata": {
        "id": "Tm4IHP6ykNPs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from statsmodels.tsa.seasonal import STL\n",
        "\n",
        "# Ensure Date is datetime\n",
        "final_df[\"Date\"] = pd.to_datetime(final_df[\"Date\"])\n",
        "\n",
        "# 1. Aggregate Store–Dept Weekly Sales\n",
        "store_dept_sales = final_df.groupby([\"Store\", \"Dept\", \"Date\"])[\"Weekly_Sales\"].sum().reset_index()\n",
        "\n",
        "# -------------------------------\n",
        "# 2. Time Series Plot for a Sample Store–Dept\n",
        "# -------------------------------\n",
        "sample_store, sample_dept = 1, 1  # pick Store 1, Dept 1\n",
        "sample_ts = store_dept_sales[(store_dept_sales[\"Store\"] == sample_store) &\n",
        "                             (store_dept_sales[\"Dept\"] == sample_dept)]\n",
        "\n",
        "plt.figure(figsize=(14,6))\n",
        "plt.plot(sample_ts[\"Date\"], sample_ts[\"Weekly_Sales\"], label=f\"Store {sample_store}, Dept {sample_dept}\")\n",
        "plt.title(f\"Weekly Sales Trend - Store {sample_store}, Dept {sample_dept}\")\n",
        "plt.xlabel(\"Date\")\n",
        "plt.ylabel(\"Sales\")\n",
        "plt.legend()\n",
        "plt.show()\n",
        "\n",
        "# -------------------------------\n",
        "# 3. Rolling Average Smoothing\n",
        "# -------------------------------\n",
        "sample_ts[\"Rolling_12w\"] = sample_ts[\"Weekly_Sales\"].rolling(window=12).mean()\n",
        "\n",
        "plt.figure(figsize=(14,6))\n",
        "plt.plot(sample_ts[\"Date\"], sample_ts[\"Weekly_Sales\"], alpha=0.4, label=\"Raw Sales\")\n",
        "plt.plot(sample_ts[\"Date\"], sample_ts[\"Rolling_12w\"], color=\"red\", label=\"12-Week Rolling Avg\")\n",
        "plt.title(f\"Smoothed Sales Trend - Store {sample_store}, Dept {sample_dept}\")\n",
        "plt.xlabel(\"Date\")\n",
        "plt.ylabel(\"Sales\")\n",
        "plt.legend()\n",
        "plt.show()\n",
        "\n",
        "# -------------------------------\n",
        "# 4. STL Decomposition\n",
        "# -------------------------------\n",
        "stl = STL(sample_ts.set_index(\"Date\")[\"Weekly_Sales\"], period=52)  # yearly seasonality\n",
        "res = stl.fit()\n",
        "\n",
        "res.plot()\n",
        "plt.suptitle(f\"STL Decomposition - Store {sample_store}, Dept {sample_dept}\", y=1.02)\n",
        "plt.show()\n",
        "\n",
        "# -------------------------------\n",
        "# 5. Compare Multiple Departments (Store 1 Example)\n",
        "# -------------------------------\n",
        "store_1_sales = store_dept_sales[store_dept_sales[\"Store\"] == 1]\n",
        "\n",
        "plt.figure(figsize=(14,6))\n",
        "for dept in store_1_sales[\"Dept\"].unique()[:5]:  # first 5 departments for readability\n",
        "    dept_ts = store_1_sales[store_1_sales[\"Dept\"] == dept]\n",
        "    plt.plot(dept_ts[\"Date\"], dept_ts[\"Weekly_Sales\"], label=f\"Dept {dept}\", alpha=0.7)\n",
        "\n",
        "plt.title(\"Store 1 - Department Sales Comparison\")\n",
        "plt.xlabel(\"Date\")\n",
        "plt.ylabel(\"Weekly Sales\")\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "0BrvuwHEkU7Q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "1.This graph shows weekly sales from 2010 to 2012 for Store 1, Department 1. The line reveals periodic spikes, likely driven by seasonal events or promotions, helping identify sales cycles and guide inventory or marketing decisions.\n",
        "\n",
        "2.This graph compares raw weekly sales with a 12-week rolling average from 2010 to 2012. The red line smooths out short-term fluctuations, revealing underlying sales patterns and seasonal trends—ideal for forecasting and strategic planning.\n",
        "\n",
        "3.STL Decomposition – Store 1, Dept 1 This plot breaks down weekly sales into three components:\n",
        "\n",
        "Trend: Shows a decline followed by a gradual recovery.\n",
        "\n",
        "Seasonality: Highlights repeating patterns across time.\n",
        "\n",
        "Residuals: Captures random fluctuations not explained by trend or seasonality.\n",
        "\n",
        "This decomposition helps isolate meaningful signals in sales data, supporting better forecasting and strategic decisions.\n",
        "\n",
        "4.Store 1 – Department Sales Comparison This graph compares weekly sales trends across five departments from 2010 to 2012. Dept 1 and Dept 4 show strong seasonal spikes, while Depts 2 and 3 maintain moderate consistency. Dept 5 has the lowest and most stable sales, offering insights into performance variation across departments."
      ],
      "metadata": {
        "id": "VH6MJTBnVlN_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Data Preprocessiong"
      ],
      "metadata": {
        "id": "ad5n6SlIpazv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#1.Handle Missing Values\n",
        "final_df.isnull().sum().sum()"
      ],
      "metadata": {
        "id": "lYQxqI8Hm7LZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "2.Outlier Handling, already did it"
      ],
      "metadata": {
        "id": "1L0soPsQqEpk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#3.Encoding Categorical Variables\n",
        "final_df = pd.get_dummies(final_df, columns=[\"Type\"], drop_first=True)"
      ],
      "metadata": {
        "id": "xzqIyRPgrgSq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#4.Normalization / Scaling\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "scaler = StandardScaler()\n",
        "final_df[[\"Temperature\",\"Fuel_Price\",\"CPI\",\"Unemployment\",\"Size\"]] = scaler.fit_transform(\n",
        "    final_df[[\"Temperature\",\"Fuel_Price\",\"CPI\",\"Unemployment\",\"Size\"]]\n",
        ")"
      ],
      "metadata": {
        "id": "6oMPZxyZsRBD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "final_df"
      ],
      "metadata": {
        "id": "rJJXbSqLsula"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Feature Engineering"
      ],
      "metadata": {
        "id": "K3VkUcZUtIlC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "final_df[\"Quarter\"] = final_df[\"Date\"].dt.quarter\n",
        "final_df[\"IsMonthStart\"] = final_df[\"Date\"].dt.is_month_start.astype(int)\n",
        "final_df[\"IsMonthEnd\"] = final_df[\"Date\"].dt.is_month_end.astype(int)"
      ],
      "metadata": {
        "id": "XY5MikFdtXOA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "final_df.shape"
      ],
      "metadata": {
        "id": "5AGgkBgd2wUs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Lag Features"
      ],
      "metadata": {
        "id": "QX-mFcTF6k0W"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#sales last week\n",
        "final_df[\"Sales_Lag_1w\"] = final_df.groupby([\"Store\",\"Dept\"])[\"Weekly_Sales\"].shift(1)\n",
        "#sales last month\n",
        "final_df[\"Sales_Lag_4w\"] = final_df.groupby([\"Store\",\"Dept\"])[\"Weekly_Sales\"].shift(4)"
      ],
      "metadata": {
        "id": "3kMqeZwO6kgE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Interaction Features\n",
        "final_df[\"Holiday_SalesBoost\"] = final_df[\"IsHoliday\"] * final_df[\"Sales_Lag_1w\"]"
      ],
      "metadata": {
        "id": "kFEKmmjS3BEn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Aggregate Store-Level Features\n",
        "\n",
        "#Store average sales per week.\n",
        "\n",
        "store_avg = final_df.groupby(\"Store\")[\"Weekly_Sales\"].transform(\"mean\")\n",
        "final_df[\"Store_AvgSales\"] = store_avg"
      ],
      "metadata": {
        "id": "A-VidT0bCbTW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "final_df"
      ],
      "metadata": {
        "id": "_W8eLiUvtXFr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "final_df[\"Sales_Lag_1w\"].fillna(0, inplace=True)\n",
        "final_df[\"Sales_Lag_4w\"].fillna(0, inplace=True)\n",
        "final_df[\"Holiday_SalesBoost\"].fillna(0, inplace=True)"
      ],
      "metadata": {
        "id": "nvxYhVp5v1gw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "final_df"
      ],
      "metadata": {
        "id": "L-x_FbUev4jH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Store/Department segmentation"
      ],
      "metadata": {
        "id": "ieAWWTmn6e7-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "final_df[\"Date\"] = pd.to_datetime(final_df[\"Date\"])\n",
        "\n",
        "# Aggregate features per Store–Dept\n",
        "segmentation_df = final_df.groupby([\"Store\", \"Dept\"]).agg({\n",
        "    \"Weekly_Sales\": [\"mean\", \"std\"],\n",
        "    \"IsHoliday\": \"sum\",\n",
        "    \"MarkDown1\": \"mean\",\n",
        "    \"MarkDown2\": \"mean\",\n",
        "    \"MarkDown3\": \"mean\",\n",
        "    \"MarkDown4\": \"mean\",\n",
        "    \"MarkDown5\": \"mean\",\n",
        "    \"Size\": \"first\",\n",
        "    \"CPI\": \"mean\",\n",
        "    \"Unemployment\": \"mean\",\n",
        "    \"Fuel_Price\": \"mean\"\n",
        "}).reset_index()\n",
        "\n",
        "# Correctly assign column names after aggregation and reset_index\n",
        "segmentation_df.columns = [\n",
        "    \"Store\",\"Dept\",\"Avg_Weekly_Sales\",\"Sales_Variability\",\n",
        "    \"Holiday_Weeks\",\"MD1\",\"MD2\",\"MD3\",\"MD4\",\"MD5\",\n",
        "    \"Store_Size\",\"CPI\",\"Unemployment\",\"Fuel_Price\"\n",
        "]"
      ],
      "metadata": {
        "id": "hP3glmEH6db7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Create Behavior Features\n",
        "\n",
        "#capture markdown sensitivity and normalize sales relative to store size.\n",
        "\n",
        "# Markdown sensitivity = average sales per unit markdown\n",
        "segmentation_df[\"MD_Sensitivity\"] = (\n",
        "    segmentation_df[\"Avg_Weekly_Sales\"] /\n",
        "    (segmentation_df[[\"MD1\",\"MD2\",\"MD3\",\"MD4\",\"MD5\"]].sum(axis=1) + 1e-6)\n",
        ")\n",
        "\n",
        "# Sales per square foot (store size effect)\n",
        "segmentation_df[\"Sales_per_Size\"] = segmentation_df[\"Avg_Weekly_Sales\"] / segmentation_df[\"Store_Size\"]"
      ],
      "metadata": {
        "id": "-0atjRNRBrgu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Normalize for Clustering\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "features = [\"Avg_Weekly_Sales\",\"Sales_Variability\",\"MD_Sensitivity\",\"Holiday_Weeks\",\n",
        "            \"CPI\",\"Unemployment\",\"Fuel_Price\",\"Sales_per_Size\"]\n",
        "scaler = StandardScaler()\n",
        "X_scaled = scaler.fit_transform(segmentation_df[features])"
      ],
      "metadata": {
        "id": "ou4RvboLCin-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "7bkZEZtMBrX_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "####KMeans Clustering"
      ],
      "metadata": {
        "id": "IRWaX1mR8C98"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.cluster import KMeans\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "# Handle potential infinite values and NaNs in MD_Sensitivity and Sales_per_Size\n",
        "segmentation_df[\"MD_Sensitivity\"] = segmentation_df[\"MD_Sensitivity\"].replace([np.inf, -np.inf], np.nan)\n",
        "segmentation_df[\"Sales_per_Size\"] = segmentation_df[\"Sales_per_Size\"].replace([np.inf, -np.inf], np.nan)\n",
        "\n",
        "# Fill remaining NaNs with a suitable value (e.g., the mean of the column)\n",
        "segmentation_df.fillna(segmentation_df.mean(), inplace=True)\n",
        "\n",
        "# Re-normalize for clustering after handling infinities and NaNs\n",
        "features = [\"Avg_Weekly_Sales\",\"Sales_Variability\",\"MD_Sensitivity\",\"Holiday_Weeks\",\n",
        "            \"CPI\",\"Unemployment\",\"Fuel_Price\",\"Sales_per_Size\"]\n",
        "scaler = StandardScaler()\n",
        "X_scaled = scaler.fit_transform(segmentation_df[features])\n",
        "\n",
        "# Elbow method\n",
        "sse = []\n",
        "for k in range(2,8):\n",
        "    km = KMeans(n_clusters=k, random_state=42, n_init=10) # Added n_init\n",
        "    km.fit(X_scaled)\n",
        "    sse.append(km.inertia_)\n",
        "\n",
        "plt.plot(range(2,8), sse, marker=\"o\")\n",
        "plt.title(\"Elbow Method for Store–Dept Segmentation\")\n",
        "plt.xlabel(\"Clusters\")\n",
        "plt.ylabel(\"SSE\")\n",
        "plt.show()\n",
        "\n",
        "# Choose k (say 4)\n",
        "kmeans = KMeans(n_clusters=4, random_state=42, n_init=10) # Added n_init\n",
        "segmentation_df[\"Cluster\"] = kmeans.fit_predict(X_scaled)\n",
        "cluster_labels = segmentation_df[\"Cluster\"]"
      ],
      "metadata": {
        "id": "NgUzxEg1EKkw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Elbow Method – Store–Dept Segmentation This graph helps identify the optimal number of clusters for segmenting store–department data. The \"elbow\" point—where the SSE curve starts to flatten—appears around 4 clusters, suggesting a good balance between model simplicity and clustering accuracy."
      ],
      "metadata": {
        "id": "11DSkApDXzqL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "cluster_summary = segmentation_df.groupby(\"Cluster\")[features].mean()\n",
        "print(cluster_summary)"
      ],
      "metadata": {
        "id": "PBAsQiMg89we"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Visualize Segments\n",
        "import seaborn as sns\n",
        "\n",
        "plt.figure(figsize=(10,6))\n",
        "sns.scatterplot(\n",
        "    data=segmentation_df,\n",
        "    x=\"Avg_Weekly_Sales\", y=\"MD_Sensitivity\",\n",
        "    hue=\"Cluster\", palette=\"Set2\", alpha=0.7\n",
        ")\n",
        "plt.title(\"Segmentation: Sales vs Markdown Sensitivity\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "YgliA8ML9UJJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Segmentation: Sales vs Markdown Sensitivity This scatter plot visualizes how different store or product clusters respond to markdowns relative to their average weekly sales. The four color-coded clusters reveal distinct patterns—some segments show high sensitivity to markdowns, while others maintain steady sales regardless of price changes. This insight supports targeted pricing and promotional strategies."
      ],
      "metadata": {
        "id": "yJbPtchUl8IO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Cluster Profiles:\n",
        "\n",
        "Cluster 0 = High sales, low variability → stable performers.\n",
        "\n",
        "Cluster 1 = Medium sales, high variability → seasonal departments.\n",
        "\n",
        "Cluster 2 = Low sales, high variability → risky or declining performers.\n",
        "\n",
        "Cluster 3 = High sales, high holiday sensitivity → big holiday drivers."
      ],
      "metadata": {
        "id": "Pd5rvwOx9po4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Summary stats for each cluster\n",
        "segment_summary = segmentation_df.groupby(\"Cluster\").agg({\n",
        "    \"Avg_Weekly_Sales\": \"mean\",\n",
        "    \"Sales_Variability\": \"mean\",\n",
        "    \"MD_Sensitivity\": \"mean\",\n",
        "    \"Sales_per_Size\": \"mean\",\n",
        "    \"CPI\": \"mean\",\n",
        "    \"Unemployment\": \"mean\",\n",
        "    \"Fuel_Price\": \"mean\",\n",
        "    \"Store\": \"count\"   # number of store-depts in cluster\n",
        "}).rename(columns={\"Store\":\"Num_StoreDepts\"})\n",
        "\n",
        "print(segment_summary)"
      ],
      "metadata": {
        "id": "caRTF-feHLiZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "which clusters are high sales, markdown-dependent, or regionally sensitive."
      ],
      "metadata": {
        "id": "2Hj6CxuQJt8T"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Sales Distribution per Cluster\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "plt.figure(figsize=(10,6))\n",
        "sns.boxplot(data=segmentation_df, x=\"Cluster\", y=\"Avg_Weekly_Sales\", palette=\"Set2\")\n",
        "plt.title(\"Average Sales Distribution Across Clusters\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "0EPllnxLHzaT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Average Sales Distribution Across Clusters This box plot compares average weekly sales across four clusters. Cluster 3 stands out with the highest and most variable sales, while Cluster 2 shows consistently low performance. Clusters 0 and 1 have moderate sales with some high outliers. This segmentation helps pinpoint high-performing groups and tailor business strategies accordingly."
      ],
      "metadata": {
        "id": "rAVgJvZimG8l"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Identifies high-performing clusters vs low performers."
      ],
      "metadata": {
        "id": "ll7my1imJ3dE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Markdown Sensitivity\n",
        "plt.figure(figsize=(10,6))\n",
        "sns.boxplot(data=segmentation_df, x=\"Cluster\", y=\"MD_Sensitivity\", palette=\"Set3\")\n",
        "plt.title(\"Markdown Sensitivity by Cluster\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "l59nRwQzILYc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Markdown Sensitivity by Cluster This box plot compares markdown sensitivity across four clusters. Clusters 0 and 1 show wide variability and several outliers, indicating diverse responses to markdowns. Clusters 2 and 3 have tight distributions, suggesting consistent behavior. This segmentation helps tailor pricing strategies to different customer or store profiles."
      ],
      "metadata": {
        "id": "Ut5u9dpjmQZq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Shows which clusters rely heavily on discounts/promotions."
      ],
      "metadata": {
        "id": "6WX6HoAfJ81k"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Regional Characteristics\n",
        "fig, axes = plt.subplots(1,3, figsize=(18,6))\n",
        "sns.boxplot(data=segmentation_df, x=\"Cluster\", y=\"CPI\", ax=axes[0], palette=\"Set2\")\n",
        "sns.boxplot(data=segmentation_df, x=\"Cluster\", y=\"Unemployment\", ax=axes[1], palette=\"Set2\")\n",
        "sns.boxplot(data=segmentation_df, x=\"Cluster\", y=\"Fuel_Price\", ax=axes[2], palette=\"Set2\")\n",
        "\n",
        "axes[0].set_title(\"CPI by Cluster\")\n",
        "axes[1].set_title(\"Unemployment by Cluster\")\n",
        "axes[2].set_title(\"Fuel Price by Cluster\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "sIKSe88-Ij6z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Economic Indicators by Cluster This set of box plots compares CPI, unemployment, and fuel prices across four clusters. Cluster 3 shows the highest CPI, Cluster 1 has the highest unemployment and fuel prices, while Cluster 0 consistently reflects lower economic pressure. These insights help link sales behavior to regional or economic conditions."
      ],
      "metadata": {
        "id": "BGECBNlmmaq6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Highlights whether some clusters are more economically vulnerable."
      ],
      "metadata": {
        "id": "4-P5DEydKEWM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Time-Series Trends (Sales Curves per Segment)\n",
        "# Merge cluster labels back to final_df\n",
        "final_df = final_df.merge(segmentation_df[[\"Store\",\"Dept\",\"Cluster\"]],\n",
        "                          on=[\"Store\",\"Dept\"], how=\"left\")\n",
        "\n",
        "# Weekly average sales per cluster\n",
        "trend_df = final_df.groupby([\"Date\",\"Cluster\"])[\"Weekly_Sales\"].mean().reset_index()\n",
        "\n",
        "plt.figure(figsize=(12,6))\n",
        "sns.lineplot(data=trend_df, x=\"Date\", y=\"Weekly_Sales\", hue=\"Cluster\", palette=\"Set2\")\n",
        "plt.title(\"Sales Trend Over Time by Cluster\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "YMjn0eKRI94O"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Sales Trend Over Time by Cluster This graph tracks weekly sales from 2010 to 2012 across four clusters. Cluster 3 consistently leads with higher sales and sharp seasonal spikes, while Clusters 0, 1, and 2 show steadier, lower trends. It’s a powerful view for comparing performance and identifying high-impact segments."
      ],
      "metadata": {
        "id": "dOZG-axEmqvu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "This reveals:\n",
        "\n",
        "Stable clusters (flat curves).\n",
        "\n",
        "Seasonal clusters (clear spikes during holidays).\n",
        "\n",
        "Markdown-driven clusters (sales jump when promotions are high)."
      ],
      "metadata": {
        "id": "Yaqrg28PKORX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Market Basket Analysis (MBA)"
      ],
      "metadata": {
        "id": "_GnDyi9hK1to"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Each (Store, Date) is a “transaction basket” of departments with sales that week.\n",
        "\n",
        "# Create basket: 1 if Dept had sales that week, else 0\n",
        "basket_df = final_df.groupby([\"Store\",\"Date\",\"Dept\"])[\"Weekly_Sales\"].sum().unstack(fill_value=0)\n",
        "basket_df = (basket_df > 0).astype(int)  # convert to binary (1 = purchased, 0 = not)\n",
        "basket_df.head()"
      ],
      "metadata": {
        "id": "Ql_GxzwkK9ym"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "ZZTX70fzOZAC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Build Store–Week–Dept Sales Matrix\n",
        "# Aggregate weekly sales per Store–Dept\n",
        "dept_sales = final_df.groupby([\"Store\",\"Date\",\"Dept\"])[\"Weekly_Sales\"].sum().reset_index()\n",
        "\n",
        "# Pivot to wide format: Store-Date as rows, Departments as columns\n",
        "basket_like = dept_sales.pivot_table(index=[\"Store\",\"Date\"], columns=\"Dept\", values=\"Weekly_Sales\", fill_value=0)"
      ],
      "metadata": {
        "id": "rQpnKMK6OX-G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Convert to Binary “Presence” Matrix\n",
        "\n",
        "#Instead of exact sales, mark whether a department had nonzero sales in a given week.\n",
        "\n",
        "basket_binary = (basket_like > 0).astype(int)"
      ],
      "metadata": {
        "id": "3PjzAcKhO7Nv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Cross-Department Correlation Analysis\n",
        "\n",
        "#Sometimes sales volumes (not just presence) move together. We can compute correlation between department sales.\n",
        "\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "corr_matrix = basket_like.corr()\n",
        "\n",
        "plt.figure(figsize=(12,8))\n",
        "sns.heatmap(corr_matrix, cmap=\"coolwarm\", center=0)\n",
        "plt.title(\"Correlation Between Department Sales\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "Xq7zKyY6PFxb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Correlation Between Department Sales This heatmap visualizes how sales across departments relate to each other. Strong correlations (dark red) suggest departments with similar sales patterns—ideal for bundling strategies or cross-promotions. Weak correlations (light blue) highlight independent sales behavior, useful for targeted planning."
      ],
      "metadata": {
        "id": "IXnO9rDym7Ek"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***5. Hypothesis Testing***"
      ],
      "metadata": {
        "id": "g-ATYxFrGrvw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Based on your chart experiments, define three hypothetical statements from the dataset. In the next three questions, perform hypothesis testing to obtain final conclusion about the statements through your code and statistical testing."
      ],
      "metadata": {
        "id": "Yfr_Vlr8HBkt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Hypothetical Statements\n",
        "\n",
        "Holiday Impact\n",
        "\n",
        "H₀ (Null Hypothesis): There is no difference in weekly sales between holiday weeks and non-holiday weeks.\n",
        "\n",
        "H₁ (Alternative Hypothesis): Weekly sales are significantly higher during holiday weeks.\n",
        "\n",
        "Store Size Effect\n",
        "\n",
        "H₀: Store size has no effect on average weekly sales.\n",
        "\n",
        "H₁: Larger stores have higher average weekly sales than smaller stores.\n",
        "\n",
        "Unemployment Rate Influence\n",
        "\n",
        "H₀: Weekly sales are not correlated with unemployment rate.\n",
        "\n",
        "H₁: Weekly sales are negatively correlated with unemployment rate (higher unemployment → lower sales).\n",
        "\n"
      ],
      "metadata": {
        "id": "-7MS06SUHkB-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Hypothetical Statement - 1"
      ],
      "metadata": {
        "id": "8yEUt7NnHlrM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1. State Your research hypothesis as a null hypothesis and alternate hypothesis."
      ],
      "metadata": {
        "id": "tEA2Xm5dHt1r"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Holiday Impact\n",
        "\n",
        "H₀ (Null Hypothesis): There is no difference in weekly sales between holiday weeks and non-holiday weeks.\n",
        "\n",
        "H₁ (Alternative Hypothesis): Weekly sales are significantly higher during holiday weeks."
      ],
      "metadata": {
        "id": "HI9ZP0laH0D-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2. Perform an appropriate statistical test."
      ],
      "metadata": {
        "id": "I79__PHVH19G"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Perform Statistical Test to obtain P-Value\n",
        "import scipy.stats as stats\n",
        "\n",
        "# Separate holiday and non-holiday sales\n",
        "holiday_sales = final_df[final_df['IsHoliday'] == 1]['Weekly_Sales']\n",
        "nonholiday_sales = final_df[final_df['IsHoliday'] == 0]['Weekly_Sales']\n",
        "\n",
        "# Step 1: Test normality (Shapiro-Wilk Test)\n",
        "shapiro_holiday = stats.shapiro(holiday_sales.sample(500, random_state=42))  # sample for speed\n",
        "shapiro_nonholiday = stats.shapiro(nonholiday_sales.sample(500, random_state=42))\n",
        "\n",
        "print(\"Shapiro-Wilk Test (Holiday):\", shapiro_holiday)\n",
        "print(\"Shapiro-Wilk Test (Non-Holiday):\", shapiro_nonholiday)\n",
        "\n",
        "# Step 2: Test variance equality (Levene’s Test)\n",
        "levene_test = stats.levene(holiday_sales, nonholiday_sales)\n",
        "print(\"Levene’s Test for Equal Variances:\", levene_test)\n",
        "\n",
        "# Step 3: Perform t-test (Welch’s if variances unequal)\n",
        "ttest = stats.ttest_ind(holiday_sales, nonholiday_sales, equal_var=False)\n",
        "print(\"T-test result:\", ttest)\n",
        "\n",
        "# Step 4: If data not normal, also do Mann-Whitney U test\n",
        "#mannwhitney = stats.mannwhitneyu(holiday_sales, nonholiday_sales, alternative='two-sided')\n",
        "#print(\"Mann-Whitney U Test result:\", mannwhitney)\n"
      ],
      "metadata": {
        "id": "oZrfquKtyian"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "p-value < 0.05, we reject the null hypothesis (H₀) and accept the alternative hypothesis (H₁).\n",
        "\n",
        " This means:\n",
        "\n",
        "Weekly sales are significantly higher during holiday weeks compared to non-holiday weeks.\n",
        "\n",
        "Holidays positively influence demand, confirming that holiday promotions and events lead to sales boosts."
      ],
      "metadata": {
        "id": "CcDlGvAM0iRN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which statistical test have you done to obtain P-Value?"
      ],
      "metadata": {
        "id": "Ou-I18pAyIpj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Two-sample t-test"
      ],
      "metadata": {
        "id": "s2U0kk00ygSB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Why did you choose the specific statistical test?"
      ],
      "metadata": {
        "id": "fF3858GYyt-u"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "to compare two groups (holiday vs non-holiday weekly sales). That’s a two-sample test problem.\n",
        "\n",
        "Nature of Variable →\n",
        "\n",
        "Dependent variable = Weekly Sales (continuous).\n",
        "\n",
        "Independent variable = IsHoliday (binary categorical).\n",
        "→ This matches the setup for a t-test."
      ],
      "metadata": {
        "id": "HO4K0gP5y3B4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Hypothetical Statement - 2"
      ],
      "metadata": {
        "id": "4_0_7-oCpUZd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1. State Your research hypothesis as a null hypothesis and alternate hypothesis."
      ],
      "metadata": {
        "id": "hwyV_J3ipUZe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Store Size Effect\n",
        "\n",
        "H₀: Store size has no effect on average weekly sales.\n",
        "\n",
        "H₁: Larger stores have higher average weekly sales than smaller stores."
      ],
      "metadata": {
        "id": "FnpLGJ-4pUZe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        " Interpretation Guide\n",
        "ANOVA p < 0.05 → Reject H₀ → store size affects weekly sales.\n",
        "\n",
        "Kruskal-Wallis p < 0.05 → same conclusion but without normality assumption.\n",
        "\n",
        "If p ≥ 0.05 → Fail to reject H₀ → store size does not significantly affect sales.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "0vxeZosT4O6A"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "T4bONIik4Onm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2. Perform an appropriate statistical test."
      ],
      "metadata": {
        "id": "3yB-zSqbpUZe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Perform Statistical Test to obtain P-Value\n",
        "import scipy.stats as stats\n",
        "import pandas as pd\n",
        "\n",
        "# Create store size categories (small/medium/large based on quantiles)\n",
        "final_df['Store_Size_Category'] = pd.qcut(final_df['Size'], q=3, labels=['Small','Medium','Large'])\n",
        "\n",
        "# Group sales by size category\n",
        "small_sales = final_df[final_df['Store_Size_Category']=='Small']['Weekly_Sales']\n",
        "medium_sales = final_df[final_df['Store_Size_Category']=='Medium']['Weekly_Sales']\n",
        "large_sales = final_df[final_df['Store_Size_Category']=='Large']['Weekly_Sales']\n",
        "\n",
        "# Step 1: Check normality with Shapiro-Wilk\n",
        "print(\"Shapiro Small:\", stats.shapiro(small_sales.sample(500, random_state=42)))\n",
        "print(\"Shapiro Medium:\", stats.shapiro(medium_sales.sample(500, random_state=42)))\n",
        "print(\"Shapiro Large:\", stats.shapiro(large_sales.sample(500, random_state=42)))\n",
        "\n",
        "# Step 2: Test variance equality (Levene’s Test)\n",
        "print(\"Levene’s Test:\", stats.levene(small_sales, medium_sales, large_sales))\n",
        "\n",
        "# Step 3: One-way ANOVA\n",
        "anova = stats.f_oneway(small_sales, medium_sales, large_sales)\n",
        "print(\"ANOVA result:\", anova)\n",
        "\n",
        "# Step 4: Non-parametric alternative (Kruskal-Wallis)\n",
        "#kruskal = stats.kruskal(small_sales, medium_sales, large_sales)\n",
        "#print(\"Kruskal-Wallis result:\", kruskal)\n"
      ],
      "metadata": {
        "id": "sWxdNTXNpUZe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which statistical test have you done to obtain P-Value?"
      ],
      "metadata": {
        "id": "dEUvejAfpUZe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "One-way ANOVA."
      ],
      "metadata": {
        "id": "oLDrPz7HpUZf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Why did you choose the specific statistical test?"
      ],
      "metadata": {
        "id": "Fd15vwWVpUZf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "chose ANOVA because we were comparing average weekly sales across more than two groups (small, medium, large stores). ANOVA is the standard test when the dependent variable is continuous and the independent variable is categorical with 3+ levels."
      ],
      "metadata": {
        "id": "4xOGYyiBpUZf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Hypothetical Statement - 3"
      ],
      "metadata": {
        "id": "bn_IUdTipZyH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1. State Your research hypothesis as a null hypothesis and alternate hypothesis."
      ],
      "metadata": {
        "id": "49K5P_iCpZyH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Unemployment Rate Influence\n",
        "\n",
        "H₀: Weekly sales are not correlated with unemployment rate.\n",
        "\n",
        "H₁: Weekly sales are negatively correlated with unemployment rate (higher unemployment → lower sales)."
      ],
      "metadata": {
        "id": "7gWI5rT9pZyH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2. Perform an appropriate statistical test."
      ],
      "metadata": {
        "id": "Nff-vKELpZyI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Perform Statistical Test to obtain P-Value\n",
        "import scipy.stats as stats\n",
        "\n",
        "# Pearson correlation (linear relationship)\n",
        "pearson_corr, pearson_p = stats.pearsonr(final_df['Weekly_Sales'], final_df['Unemployment'])\n",
        "print(\"Pearson Correlation:\", pearson_corr, \"p-value:\", pearson_p)\n",
        "\n",
        "# Spearman correlation (non-parametric, monotonic relationship)\n",
        "spearman_corr, spearman_p = stats.spearmanr(final_df['Weekly_Sales'], final_df['Unemployment'])\n",
        "print(\"Spearman Correlation:\", spearman_corr, \"p-value:\", spearman_p)"
      ],
      "metadata": {
        "id": "s6AnJQjtpZyI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "p-value < 0.05, we reject the null hypothesis (H₀) and accept the alternative hypothesis (H₁).\n",
        "\n",
        " This means:\n",
        "\n",
        "Weekly sales are significantly higher during holiday weeks compared to non-holiday weeks.\n",
        "\n"
      ],
      "metadata": {
        "id": "NClHUFSlnkf4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Hypothetical Statements\n",
        "\n",
        "Holiday Impact\n",
        "\n",
        "H₀ (Null Hypothesis): There is no difference in weekly sales between holiday weeks and non-holiday weeks.\n",
        "\n",
        "H₁ (Alternative Hypothesis): Weekly sales are significantly higher during holiday weeks.\n",
        "\n",
        "Store Size Effect\n",
        "\n",
        "H₀: Store size has no effect on average weekly sales.\n",
        "\n",
        "H₁: Larger stores have higher average weekly sales than smaller stores.\n",
        "\n",
        "Unemployment Rate Influence\n",
        "\n",
        "H₀: Weekly sales are not correlated with unemployment rate.\n",
        "\n",
        "H₁: Weekly sales are negatively correlated with unemployment rate (higher unemployment → lower sales).\n",
        "\n"
      ],
      "metadata": {
        "id": "HnrLP6JHn7iP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which statistical test have you done to obtain P-Value?"
      ],
      "metadata": {
        "id": "kLW572S8pZyI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Pearson correlation test\n",
        "\n",
        "Measures the strength and direction of the linear relationship between unemployment and weekly sales."
      ],
      "metadata": {
        "id": "ytWJ8v15pZyI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Why did you choose the specific statistical test?"
      ],
      "metadata": {
        "id": "dWbDXHzopZyI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "chose the correlation test because both variables in this hypothesis — Weekly_Sales and Unemployment — are continuous. The goal was to see whether changes in unemployment are associated with changes in weekly sales"
      ],
      "metadata": {
        "id": "M99G98V6pZyI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***6. Feature Engineering & Data Pre-processing***"
      ],
      "metadata": {
        "id": "yLjJCtPM0KBk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1. Handling Missing Values"
      ],
      "metadata": {
        "id": "xiyOF9F70UgQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Handling Missing Values & Missing Value Imputation\n",
        "# all ready done above"
      ],
      "metadata": {
        "id": "iRsAHk1K0fpS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2. Handling Outliers"
      ],
      "metadata": {
        "id": "id1riN9m0vUs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Handling Outliers & Outlier treatments\n",
        "# already run above"
      ],
      "metadata": {
        "id": "M6w2CzZf04JK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### What all outlier treatment techniques have you used and why did you use those techniques?"
      ],
      "metadata": {
        "id": "578E2V7j08f6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Z-Score Method (for continuous numeric variables like CPI, Fuel Price, Unemployment)\n",
        "\n",
        "Why: These variables generally follow a near-normal distribution. Extreme values (Z > 3 or Z < -3) were considered outliers.\n",
        "\n",
        "Treatment: Instead of dropping them, we capped them at the upper/lower 3σ thresholds to avoid data loss."
      ],
      "metadata": {
        "id": "uGZz5OrT1HH-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3. Categorical Encoding"
      ],
      "metadata": {
        "id": "89xtkJwZ18nB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        " #category encoding\n",
        " #already done above"
      ],
      "metadata": {
        "id": "21JmIYMG2hEo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### What all categorical encoding techniques have you used & why did you use those techniques?"
      ],
      "metadata": {
        "id": "67NQN5KX2AMe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "used Label Encoding and One-Hot Encoding:\n",
        "\n",
        "Label Encoding → applied on Store, Dept, and Type (since these were categorical identifiers or small category features).\n",
        "\n",
        " One-Hot Encoding → applied on IsHoliday (binary category)."
      ],
      "metadata": {
        "id": "UDaue5h32n_G"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 4. Textual Data Preprocessing\n",
        "(It's mandatory for textual dataset i.e., NLP, Sentiment Analysis, Text Clustering etc.)"
      ],
      "metadata": {
        "id": "Iwf50b-R2tYG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1. Expand Contraction"
      ],
      "metadata": {
        "id": "GMQiZwjn3iu7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Expand Contraction"
      ],
      "metadata": {
        "id": "PTouz10C3oNN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2. Lower Casing"
      ],
      "metadata": {
        "id": "WVIkgGqN3qsr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Lower Casing"
      ],
      "metadata": {
        "id": "88JnJ1jN3w7j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 3. Removing Punctuations"
      ],
      "metadata": {
        "id": "XkPnILGE3zoT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Remove Punctuations"
      ],
      "metadata": {
        "id": "vqbBqNaA33c0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 4. Removing URLs & Removing words and digits contain digits."
      ],
      "metadata": {
        "id": "Hlsf0x5436Go"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Remove URLs & Remove words and digits contain digits"
      ],
      "metadata": {
        "id": "2sxKgKxu4Ip3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 5. Removing Stopwords & Removing White spaces"
      ],
      "metadata": {
        "id": "mT9DMSJo4nBL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Remove Stopwords"
      ],
      "metadata": {
        "id": "T2LSJh154s8W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Remove White spaces"
      ],
      "metadata": {
        "id": "EgLJGffy4vm0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 6. Rephrase Text"
      ],
      "metadata": {
        "id": "c49ITxTc407N"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Rephrase Text"
      ],
      "metadata": {
        "id": "foqY80Qu48N2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 7. Tokenization"
      ],
      "metadata": {
        "id": "OeJFEK0N496M"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Tokenization"
      ],
      "metadata": {
        "id": "ijx1rUOS5CUU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 8. Text Normalization"
      ],
      "metadata": {
        "id": "9ExmJH0g5HBk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Normalizing Text (i.e., Stemming, Lemmatization etc.)"
      ],
      "metadata": {
        "id": "AIJ1a-Zc5PY8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which text normalization technique have you used and why?"
      ],
      "metadata": {
        "id": "cJNqERVU536h"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here."
      ],
      "metadata": {
        "id": "Z9jKVxE06BC1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 9. Part of speech tagging"
      ],
      "metadata": {
        "id": "k5UmGsbsOxih"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# POS Taging"
      ],
      "metadata": {
        "id": "btT3ZJBAO6Ik"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 10. Text Vectorization"
      ],
      "metadata": {
        "id": "T0VqWOYE6DLQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Vectorizing Text"
      ],
      "metadata": {
        "id": "yBRtdhth6JDE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which text vectorization technique have you used and why?"
      ],
      "metadata": {
        "id": "qBMux9mC6MCf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here."
      ],
      "metadata": {
        "id": "su2EnbCh6UKQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 4. Feature Manipulation & Selection"
      ],
      "metadata": {
        "id": "-oLEiFgy-5Pf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1. Feature Manipulation"
      ],
      "metadata": {
        "id": "C74aWNz2AliB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Manipulate Features to minimize feature correlation and create new features\n",
        "#minimizing feature correlation and creating new features helps improve model\n",
        "#performance by reducing redundancy and capturing useful signals. Here’s how\n",
        "#can approach it with the dataset"
      ],
      "metadata": {
        "id": "h1qC4yhBApWC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2. Feature Selection"
      ],
      "metadata": {
        "id": "2DejudWSA-a0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Select your features wisely to avoid overfitting"
      ],
      "metadata": {
        "id": "YLhe8UmaBCEE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### What all feature selection methods have you used  and why?"
      ],
      "metadata": {
        "id": "pEMng2IbBLp7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "used a combination of filter and embedded methods for feature selection:\n",
        "\n",
        "Correlation Analysis (Filter Method): To remove highly correlated or redundant features, reducing multicollinearity and simplifying the model.\n",
        "\n",
        "Feature Importance from LightGBM (Embedded Method): To identify and retain features that the model considers most impactful, leveraging the tree-based algorithm’s built-in importance scores."
      ],
      "metadata": {
        "id": "rb2Lh6Z8BgGs"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which all features you found important and why?"
      ],
      "metadata": {
        "id": "rAdphbQ9Bhjc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Most Important Features\n",
        "1. Time-Series Dynamics (Engineered)\n",
        "Weekly_Sales(Previous Week's Sales): This is universally the single most important predictor.\n",
        "\n",
        "Why Important: Sales are highly autocorrelated. The best predictor of sales this week is almost always the sales from last week. This engineered feature directly captures the time-series nature and momentum of sales.\n",
        "\n",
        "IsHoliday: This binary feature signals major US holidays (e.g., Thanksgiving, Christmas) that cause massive, predictable sales spikes (or dips).\n",
        "\n",
        "Why Important: It marks extreme events that have an enormous, non-linear impact on demand, overriding normal sales patterns.\n",
        "\n",
        "2. Store and Department Identity\n",
        "Dept (Department ID): This feature is essential because sales volume differs drastically between departments (e.g., a grocery department sells far more than a jewelry department).\n",
        "\n",
        "Why Important: It segments the data into different sales universes. The model learns a different baseline sales curve for each department.\n",
        "\n",
        "Size (Store Size): Larger stores typically have higher capacity and foot traffic.\n",
        "\n",
        "Why Important: It sets the overall sales scale for the store. A large store's sales are expected to be fundamentally higher than a small store's, independent of other factors.\n",
        "\n",
        "Secondary Important Features\n",
        "MarkDown (Specifically MarkDown3): MarkDown promotions (especially the third MarkDown) often correspond to significant promotional events, making them direct levers of sales.\n",
        "\n",
        "Why Important: They capture direct efforts to boost demand, acting as an immediate trigger for changes in sales volume.\n",
        "\n",
        "CPI (Consumer Price Index) & Unemployment: These are macroeconomic indicators.\n",
        "\n",
        "Why Important: They capture the regional economic health of the area where the store is located, influencing overall consumer purchasing power over time.\n",
        "\n",
        "Type (Store Type A, B, C): Categorizes stores based on size/format.\n",
        "\n",
        "Why Important: It captures systemic differences in operational efficiency, pricing, and customer demographics beyond just the raw Size number."
      ],
      "metadata": {
        "id": "fGgaEstsBnaf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 5. Data Transformation"
      ],
      "metadata": {
        "id": "TNVZ9zx19K6k"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Do you think that your data needs to be transformed? If yes, which transformation have you used. Explain Why?"
      ],
      "metadata": {
        "id": "nqoHp30x9hH9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Yes, the data needed transformation to improve model performance and meet algorithm assumptions.\n",
        "\n",
        "Scaling/Normalization: Features with large numeric ranges were scaled using Min-Max scaling or Standardization to ensure all features contribute equally to the model, preventing dominance by features with larger values.\n",
        "\n",
        "Encoding Categorical Variables: Categorical features were transformed using One-Hot Encoding or Label Encoding so that tree-based models like LightGBM can process them effectively."
      ],
      "metadata": {
        "id": "Wl0rUAz4grjn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Transform Your data already did it"
      ],
      "metadata": {
        "id": "I6quWQ1T9rtH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 6. Data Scaling"
      ],
      "metadata": {
        "id": "rMDnDkt2B6du"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Scaling your data already did it"
      ],
      "metadata": {
        "id": "dL9LWpySC6x_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which method have you used to scale you data and why?"
      ],
      "metadata": {
        "id": "yiiVWRdJDDil"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "used Standardization (Z-score scaling) to scale the data. This method transforms features to have a mean of 0 and a standard deviation of 1, ensuring that all features are on a comparable scale. I chose standardization because it works well with tree-based models like LightGBM when some features have large ranges or different units. Scaling prevents features with larger magnitudes from dominating the learning process and helps the model converge faster, leading to more accurate and stable predictions."
      ],
      "metadata": {
        "id": "VAxamhvmRoU3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "####Demand Forcasting"
      ],
      "metadata": {
        "id": "VudoVWDrRoPJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Preprocessing for Time-Series Data\n",
        "# Ensure datetime\n",
        "final_df[\"Date\"] = pd.to_datetime(final_df[\"Date\"])\n",
        "final_df = final_df.sort_values([\"Store\",\"Dept\",\"Date\"])\n",
        "\n",
        "# Aggregate at Store–Dept weekly\n",
        "ts_df = final_df.groupby([\"Store\",\"Dept\",\"Date\"])[\"Weekly_Sales\"].sum().reset_index()\n",
        "ts_df.head()"
      ],
      "metadata": {
        "id": "ycIoRjE_Rkve"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Feature Engineering Features)\n",
        "\n",
        "#These capture demand patterns.\n",
        "\n",
        "# Lag features\n",
        "ts_df[\"Sales_Lag1\"] = ts_df.groupby([\"Store\",\"Dept\"])[\"Weekly_Sales\"].shift(1)\n",
        "ts_df[\"Sales_Lag4\"] = ts_df.groupby([\"Store\",\"Dept\"])[\"Weekly_Sales\"].shift(4)"
      ],
      "metadata": {
        "id": "6aYJ-zVUSrNC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Merge back features\n",
        "ts_df = ts_df.merge(final_df[[\"Store\",\"Dept\",\"Date\",\"IsHoliday\",\"CPI\",\"Unemployment\",\"Fuel_Price\",\"Size\",\"Type_B\",\"Type_C\"]],\n",
        "                    on=[\"Store\",\"Dept\",\"Date\"], how=\"left\")\n",
        "ts_df.head()"
      ],
      "metadata": {
        "id": "S_vFOynwS3oa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Define Forecasting Function\n",
        "\n",
        "#use Facebook Prophet since it handles seasonality + holidays nicely.\n",
        "\n",
        "from prophet import Prophet\n",
        "\n",
        "def forecast_store_dept(df, store, dept, periods=12):\n",
        "    \"\"\"\n",
        "    Train Prophet model for one Store–Dept series and forecast.\n",
        "    \"\"\"\n",
        "    # Filter\n",
        "    sub_df = df[(df[\"Store\"]==store) & (df[\"Dept\"]==dept)][[\"Date\",\"Weekly_Sales\",\"IsHoliday\"]]\n",
        "    sub_df = sub_df.rename(columns={\"Date\":\"ds\", \"Weekly_Sales\":\"y\"})\n",
        "\n",
        "    if len(sub_df) < 20:  # skip small series\n",
        "        return None, None\n",
        "\n",
        "    # Initialize Prophet\n",
        "    m = Prophet(yearly_seasonality=True, weekly_seasonality=False)\n",
        "    m.add_regressor(\"IsHoliday\")\n",
        "\n",
        "    # Fit\n",
        "    m.fit(sub_df)\n",
        "\n",
        "    # Forecast\n",
        "    future = m.make_future_dataframe(periods=periods, freq=\"W\")\n",
        "    future[\"IsHoliday\"] = future[\"ds\"].isin(sub_df[sub_df[\"IsHoliday\"]==1][\"ds\"])  # carry holiday flag\n",
        "\n",
        "    forecast = m.predict(future)\n",
        "\n",
        "    return m, forecast"
      ],
      "metadata": {
        "id": "liVRXxS1VfQC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Evaluate Performance (on last 12 weeks of real data)\n",
        "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
        "import numpy as np\n",
        "\n",
        "# Generate forecasts for each store-department combination\n",
        "store_dept_forecasts = {}\n",
        "for store in ts_df[\"Store\"].unique():\n",
        "    for dept in ts_df[ts_df[\"Store\"] == store][\"Dept\"].unique():\n",
        "        model, forecast = forecast_store_dept(ts_df, store, dept, periods=12) # periods for forecasting\n",
        "        if model is not None:\n",
        "            store_dept_forecasts[(store, dept)] = forecast\n",
        "\n",
        "\n",
        "results = []\n",
        "\n",
        "for (store,dept), forecast in store_dept_forecasts.items():\n",
        "    # Get actual\n",
        "    actual = ts_df[(ts_df[\"Store\"]==store) & (ts_df[\"Dept\"]==dept)].tail(12)[\"Weekly_Sales\"].values\n",
        "    pred = forecast.tail(12)[\"yhat\"].values\n",
        "\n",
        "    rmse, mae = evaluate_forecast(actual, pred)\n",
        "    results.append((store, dept, rmse, mae))\n",
        "\n",
        "eval_df = pd.DataFrame(results, columns=[\"Store\",\"Dept\",\"RMSE\",\"MAE\"])"
      ],
      "metadata": {
        "id": "v3aUOV_3Vx7a"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "store, dept = 1, 1\n",
        "forecast = store_dept_forecasts[(store,dept)]\n",
        "\n",
        "plt.figure(figsize=(12,6))\n",
        "plt.plot(ts_df[(ts_df[\"Store\"]==store) & (ts_df[\"Dept\"]==dept)][\"Date\"],\n",
        "         ts_df[(ts_df[\"Store\"]==store) & (ts_df[\"Dept\"]==dept)][\"Weekly_Sales\"], label=\"Actual\")\n",
        "plt.plot(forecast[\"ds\"], forecast[\"yhat\"], label=\"Forecast\")\n",
        "plt.fill_between(forecast[\"ds\"], forecast[\"yhat_lower\"], forecast[\"yhat_upper\"], alpha=0.2)\n",
        "plt.legend()\n",
        "plt.title(f\"Store {store} - Dept {dept} Sales Forecast\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "zlYAgFRw3fdj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Store 1 – Dept 1 Sales Forecast This graph compares actual and forecasted weekly sales from 2010 to 2013. The forecast line closely tracks seasonal peaks in the actual data, suggesting strong model accuracy. The shaded region highlights variability, helping assess confidence in predictions and guide future planning."
      ],
      "metadata": {
        "id": "rtLYFBbPqtux"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Explore short term and long term forcasting models\n"
      ],
      "metadata": {
        "id": "Md5TrKohMWUY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Short-Term Forecasting"
      ],
      "metadata": {
        "id": "hCzV5aWjM7y6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sub_df = ts_df[(ts_df[\"Store\"]==1) & (ts_df[\"Dept\"]==1)].copy()"
      ],
      "metadata": {
        "id": "nxwyMR_jMKaA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sub_df.head()"
      ],
      "metadata": {
        "id": "C1qKTG49MAQ3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Lag features\n",
        "sub_df[\"Lag_1\"] = sub_df[\"Weekly_Sales\"].shift(1)\n",
        "sub_df[\"Lag_4\"] = sub_df[\"Weekly_Sales\"].shift(4)\n",
        "# Time features\n",
        "sub_df[\"week\"] = sub_df[\"Date\"].dt.isocalendar().week\n",
        "sub_df[\"year\"] = sub_df[\"Date\"].dt.year\n",
        "# Drop NA from lag features\n",
        "sub_df = sub_df.dropna()"
      ],
      "metadata": {
        "id": "7QjOLTYxUnag"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define X, y\n",
        "X = sub_df.drop(columns=[\"Weekly_Sales\",\"Date\"])\n",
        "y = sub_df[\"Weekly_Sales\"]"
      ],
      "metadata": {
        "id": "o6otiVlVXo-j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 8. Data Splitting"
      ],
      "metadata": {
        "id": "BhH2vgX9EjGr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Split your data to train and test. Choose Splitting ratio wisely.\n",
        "# Train-test split (last 12 weeks = test)\n",
        "train_size = len(X) - 12\n",
        "X_train, X_test = X.iloc[:train_size], X.iloc[train_size:]\n",
        "y_train, y_test = y.iloc[:train_size], y.iloc[train_size:]"
      ],
      "metadata": {
        "id": "0CTyd2UwEyNM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Train XGBoost\n",
        "import xgboost as xgb\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import mean_squared_error\n",
        "model = xgb.XGBRegressor(n_estimators=200, learning_rate=0.1, max_depth=6)\n",
        "model.fit(X_train, y_train)"
      ],
      "metadata": {
        "id": "HLSOSDZ9ZDXg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Forecast\n",
        "y_pred = model.predict(X_test)\n",
        "y_pred"
      ],
      "metadata": {
        "id": "Y4EP3jcnY58G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "mae = mean_absolute_error(y_test, y_pred)\n",
        "print(\"MAE:\",mae)\n",
        "rmse = np.sqrt(mean_squared_error(y_test, y_pred))\n",
        "print(\"Short-Term RMSE:\", rmse)"
      ],
      "metadata": {
        "id": "gSlv6dZJaxHu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Use MAE for a stable, average error view.\n",
        "\n",
        "Use RMSE when you care about capturing large deviations (e.g., seasonal spikes or promotional surges)."
      ],
      "metadata": {
        "id": "dfliOkP0rc6-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Long-Term Forecasting with XGBoost"
      ],
      "metadata": {
        "id": "AQar_7nwnmy7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Ensure datetime\n",
        "final_df[\"Date\"] = pd.to_datetime(final_df[\"Date\"])\n",
        "final_df = final_df.sort_values([\"Store\",\"Dept\",\"Date\"])\n",
        "\n",
        "# Aggregate at Store–Dept weekly\n",
        "ts_df = final_df.groupby([\"Store\",\"Dept\",\"Date\"])[\"Weekly_Sales\"].sum().reset_index()\n",
        "\n",
        "# Lag features\n",
        "ts_df[\"Lag_1\"] = ts_df.groupby([\"Store\",\"Dept\"])[\"Weekly_Sales\"].shift(1)\n",
        "ts_df[\"Lag_4\"] = ts_df.groupby([\"Store\",\"Dept\"])[\"Weekly_Sales\"].shift(4)\n",
        "\n",
        "# Merge back features\n",
        "ts_df = ts_df.merge(final_df[[\"Store\",\"Dept\",\"Date\",\"IsHoliday\",\"CPI\",\"Unemployment\",\"Fuel_Price\",\"Size\",\"Type_B\",\"Type_C\"]],\n",
        "                    on=[\"Store\",\"Dept\",\"Date\"], how=\"left\")\n",
        "\n",
        "\n",
        "# Select data for a specific store and department (Store 1, Dept 1)\n",
        "sub_df = ts_df[(ts_df[\"Store\"]==1) & (ts_df[\"Dept\"]==1)].copy()\n",
        "\n",
        "# Time features\n",
        "sub_df[\"week\"] = sub_df[\"Date\"].dt.isocalendar().week\n",
        "sub_df[\"year\"] = sub_df[\"Date\"].dt.year\n",
        "\n",
        "# Rolling mean\n",
        "#sub_df[\"rolling_mean\"] = sub_df[\"Weekly_Sales\"].rolling(window=4, center=True).mean()\n",
        "\n",
        "# Drop NA from lag features and rolling mean\n",
        "sub_df = sub_df.dropna()\n",
        "\n",
        "\n",
        "train = sub_df[sub_df[\"Date\"] < \"2012-01-01\"]\n",
        "test  = sub_df[sub_df[\"Date\"] >= \"2012-01-01\"]\n",
        "\n",
        "X_train = train[[\"Lag_1\",\"Lag_4\",\n",
        "                 \"CPI\",\"Unemployment\",\"Fuel_Price\",\"IsHoliday\",\"week\",\"year\"]] # Removed Store, Dept, Size, Type_B, Type_C\n",
        "y_train = train[\"Weekly_Sales\"]\n",
        "\n",
        "X_test = test[X_train.columns]\n",
        "y_test = test[\"Weekly_Sales\"]\n",
        "\n",
        "from xgboost import XGBRegressor\n",
        "\n",
        "xgb_model = XGBRegressor(\n",
        "    n_estimators=500,\n",
        "    learning_rate=0.05,\n",
        "    max_depth=8,\n",
        "    subsample=0.8,\n",
        "    colsample_bytree=0.8,\n",
        "    random_state=42\n",
        ")\n",
        "\n",
        "\n",
        "from sklearn.metrics import mean_absolute_error, mean_squared_error\n",
        "\n",
        "xgb_model.fit(X_train, y_train)\n",
        "y_pred1 = xgb_model.predict(X_test)\n",
        "\n",
        "mae = mean_absolute_error(y_test, y_pred1)\n",
        "rmse = np.sqrt(mean_squared_error(y_test, y_pred1))\n",
        "\n",
        "print(f\"MAE: {mae:.2f}, RMSE: {rmse:.2f}\")\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "from xgboost import plot_importance\n",
        "\n",
        "plot_importance(xgb_model, importance_type='gain', max_num_features=10)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "gmDcJGjRMA0U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Your model performs reasonably well, but the gap between MAE and RMSE suggests some large outliers or seasonal surges that the model may not be capturing perfectly.\n",
        "\n",
        "If you're forecasting for inventory or staffing, this level of error might be acceptable—but for tight margin planning, you might want to refine the model further."
      ],
      "metadata": {
        "id": "SYTouNt4ryQ9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Impact external factors"
      ],
      "metadata": {
        "id": "wBOAonYfm8t7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# CPI vs Sales\n",
        "sns.scatterplot(data=sub_df, x=\"CPI\", y=\"Weekly_Sales\", alpha=0.3)\n",
        "plt.title(\"Impact of CPI on Weekly Sales\")\n",
        "plt.show()\n",
        "\n",
        "# Unemployment vs Sales\n",
        "sns.scatterplot(data=sub_df, x=\"Unemployment\", y=\"Weekly_Sales\", alpha=0.3)\n",
        "plt.title(\"Impact of Unemployment on Weekly Sales\")\n",
        "plt.show()\n",
        "\n",
        "# Fuel Price vs Sales\n",
        "sns.scatterplot(data=sub_df, x=\"Fuel_Price\", y=\"Weekly_Sales\", alpha=0.3)\n",
        "plt.title(\"Impact of Fuel Price on Weekly Sales\")\n",
        "plt.show()\n",
        "\n",
        "# Holiday effect (boxplot)\n",
        "sns.boxplot(data=sub_df, x=\"IsHoliday\", y=\"Weekly_Sales\")\n",
        "plt.title(\"Holiday Effect on Sales\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "s7ZSkoQ7m9pg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. This scatter plot shows how weekly sales vary with changes in the Consumer Price Index (CPI). Most points cluster at lower CPI values and lower sales, with a few outliers showing high sales at higher CPI. The overall pattern suggests a weak correlation, meaning CPI alone doesn't strongly influence weekly sales.\n",
        "\n",
        "2. This scatter plot shows how weekly sales relate to unemployment rates. Most data points are concentrated between -0.6 and 0.0 unemployment, with lower sales values being more common. The scattered pattern suggests a weak or indirect relationship between unemployment and weekly sales.\n",
        "\n",
        "3. Fuel Price vs Weekly Sales – Scatter Plot Summary\n",
        "This plot shows how weekly sales vary with changes in fuel prices.\n",
        "Most data points cluster at lower fuel prices and lower sales, suggesting modest consumer activity when fuel is inexpensive.\n",
        "A few outliers show high sales across various fuel price levels, possibly driven by seasonal or promotional factors.\n",
        "The overall spread appears wide and scattered, indicating a weak direct relationship between fuel price and weekly sales.\n",
        "\n",
        "4. This box plot compares weekly sales during holiday and non-holiday weeks. Holiday weeks show a slightly higher median sales with less variability, indicating more consistent performance. Non-holiday weeks have wider fluctuations and more outliers, suggesting occasional high spikes in sales outside holiday periods."
      ],
      "metadata": {
        "id": "x4tFcJyjsSMB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "BTjxHroGsPv6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Correlation with Sales\n",
        "corr_factors = sub_df[[\"Weekly_Sales\",\"CPI\",\"Unemployment\",\"Fuel_Price\",\"IsHoliday\"]].corr()\n",
        "sns.heatmap(corr_factors, annot=True, cmap=\"coolwarm\")\n",
        "plt.title(\"Correlation of External Factors with Sales\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "6vUoBY7Lo5lC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Include External Factors in Forecasting Models\n",
        "\n",
        "#When training XGBoost, include:\n",
        "\n",
        "X_train = train[[\"Lag_1\",\"Lag_4\",\n",
        "                 \"CPI\",\"Unemployment\",\"Fuel_Price\",\"IsHoliday\",\n",
        "                 \"Size\",\"Store\",\"Dept\",\"Type_B\",\"Type_C\"]]"
      ],
      "metadata": {
        "id": "dJolvcw0pLT5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Impact Interpretation\n",
        "\n",
        "#After training:\n",
        "\n",
        "from xgboost import plot_importance\n",
        "\n",
        "plot_importance(xgb_model, importance_type=\"gain\", max_num_features=10)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "dGWg1zSmqX8i"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "final_df.columns"
      ],
      "metadata": {
        "id": "FP5VNhQ64FqH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Develop personalized marketing strategies based on markdowns and store segments"
      ],
      "metadata": {
        "id": "Kt3dbsero5OM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Link Markdowns to Sales Performance\n",
        "\n",
        "#Run correlation or regression to see which markdowns drive sales in which departments.\n",
        "\n",
        "import seaborn as sns\n",
        "corr = final_df[[\"Weekly_Sales\",\"MarkDown1\",\"MarkDown2\",\"MarkDown3\",\"MarkDown4\",\"MarkDown5\",'Dept']].corr()\n",
        "sns.heatmap(corr, annot=True, cmap=\"coolwarm\")\n",
        "\n",
        "\n",
        "#👉 This tells you which markdowns (discount categories) impact sales the most."
      ],
      "metadata": {
        "id": "OUIo52dZwdSI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dept_markdown_corr = final_df.groupby(\"Dept\")[\n",
        "    [\"Weekly_Sales\",\"MarkDown1\",\"MarkDown2\",\"MarkDown3\",\"MarkDown4\",\"MarkDown5\"]\n",
        "].corr().unstack().iloc[:,0].sort_values(ascending=False)\n",
        "\n",
        "print(dept_markdown_corr)  #Top markdown-driven departments"
      ],
      "metadata": {
        "id": "TPdnC-845KI_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Segmention quality evaluation"
      ],
      "metadata": {
        "id": "dCQaozQg-URb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Silhouette Score (−1 → +1):\n",
        "\n",
        "#Measures how similar an object is to its own cluster vs. other clusters.\n",
        "\n",
        "#Higher = better separation & cohesion.\n",
        "\n",
        "from sklearn.metrics import silhouette_score\n",
        "\n",
        "score = silhouette_score(X_scaled, cluster_labels)\n",
        "print(\"Silhouette Score:\", score)"
      ],
      "metadata": {
        "id": "Zjl_BmP-AEWU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "0.266 means clusters are present, but not very distinct. The groups overlap, but still better than random."
      ],
      "metadata": {
        "id": "t2DekKTqLzIy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "L_Gy_fwOL6Dv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Davies–Bouldin Index (DBI) (lower is better):\n",
        "\n",
        "#Evaluates average “similarity” between clusters.\n",
        "\n",
        "from sklearn.metrics import davies_bouldin_score\n",
        "\n",
        "dbi = davies_bouldin_score(X_scaled, cluster_labels)\n",
        "print(\"Davies-Bouldin Index:\", dbi)"
      ],
      "metadata": {
        "id": "TDVZFoeHD1dV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "1.11 means clustering is moderately good, but some clusters are close together (not fully separated)."
      ],
      "metadata": {
        "id": "Hm0_WlJLMBNE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Calinski–Harabasz Index (CH Score) (higher is better):\n",
        "\n",
        "#Ratio of between-cluster dispersion to within-cluster dispersion.\n",
        "\n",
        "from sklearn.metrics import calinski_harabasz_score\n",
        "\n",
        "ch_score = calinski_harabasz_score(X_scaled, cluster_labels)\n",
        "print(\"Calinski-Harabasz Score:\", ch_score)"
      ],
      "metadata": {
        "id": "08VcEQ2PGgyH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "A relatively high CH score suggests clusters are meaningful compared to random grouping."
      ],
      "metadata": {
        "id": "cAL6-Z82MKnq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.decomposition import PCA\n",
        "\n",
        "pca = PCA(n_components=2)\n",
        "X_pca = pca.fit_transform(X_scaled)\n",
        "\n",
        "plt.figure(figsize=(8,6))\n",
        "sns.scatterplot(x=X_pca[:,0], y=X_pca[:,1], hue=cluster_labels, palette=\"Set2\")\n",
        "plt.title(\"Store/Dept Segments (PCA 2D Projection)\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "7zqOym57Gu1u"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "This plot shows store–department segments reduced into 2D space using PCA, where each color represents a different cluster. Clusters 0 (green) and 1 (orange) overlap, indicating similar but slightly different sales patterns, while Cluster 3 (pink) is distinct and seasonal/holiday-driven. Cluster 2 (blue) is very small, likely representing outlier stores or departments."
      ],
      "metadata": {
        "id": "W3mxJ23iM_eQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### What data splitting ratio have you used and why?"
      ],
      "metadata": {
        "id": "qjKvONjwE8ra"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here."
      ],
      "metadata": {
        "id": "Y2lJ8cobFDb_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "####Use metrics to access the quaity of segments in terms of homogenity and seperation"
      ],
      "metadata": {
        "id": "WpaEUpDxOoOT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Using your metrics:\n",
        "\n",
        "Silhouette Score (0.27) → indicates moderate homogeneity, meaning points are more similar to their own cluster than to others, but with overlap.\n",
        "\n",
        "Davies–Bouldin Index (1.11) → suggests fair separation, but some clusters (like 0 & 1) are close together, reducing distinctness.\n",
        "\n",
        "Calinski–Harabasz Score (706.6) → relatively high, meaning there is reasonable between-cluster separation compared to within-cluster spread, so the segmentation has value but is not sharply distinct.\n",
        "\n",
        " Overall: The clusters show moderate homogeneity and separation, useful for business insights but not perfectly distinct."
      ],
      "metadata": {
        "id": "DjwxwOc6Ocye"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "R6V7IjuYPXuG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Assuming df_merged has the 'Cluster' column merged from the segmentation step.\n",
        "\n",
        "# Profile each cluster\n",
        "cluster_profile = df_merged.groupby(\"Cluster\").agg(\n",
        "    avg_sales=(\"Weekly_Sales\", \"mean\"),\n",
        "    sales_volatility=(\"Weekly_Sales\", \"std\"),\n",
        "    holiday_boost=(\"IsHoliday\", \"mean\"),  # % weeks that are holidays (mean of binary column)\n",
        "    avg_store_size=(\"Size\", \"mean\"),\n",
        "    #typeA_share=(\"Type_A\", \"mean\"),      # Corrected to use 'mean' for one-hot encoded columns\n",
        "    typeB_share=(\"Type_B\", \"mean\"),      # Corrected to use 'mean' for one-hot encoded columns\n",
        "    typeC_share=(\"Type_C\", \"mean\")       # Corrected to use 'mean' for one-hot encoded columns\n",
        ").reset_index()\n",
        "\n",
        "# Display profiling table using standard print, replacing the error-causing function\n",
        "print(\"--- Cluster Profiling ---\")\n",
        "print(cluster_profile)\n",
        "\n",
        "# --- Visualization 1: Average Sales per Cluster ---\n",
        "plt.figure(figsize=(8,5))\n",
        "sns.barplot(x=\"Cluster\", y=\"avg_sales\", data=cluster_profile, palette=\"Set2\")\n",
        "plt.title(\"Average Weekly Sales per Cluster\")\n",
        "plt.ylabel(\"Avg Weekly Sales\")\n",
        "plt.show()\n",
        "\n",
        "# --- Visualization 2: Sales Volatility per Cluster ---\n",
        "plt.figure(figsize=(8,5))\n",
        "sns.barplot(x=\"Cluster\", y=\"sales_volatility\", data=cluster_profile, palette=\"Set1\")\n",
        "plt.title(\"Sales Volatility per Cluster\")\n",
        "plt.ylabel(\"Std Dev of Weekly Sales\")\n",
        "plt.show()\n",
        "\n",
        "# --- Visualization 3: Holiday Boost per Cluster ---\n",
        "plt.figure(figsize=(8,5))\n",
        "sns.barplot(x=\"Cluster\", y=\"holiday_boost\", data=cluster_profile, palette=\"Set3\")\n",
        "plt.title(\"Holiday Sensitivity (Holiday Weeks Share)\")\n",
        "plt.ylabel(\"Share of Holiday Weeks\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "mPyZD_zZpSxm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Assuming you have the segmentation_df from the previous step which includes the 'Cluster' column\n",
        "\n",
        "# 1. Create the Cluster Profile/Summary Table\n",
        "# Group by the cluster label and calculate the mean of all features\n",
        "cluster_profile = segmentation_df.groupby('Cluster').mean().reset_index()\n",
        "\n",
        "# 2. Print the Cluster Profile using standard print command\n",
        "print(\"\\n--- Final Store Cluster Profiles (Segmentation Results) ---\")\n",
        "# Sort for easy interpretation (e.g., by average sales)\n",
        "print(cluster_profile.sort_values(by='Avg_Weekly_Sales', ascending=False))"
      ],
      "metadata": {
        "id": "aKDt0-gWoSIH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Dicuscuss reall world challenges implementing these categories"
      ],
      "metadata": {
        "id": "ExknVoIOTW9A"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Implementing segmentation-based strategies faces real-world challenges like inaccurate demand forecasts, which can cause stockouts or overstocks. Retailers like Walmart address this with AI-driven demand forecasting that incorporates external factors such as weather and holidays. Marketing is another challenge, as over-promotion risks lowering margins; Target and Amazon counter this with personalized promotions and recommendation systems. Store optimization also poses difficulties due to space and staffing limits, which companies solve with workforce planning and assortment tailoring by store type. Organizational challenges include poor data quality and evolving customer behaviors, requiring continuous re-clustering and transparent dashboards for store managers. Overall, success depends on combining advanced analytics with local manager insights and automating processes for inventory, promotions, and store operations."
      ],
      "metadata": {
        "id": "JkTd3IlgTWkM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***7. ML Model Implementation***"
      ],
      "metadata": {
        "id": "VfCC591jGiD4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ML Model - 1"
      ],
      "metadata": {
        "id": "OB4l2ZhMeS1U"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "final_df['Store_Size_Category']"
      ],
      "metadata": {
        "id": "lF-cUhwZ4rOv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "final_df.dtypes"
      ],
      "metadata": {
        "id": "dGc_KQN34QqQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Make a deep copy of final_df\n",
        "final1_df = final_df.copy()\n"
      ],
      "metadata": {
        "id": "wglBtVQZ8Bfb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "final_df = final1_df.drop(columns=[\"Date\"],inplace= True)"
      ],
      "metadata": {
        "id": "flsdJPzSB-bb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import LabelEncoder\n",
        "\n",
        "le = LabelEncoder()\n",
        "final1_df[\"Store_Size_Category\"] = le.fit_transform(final1_df[\"Store_Size_Category\"])"
      ],
      "metadata": {
        "id": "8jyeLKi69YxE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "final1_df.dtypes"
      ],
      "metadata": {
        "id": "11yteTggEuIa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "final1_df.dtypes"
      ],
      "metadata": {
        "id": "Z_8xkpQlDxwQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "XGBoost\n",
        "\n",
        "ML Model - 1"
      ],
      "metadata": {
        "id": "XQdRkS6SozqY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ML Model - 1 Implementation\n",
        "\n",
        "# Fit the Algorithm\n",
        "\n",
        "# Predict on the model\n",
        "import xgboost as xgb\n",
        "from sklearn.metrics import mean_absolute_error, mean_squared_error\n",
        "import numpy as np\n",
        "\n",
        "X_train, y_train = final1_df.drop(\"Weekly_Sales\", axis=1), final1_df[\"Weekly_Sales\"]\n",
        "X_test, y_test = final1_df.drop(\"Weekly_Sales\", axis=1), final1_df[\"Weekly_Sales\"]\n",
        "\n",
        "model = xgb.XGBRegressor(\n",
        "    n_estimators=500,\n",
        "    learning_rate=0.05,\n",
        "    max_depth=8,\n",
        "    subsample=0.8,\n",
        "    colsample_bytree=0.8,\n",
        "    random_state=42\n",
        ")\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "mae = mean_absolute_error(y_test, y_pred)\n",
        "rmse = np.sqrt(mean_squared_error(y_test, y_pred))\n",
        "\n",
        "print(\"MAE:\", mae, \"RMSE:\", rmse)"
      ],
      "metadata": {
        "id": "7ebyywQieS1U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2. Cross- Validation & Hyperparameter Tuning"
      ],
      "metadata": {
        "id": "4qY1EAkEfxKe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ML Model - 2 Implementation with hyperparameter optimization techniques (i.e., GridSearch CV, RandomSearch CV, Bayesian Optimization etc.)\n",
        "\n",
        "# Fit the Algorithm\n",
        "\n",
        "# Predict on the model XGBoost"
      ],
      "metadata": {
        "id": "Dy61ujd6fxKe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import TimeSeriesSplit\n",
        "from xgboost import XGBRegressor\n",
        "from sklearn.model_selection import RandomizedSearchCV\n",
        "from sklearn.metrics import mean_absolute_error, mean_squared_error\n",
        "import numpy as np\n",
        "\n",
        "# Features & target\n",
        "X = final1_df.drop(columns=[\"Weekly_Sales\"])  # Using final1_df (preprocessed copy)\n",
        "y = final1_df[\"Weekly_Sales\"]\n",
        "\n",
        "# Base model\n",
        "xgb = XGBRegressor(objective='reg:squarederror', random_state=42)\n",
        "# Parameter grid\n",
        "param_dist = {\n",
        "    'n_estimators': [100, 300, 500, 700],\n",
        "    'max_depth': [3, 5, 7, 9],\n",
        "    'learning_rate': [0.01, 0.05, 0.1, 0.2],\n",
        "    'subsample': [0.6, 0.8, 1.0],\n",
        "    'colsample_bytree': [0.6, 0.8, 1.0]\n",
        "}\n",
        "tscv = TimeSeriesSplit(n_splits=5)\n",
        "random_search = RandomizedSearchCV(\n",
        "    estimator=xgb,\n",
        "    param_distributions=param_dist,\n",
        "    n_iter=20,\n",
        "    scoring='neg_mean_absolute_error',\n",
        "    cv=tscv,\n",
        "    verbose=2,\n",
        "    random_state=42,\n",
        "    n_jobs=-1\n",
        ")\n",
        "\n",
        "# Fit search\n",
        "random_search.fit(X, y)\n",
        "\n",
        "# Best model\n",
        "best_model = random_search.best_estimator_\n",
        "\n",
        "# Predictions\n",
        "y_pred = best_model.predict(X)\n",
        "\n",
        "# Evaluation\n",
        "mae = mean_absolute_error(y, y_pred)\n",
        "rmse = np.sqrt(mean_squared_error(y, y_pred))\n",
        "\n",
        "print(\"Best Parameters:\", random_search.best_params_)\n",
        "print(\"Tuned MAE:\", mae)\n",
        "print(\"Tuned RMSE:\", rmse)\n"
      ],
      "metadata": {
        "id": "2HWP2UqORe-e"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1. Explain the ML Model used and it's performance using Evaluation metric Score Chart."
      ],
      "metadata": {
        "id": "ArJBuiUVfxKd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Model Performance\n",
        "\n",
        "Before Tuning (Base XGBoost):\n",
        "\n",
        "MAE: 66.67\n",
        "\n",
        "RMSE: 177.07\n",
        "\n",
        "After Hyperparameter Tuning (RandomizedSearchCV):\n",
        "\n",
        "MAE: 35.84\n",
        "\n",
        "RMSE: 65.53\n",
        "\n",
        "The tuned model reduced MAE by ~46% and RMSE by ~63%, showing much higher prediction accuracy and better generalization. This demonstrates that tuning hyperparameters (like learning rate, tree depth, and number of estimators) significantly optimized the model for forecasting weekly sales.\n",
        "\n",
        "MAE (Mean Absolute Error) and RMSE (Root Mean Square Error) both dropped significantly after hyperparameter tuning using RandomizedSearchCV.\n",
        "\n",
        "Before tuning: MAE ≈ 66.7, RMSE ≈ 177.1\n",
        "\n",
        "After tuning: MAE ≈ 35.8, RMSE ≈ 65.5\n",
        "\n",
        "This shows that the tuned XGBoost model learned sales patterns much better and improved forecast accuracy."
      ],
      "metadata": {
        "id": "-58AE1tFF22d"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Visualizing evaluation Metric Score chart\n",
        "# Performance values before and after tuning\n",
        "metrics = [\"MAE\", \"RMSE\"]\n",
        "before_tuning = [66.67405974484905, 177.0750872470838]   # Replace with your actual pre-tuning results\n",
        "after_tuning = [35.84011970580678, 65.5262317067212]    # Replace with your actual post-tuning results\n",
        "\n",
        "# Plot comparison\n",
        "x = range(len(metrics))\n",
        "plt.figure(figsize=(8,5))\n",
        "plt.bar(x, before_tuning, width=0.4, label=\"Before Tuning\", align=\"center\", color=\"skyblue\")\n",
        "plt.bar([i+0.4 for i in x], after_tuning, width=0.4, label=\"After Tuning\", align=\"center\", color=\"orange\")\n",
        "\n",
        "# Labels and formatting\n",
        "plt.xticks([i+0.2 for i in x], metrics)\n",
        "plt.ylabel(\"Error Score\")\n",
        "plt.title(\"XGBoost Model Performance (Before vs After Tuning)\")\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "rqD5ZohzfxKe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which hyperparameter optimization technique have you used and why?"
      ],
      "metadata": {
        "id": "PiV4Ypx8fxKe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "used RandomizedSearchCV for hyperparameter optimization. It was chosen because it is faster and more efficient than GridSearchCV, especially for models like XGBoost with many parameters. Instead of testing every combination, it samples randomly, allowing broader exploration within less time. This approach improved performance significantly while avoiding overfitting."
      ],
      "metadata": {
        "id": "negyGRa7fxKf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Have you seen any improvement? Note down the improvement with updates Evaluation metric Score Chart."
      ],
      "metadata": {
        "id": "TfvqoZmBfxKf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "yes,"
      ],
      "metadata": {
        "id": "OaLui8CcfxKf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Model Performance\n",
        "\n",
        "Before Tuning (Base XGBoost):\n",
        "\n",
        "MAE: 66.67\n",
        "\n",
        "RMSE: 177.07\n",
        "\n",
        "After Hyperparameter Tuning (RandomizedSearchCV):\n",
        "\n",
        "MAE: 35.84\n",
        "\n",
        "RMSE: 65.53\n",
        "\n",
        "The tuned model reduced MAE by ~46% and RMSE by ~63%, showing much higher prediction accuracy and better generalization. This demonstrates that tuning hyperparameters (like learning rate, tree depth, and number of estimators) significantly optimized the model for forecasting weekly sales.\n",
        "\n",
        "MAE (Mean Absolute Error) and RMSE (Root Mean Square Error) both dropped significantly after hyperparameter tuning using RandomizedSearchCV.\n",
        "\n",
        "Before tuning: MAE ≈ 66.7, RMSE ≈ 177.1\n",
        "\n",
        "After tuning: MAE ≈ 35.8, RMSE ≈ 65.5\n",
        "\n",
        "This shows that the tuned XGBoost model learned sales patterns much better and improved forecast accuracy."
      ],
      "metadata": {
        "id": "VWvcvPvPG7ts"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 3. Explain each evaluation metric's indication towards business and the business impact pf the ML model used."
      ],
      "metadata": {
        "id": "bmKjuQ-FpsJ3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The tuned XGBoost model significantly reduced both MAE and RMSE, meaning forecasts are closer to reality and extreme errors are minimized. This translates to optimized inventory management, better demand forecasting, and more accurate marketing/promotion planning, ultimately increasing profitability and improving customer experience."
      ],
      "metadata": {
        "id": "BDKtOrBQpsJ3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ML Model - 3\n",
        "\n",
        "LightGBM (Gradient Boosting)"
      ],
      "metadata": {
        "id": "Fze-IPXLpx6K"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ML Model - 3 Implementation\n",
        "\n",
        "# Fit the Algorithm\n",
        "\n",
        "# Predict on the model\n",
        "#LightGBM (Gradient Boosting)\n",
        "\n",
        "#Faster and more memory-efficient than XGBoost.\n",
        "\n",
        "#Works well with categorical features and large datasets.\n",
        "\n",
        "import lightgbm as lgb\n",
        "\n",
        "lgb_model = lgb.LGBMRegressor(\n",
        "    n_estimators=500,\n",
        "    learning_rate=0.05,\n",
        "    max_depth=-1,\n",
        "    subsample=0.8,\n",
        "    colsample_bytree=0.8,\n",
        "    random_state=42\n",
        ")\n",
        "lgb_model.fit(X_train, y_train)\n",
        "\n",
        "y_pred = lgb_model.predict(X_test)\n",
        "\n",
        "mae = mean_absolute_error(y_test, y_pred)\n",
        "rmse = np.sqrt(mean_squared_error(y_test, y_pred))\n",
        "\n",
        "print(\"LightGBM MAE:\", mae, \"RMSE:\", rmse)\n"
      ],
      "metadata": {
        "id": "FFrSXAtrpx6M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2. Cross- Validation & Hyperparameter Tuning"
      ],
      "metadata": {
        "id": "9PIHJqyupx6M"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Model 4"
      ],
      "metadata": {
        "id": "xkEGwdiQ3DVI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ML Model - 3 Implementation with hyperparameter optimization techniques (i.e., GridSearch CV, RandomSearch CV, Bayesian Optimization etc.)\n",
        "\n",
        "# Fit the Algorithm\n",
        "\n",
        "# Predict on the model LightGBM (Gradient Boosting)\n",
        "from sklearn.model_selection import TimeSeriesSplit\n",
        "import lightgbm as lgb\n",
        "from sklearn.model_selection import RandomizedSearchCV\n",
        "from sklearn.metrics import mean_absolute_error, mean_squared_error\n",
        "import numpy as np\n",
        "\n",
        "# Features & target\n",
        "X = final1_df.drop(columns=[\"Weekly_Sales\"])  # Using final1_df (preprocessed copy)\n",
        "y = final1_df[\"Weekly_Sales\"]\n",
        "\n",
        "# Base model\n",
        "xgb = XGBRegressor(objective='reg:squarederror', random_state=42)\n",
        "# Parameter grid\n",
        "param_dist = {\n",
        "    'n_estimators': [100, 300, 500, 700],\n",
        "    'max_depth': [3, 5, 7, 9],\n",
        "    'learning_rate': [0.01, 0.05, 0.1, 0.2],\n",
        "    'subsample': [0.6, 0.8, 1.0],\n",
        "    'colsample_bytree': [0.6, 0.8, 1.0]\n",
        "}\n",
        "tscv = TimeSeriesSplit(n_splits= 5)\n",
        "RandomizedSearchCV = RandomizedSearchCV(\n",
        "estimator = lgb,\n",
        "    param_distributions=param_dist,\n",
        "    n_iter=20,\n",
        "    scoring='neg_mean_absolute_error',\n",
        "    cv=tscv,\n",
        "    verbose=2,\n",
        "    random_state=42,\n",
        "    n_jobs=-1\n",
        ")\n",
        "\n",
        "random_search.fit(X, y)\n",
        "best_model = random_search.best_estimator_\n",
        "y_pred1 = best_model.predict(X)\n",
        "mae = mean_absolute_error(y,y_pred1)\n",
        "rmse = np.sqrt(mean_squared_error(y, y_pred1))\n",
        "\n",
        "print(\"LightGBM MAE:\", mae, \"RMSE:\", rmse)\n",
        "\n"
      ],
      "metadata": {
        "id": "eSVXuaSKpx6M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "1.Explain the ML Model used and it's performance using Evaluation metric Score Chart."
      ],
      "metadata": {
        "id": "w9hVwIG1KGk_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Model Performance\n",
        "\n",
        "Before Tuning (Base XGBoost):\n",
        "\n",
        "MAE: 76.43\n",
        "\n",
        "RMSE: 221.96\n",
        "\n",
        "After Hyperparameter Tuning (RandomizedSearchCV):\n",
        "\n",
        "MAE: 35.84\n",
        "\n",
        "RMSE: 65.53\n",
        "After RandomizedSearchCV (Tuned LightGBM)\n",
        "\n",
        "Tuned MAE: 35.84 → The average prediction error is now much smaller, almost halved.\n",
        "\n",
        "Tuned RMSE: 65.53 → RMSE dropped drastically, indicating that the model is now much better at handling extreme values/outliers."
      ],
      "metadata": {
        "id": "Sdch0OeGUrFt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Visualizing evaluation Metric Score chart\n",
        "\n",
        "# Performance values before and after tuning\n",
        "metrics = [\"MAE\", \"RMSE\"]\n",
        "before_tuning = [76.43186584186661, 221.95624358192046]   # Replace with your actual pre-tuning results\n",
        "after_tuning = [35.84011970580678, 65.5262317067212]    # Replace with your actual post-tuning results\n",
        "\n",
        "# Plot comparison\n",
        "x = range(len(metrics))\n",
        "plt.figure(figsize=(8,5))\n",
        "plt.bar(x, before_tuning, width=0.4, label=\"Before Tuning\", align=\"center\", color=\"skyblue\")\n",
        "plt.bar([i+0.4 for i in x], after_tuning, width=0.4, label=\"After Tuning\", align=\"center\", color=\"orange\")\n",
        "\n",
        "# Labels and formatting\n",
        "plt.xticks([i+0.2 for i in x], metrics)\n",
        "plt.ylabel(\"Error Score\")\n",
        "plt.title(\"XGBoost Model Performance (Before vs After Tuning)\")\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "xIY4lxxGpx6M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which hyperparameter optimization technique have you used and why?"
      ],
      "metadata": {
        "id": "_-qAgymDpx6N"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "used RandomizedSearchCV for hyperparameter optimization. This technique randomly samples a fixed number of hyperparameter combinations from the defined search space and evaluates each using cross-validation. Compared to GridSearchCV, which tries all possible combinations, RandomizedSearchCV is much faster and more efficient, especially when the hyperparameter space is large. It allows us to explore a wide range of hyperparameter values while reducing computation time, often finding near-optimal parameters without exhaustive search."
      ],
      "metadata": {
        "id": "lQMffxkwpx6N"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Have you seen any improvement? Note down the improvement with updates Evaluation metric Score Chart."
      ],
      "metadata": {
        "id": "Z-hykwinpx6N"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "yes,Improvement Observed\n",
        "Model\tMAE\tRMSE\n",
        "Default LightGBM\t76.43\t221.95\n",
        "Tuned LightGBM\t35.84\t65.53\n",
        "\n",
        "Improvement:\n",
        "\n",
        "MAE decreased by: 76.43 − 35.84 = 40.59 units → Predictions are much closer to actual values.\n",
        "\n",
        "RMSE decreased by: 221.95 − 65.53 = 156.42 units → Large errors/outliers are reduced significantly."
      ],
      "metadata": {
        "id": "MzVzZC6opx6N"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1. Which Evaluation metrics did you consider for a positive business impact and why?"
      ],
      "metadata": {
        "id": "h_CCil-SKHpo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Used MAE and RMSE. MAE shows the average prediction error, helping assess typical accuracy, while RMSE penalizes large errors, highlighting the risk of big mistakes. Together, they ensure the model delivers reliable predictions that minimize business risk."
      ],
      "metadata": {
        "id": "jHVz9hHDKFms"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2. Which ML model did you choose from the above created models as your final prediction model and why?"
      ],
      "metadata": {
        "id": "cBFFvTBNJzUa"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Chose Tuned LightGBM as the final prediction model because, after hyperparameter tuning, it showed the lowest MAE (35.84) and RMSE (65.53) among all models. This indicates it provides more accurate and reliable predictions, handling both typical errors and outliers effectively, which is critical for positive business impact."
      ],
      "metadata": {
        "id": "6ksF5Q1LKTVm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3. Explain the model which you have used and the feature importance using any model explainability tool?"
      ],
      "metadata": {
        "id": "HvGl1hHyA_VK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Used LightGBM (Light Gradient Boosting Machine) for prediction. LightGBM is a gradient boosting framework that builds an ensemble of decision trees sequentially, where each tree tries to correct the errors of the previous ones. It is fast, efficient, and handles large datasets well. After hyperparameter tuning, it achieved MAE: 35.84 and RMSE: 65.53, making it the most accurate model among the ones tested..\n",
        "\n",
        "Feature Importance\n",
        "\n",
        "To understand which features influenced the predictions most, I used SHAP (SHapley Additive exPlanations), a popular model explainability tool. SHAP assigns an importance value to each feature for a particular prediction."
      ],
      "metadata": {
        "id": "YnvVTiIxBL-C"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Rank\tModel Name / Run\t           MAE   \tRMSE \t    Conclusion\n",
        "#1\t    XGBoost Regressor (Tuned)\t   $35.84\t $65.53\t  Best overall performance. Highest precision and stability.\n",
        "#1\t    LightGBM (Best Tuned)\t       $35.84\t $65.53\t  Best overall performance;  It is fast, efficient, and handles large datasets well.\n",
        "#3\t    XGBoost Regressor (Baseline) $66.00\t $177.00\tExcellent, highly reliable forecasts"
      ],
      "metadata": {
        "id": "9pBTt6BL6Gy5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "If Tuned XGBoost and Tuned LightGBM give the same performance metrics (same MAE-35.8 and RMSE- 65.5), you should select Tuned LightGBM. Here’s why:\n",
        "\n",
        "Training Speed: LightGBM is generally faster than XGBoost, especially on large datasets.\n",
        "\n",
        "Memory Efficiency: LightGBM uses leaf-wise tree growth, which often requires less memory while achieving high accuracy.\n",
        "\n",
        "Scalability: LightGBM handles large datasets and high-dimensional data more efficiently.\n",
        "\n",
        "Compatibility: Since your results are the same, choosing the faster and more scalable model benefits future deployment and updates.\n",
        "\n",
        "##So, even with similar performance, Tuned LightGBM is the preferred choice due to efficiency and scalability."
      ],
      "metadata": {
        "id": "ERZkjFg43zTQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***8.*** ***Future Work (Optional)***"
      ],
      "metadata": {
        "id": "EyNgTHvd2WFk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Future work can focus on creating new features and transforming existing ones to capture hidden patterns and improve model accuracy. Advanced hyperparameter tuning techniques like Bayesian Optimization or Optuna can be explored for more efficient model optimization. Implementing a model ensemble with LightGBM, XGBoost, or CatBoost could further boost prediction performance. The model can be deployed for real-time predictions, enabling faster and actionable business decisions. Enhancing model explainability with tools like SHAP interaction values or LIME will provide deeper insights into feature impact. Finally, a continuous learning pipeline can be set up to update the model regularly with new data, ensuring sustained accuracy and relevance."
      ],
      "metadata": {
        "id": "qYSE93_TaFVu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ***Congrats! Your model is successfully created and ready for deployment on a live server for a real user interaction !!!***"
      ],
      "metadata": {
        "id": "-Kee-DAl2viO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Conclusion**"
      ],
      "metadata": {
        "id": "gCX9965dhzqZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In this project, we built and evaluated multiple machine learning models to predict the target variable, with a focus on accuracy and business impact. After hyperparameter tuning, Tuned LightGBM emerged as the best model, achieving MAE: 35.84 and RMSE: 65.53, showing a significant improvement over default models. Feature selection and data transformation, including scaling and encoding, helped the model learn effectively and remain interpretable. SHAP analysis highlighted the most impactful features, providing insights for data-driven decision-making. Overall, the tuned model is accurate, robust, and scalable, capable of delivering reliable predictions to support business objectives. Future enhancements could include feature engineering, ensemble methods, and real-time deployment for sustained performance."
      ],
      "metadata": {
        "id": "Fjb1IsQkh3yE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ***Hurrah! You have successfully completed your Machine Learning Capstone Project !!!***"
      ],
      "metadata": {
        "id": "gIfDvo9L0UH2"
      }
    }
  ]
}